# Chat Sessions y Historial

**Tiempo estimado**: 35 minutos
**Nivel**: Intermedio
**Prerrequisitos**: generate_content (1.1.1)

## ¿Por qué importa este concepto?

Los chat sessions permiten conversaciones multi-turno donde el modelo recuerda el contexto anterior. Esto es fundamental para:
- Chatbots que mantienen contexto
- Asistentes que recuerdan preferencias
- Agentes que ejecutan tareas en múltiples pasos
- Cualquier interacción que requiera continuidad

---

## Crear y usar chat sessions

### Básico

```python
import google.generativeai as genai
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-1.5-flash")

# Iniciar chat
chat = model.start_chat(history=[])

# Enviar mensajes
response1 = chat.send_message("Hola, mi nombre es Carlos")
print(f"Gemini: {response1.text}")

response2 = chat.send_message("¿Cuál es mi nombre?")
print(f"Gemini: {response2.text}")  # Debería recordar "Carlos"
```

### Con historial inicial

```python
from google.generativeai.types import ContentDict


def create_chat_with_context(context_messages: list) -> genai.ChatSession:
    """
    Crea un chat con historial pre-existente.

    Args:
        context_messages: Lista de dicts con 'role' y 'content'

    Returns:
        ChatSession configurado
    """
    history = []
    for msg in context_messages:
        history.append(ContentDict(
            role=msg["role"],
            parts=[msg["content"]]
        ))

    return model.start_chat(history=history)


# Crear chat con contexto previo
chat = create_chat_with_context([
    {"role": "user", "content": "Estoy aprendiendo Python"},
    {"role": "model", "content": "¡Genial! Python es un excelente lenguaje para empezar. ¿En qué área te interesa más?"},
    {"role": "user", "content": "Me interesa la automatización"},
])

# Continuar la conversación
response = chat.send_message("¿Qué librerías me recomiendas?")
print(response.text)  # Responderá en contexto de Python + automatización
```

---

## Gestión del historial

### Acceder al historial

```python
def get_chat_history(chat: genai.ChatSession) -> list:
    """
    Obtiene el historial formateado.

    Returns:
        Lista de mensajes con rol y contenido
    """
    history = []
    for message in chat.history:
        history.append({
            "role": message.role,
            "content": message.parts[0].text if message.parts else "",
            "timestamp": None,  # El SDK no proporciona timestamp
        })
    return history


# Uso
chat = model.start_chat()
chat.send_message("Hola")
chat.send_message("¿Cómo estás?")

for msg in get_chat_history(chat):
    role = "Usuario" if msg["role"] == "user" else "Asistente"
    print(f"{role}: {msg['content'][:50]}...")
```

### Persistir historial

```python
import json
from pathlib import Path


class PersistentChat:
    """Chat con historial persistente en disco."""

    def __init__(self, session_id: str, storage_dir: str = "./chat_sessions"):
        self.session_id = session_id
        self.storage_path = Path(storage_dir) / f"{session_id}.json"
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)

        # Cargar historial existente
        history = self._load_history()

        # Crear chat
        self.chat = model.start_chat(history=history)

    def _load_history(self) -> list:
        """Carga historial desde disco."""
        if not self.storage_path.exists():
            return []

        with open(self.storage_path) as f:
            data = json.load(f)

        history = []
        for msg in data.get("messages", []):
            history.append(ContentDict(
                role=msg["role"],
                parts=[msg["content"]]
            ))
        return history

    def _save_history(self):
        """Guarda historial a disco."""
        messages = []
        for msg in self.chat.history:
            messages.append({
                "role": msg.role,
                "content": msg.parts[0].text if msg.parts else ""
            })

        with open(self.storage_path, 'w') as f:
            json.dump({"messages": messages}, f, indent=2)

    def send(self, message: str) -> str:
        """Envía mensaje y guarda historial."""
        response = self.chat.send_message(message)
        self._save_history()
        return response.text

    def get_history(self) -> list:
        """Retorna historial actual."""
        return get_chat_history(self.chat)


# Uso
chat = PersistentChat("usuario_123")
response = chat.send("Hola, necesito ayuda con Python")
print(response)

# Más tarde, se puede retomar
chat2 = PersistentChat("usuario_123")  # Carga historial automáticamente
response = chat2.send("Continuando con lo anterior...")
```

### Limitar tamaño del historial

```python
def trim_history(chat: genai.ChatSession, max_turns: int = 10) -> genai.ChatSession:
    """
    Crea nuevo chat con historial recortado.

    Args:
        chat: Chat actual
        max_turns: Máximo de turnos a mantener (par user+model = 1 turno)

    Returns:
        Nuevo chat con historial recortado
    """
    history = chat.history

    # Cada "turno" es un par user + model
    max_messages = max_turns * 2

    if len(history) <= max_messages:
        return chat

    # Mantener solo los últimos mensajes
    trimmed = history[-max_messages:]

    return model.start_chat(history=trimmed)


# Uso
# Si el chat tiene mucho historial
if len(chat.history) > 20:
    chat = trim_history(chat, max_turns=10)
```

---

## Streaming en chat

```python
def send_with_streaming(chat: genai.ChatSession, message: str):
    """
    Envía mensaje con respuesta en streaming.

    Yields:
        Fragmentos de texto
    """
    response = chat.send_message(message, stream=True)

    full_response = ""
    for chunk in response:
        if chunk.text:
            full_response += chunk.text
            yield chunk.text

    # El historial se actualiza automáticamente


# Uso
chat = model.start_chat()

print("Asistente: ", end="")
for chunk in send_with_streaming(chat, "Cuéntame sobre Python"):
    print(chunk, end="", flush=True)
print()
```

---

## Manejo de errores en chat

```python
from google.api_core import exceptions


class RobustChat:
    """Chat con manejo de errores robusto."""

    def __init__(self, model_name: str = "gemini-1.5-flash"):
        self.model = genai.GenerativeModel(model_name)
        self.chat = None
        self.reset()

    def reset(self):
        """Reinicia el chat."""
        self.chat = self.model.start_chat(history=[])

    def send(self, message: str, max_retries: int = 3) -> str:
        """
        Envía mensaje con reintentos.
        """
        import time

        last_error = None
        for attempt in range(max_retries):
            try:
                response = self.chat.send_message(message)
                return response.text

            except exceptions.ResourceExhausted as e:
                last_error = e
                wait = 2 ** attempt
                print(f"Cuota excedida, esperando {wait}s...")
                time.sleep(wait)

            except exceptions.InvalidArgument as e:
                # Posible problema con el historial
                if "history" in str(e).lower():
                    print("Reiniciando chat por error de historial")
                    self.reset()
                    # Reintentar con chat limpio
                    return self.send(message, max_retries - 1)
                raise

            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                raise

        raise last_error

    def get_token_count(self) -> int:
        """Cuenta tokens en el historial actual."""
        if not self.chat.history:
            return 0

        # Concatenar todo el historial
        full_text = ""
        for msg in self.chat.history:
            if msg.parts:
                full_text += msg.parts[0].text + "\n"

        return self.model.count_tokens(full_text).total_tokens


# Uso
chat = RobustChat()
print(chat.send("Hola"))
print(f"Tokens en historial: {chat.get_token_count()}")
```

---

## Casos de prueba

```python
# test_chat_sessions.py
import pytest
import google.generativeai as genai
import os


@pytest.fixture
def model():
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    return genai.GenerativeModel("gemini-1.5-flash")


def test_basic_chat(model):
    """Test chat básico."""
    chat = model.start_chat()
    response = chat.send_message("Di 'hola'")
    assert response.text is not None
    assert len(chat.history) == 2  # user + model
    print("✓ Chat básico funciona")


def test_memory_retention(model):
    """Test que el chat recuerda contexto."""
    chat = model.start_chat()

    chat.send_message("Mi color favorito es el azul")
    response = chat.send_message("¿Cuál dije que era mi color favorito?")

    assert "azul" in response.text.lower()
    print("✓ Chat recuerda contexto")


def test_history_access(model):
    """Test acceso al historial."""
    chat = model.start_chat()
    chat.send_message("Mensaje 1")
    chat.send_message("Mensaje 2")

    assert len(chat.history) == 4  # 2 user + 2 model
    assert chat.history[0].role == "user"
    assert chat.history[1].role == "model"
    print(f"✓ Historial accesible: {len(chat.history)} mensajes")


def test_chat_with_initial_history(model):
    """Test chat con historial inicial."""
    from google.generativeai.types import ContentDict

    history = [
        ContentDict(role="user", parts=["Soy programador Python"]),
        ContentDict(role="model", parts=["¡Genial! ¿En qué puedo ayudarte?"]),
    ]

    chat = model.start_chat(history=history)
    response = chat.send_message("¿Qué lenguaje dije que uso?")

    assert "python" in response.text.lower()
    print("✓ Chat con historial inicial funciona")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## Resumen del concepto

**En una frase**: Los chat sessions mantienen el historial de la conversación automáticamente, permitiendo interacciones multi-turno con contexto.

**Patrón básico**:
```python
chat = model.start_chat()
response = chat.send_message("Primer mensaje")
response = chat.send_message("Segundo mensaje")  # Recuerda el primero
```

**Consideraciones**:
- El historial consume tokens del contexto
- Persistir historial para sesiones largas
- Recortar historial si crece demasiado

**Siguiente paso**: Tema 1.3.2 - System Instructions.
