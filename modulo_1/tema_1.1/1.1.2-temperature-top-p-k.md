# Configuración de Temperature, Top-P, Top-K

**Tiempo estimado**: 40 minutos
**Nivel**: Intermedio
**Prerrequisitos**: Generación de texto (0.1.2), generate_content (1.1.1)

## ¿Por qué importa este concepto?

Temperature, Top-P y Top-K son los "diales de control" de la creatividad del modelo. Ajustarlos correctamente es la diferencia entre un modelo que da respuestas robóticas repetitivas y uno que genera contenido creativo y variado - o entre uno que alucina y uno que es preciso.

---

## Los tres parámetros explicados

### Temperature: El dial de creatividad

```
Temperature = 0.0: Siempre elige el token más probable (determinístico)
Temperature = 1.0: Distribución original del modelo
Temperature = 2.0: Distribución más uniforme (más aleatorio)
```

**Efecto en la distribución**:

```
Logits originales: [gato: 5.0, perro: 3.0, mesa: 1.0]

Temperature = 0.1:
  gato: 99.9%    <- Casi siempre elige "gato"
  perro: 0.1%
  mesa: ~0%

Temperature = 1.0:
  gato: 72%      <- Probabilidades originales
  perro: 22%
  mesa: 6%

Temperature = 2.0:
  gato: 45%      <- Más uniforme
  perro: 35%
  mesa: 20%
```

### Top-K: Limitar el vocabulario

Top-K restringe el muestreo a los K tokens más probables.

```
Vocabulario completo: [gato, perro, mesa, silla, pez, ...]
                       72%   22%    3%    2%    0.5%

Top-K = 3:            [gato, perro, mesa] <- Solo considera estos 3
                       Re-normalizado: 74%, 23%, 3%
```

**Efecto**: Elimina tokens improbables que podrían causar incoherencia.

### Top-P (Nucleus Sampling): Vocabulario dinámico

Top-P selecciona el conjunto mínimo de tokens cuya probabilidad acumulada supera P.

```
Cuando el modelo está seguro:
  [gato: 95%, perro: 4%, mesa: 0.5%, ...]
  Top-P = 0.9 → Solo [gato] porque 95% > 90%

Cuando el modelo está incierto:
  [opción1: 30%, opción2: 25%, opción3: 20%, opción4: 15%, ...]
  Top-P = 0.9 → [opción1, opción2, opción3, opción4] para alcanzar 90%
```

**Ventaja sobre Top-K**: Se adapta a la certeza del modelo en cada paso.

---

## Implementación práctica

### Configuración con GenerationConfig

```python
import google.generativeai as genai
from google.generativeai import GenerationConfig
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])


def create_configured_model(
    temperature: float = 1.0,
    top_p: float = 0.95,
    top_k: int = 40,
    max_tokens: int = 1024
) -> genai.GenerativeModel:
    """
    Crea un modelo con configuración específica.

    Args:
        temperature: 0.0-2.0, controla creatividad
        top_p: 0.0-1.0, nucleus sampling threshold
        top_k: 1-100, número de tokens a considerar
        max_tokens: Límite de tokens de salida

    Returns:
        Modelo configurado
    """
    config = GenerationConfig(
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        max_output_tokens=max_tokens,
    )

    return genai.GenerativeModel(
        "gemini-1.5-flash",
        generation_config=config
    )


# Perfiles predefinidos para diferentes casos de uso
GENERATION_PROFILES = {
    "deterministic": GenerationConfig(
        temperature=0,
        top_k=1,
        max_output_tokens=2048,
    ),
    "factual": GenerationConfig(
        temperature=0.2,
        top_p=0.9,
        top_k=40,
        max_output_tokens=2048,
    ),
    "balanced": GenerationConfig(
        temperature=0.7,
        top_p=0.95,
        top_k=40,
        max_output_tokens=2048,
    ),
    "creative": GenerationConfig(
        temperature=1.0,
        top_p=0.95,
        top_k=100,
        max_output_tokens=4096,
    ),
    "very_creative": GenerationConfig(
        temperature=1.5,
        top_p=0.98,
        top_k=150,
        max_output_tokens=4096,
    ),
}


def get_model_for_task(task_type: str) -> genai.GenerativeModel:
    """
    Obtiene modelo con configuración apropiada para el tipo de tarea.

    Args:
        task_type: 'deterministic', 'factual', 'balanced', 'creative', 'very_creative'

    Returns:
        Modelo configurado
    """
    config = GENERATION_PROFILES.get(task_type, GENERATION_PROFILES["balanced"])
    return genai.GenerativeModel("gemini-1.5-flash", generation_config=config)
```

### Demostración de efectos

```python
def demonstrate_temperature_effect(prompt: str, n_samples: int = 3):
    """
    Muestra cómo la temperatura afecta las respuestas.
    """
    temperatures = [0.0, 0.5, 1.0, 1.5]

    print(f"Prompt: '{prompt}'\n")
    print("=" * 60)

    for temp in temperatures:
        print(f"\nTemperature = {temp}")
        print("-" * 40)

        model = create_configured_model(temperature=temp)

        for i in range(n_samples):
            response = model.generate_content(prompt)
            # Mostrar solo primeras 100 chars para comparar
            text = response.text[:100].replace('\n', ' ')
            print(f"  Sample {i+1}: {text}...")

    print("\n" + "=" * 60)


# Ejecutar demostración
demonstrate_temperature_effect(
    "Completa esta frase de forma creativa: El robot decidió que...",
    n_samples=2
)
```

### Análisis de variabilidad

```python
import hashlib
from collections import Counter


def measure_variability(
    prompt: str,
    config: GenerationConfig,
    n_samples: int = 10
) -> dict:
    """
    Mide la variabilidad de respuestas con una configuración dada.

    Returns:
        Dict con métricas de variabilidad
    """
    model = genai.GenerativeModel("gemini-1.5-flash", generation_config=config)

    responses = []
    hashes = []

    for _ in range(n_samples):
        response = model.generate_content(prompt)
        text = response.text.strip()
        responses.append(text)
        # Hash para detectar respuestas idénticas
        hashes.append(hashlib.md5(text.encode()).hexdigest()[:8])

    # Calcular métricas
    unique_hashes = set(hashes)
    hash_counts = Counter(hashes)
    most_common = hash_counts.most_common(1)[0][1]

    # Longitud promedio de respuestas
    avg_length = sum(len(r) for r in responses) / len(responses)

    return {
        "total_samples": n_samples,
        "unique_responses": len(unique_hashes),
        "uniqueness_ratio": len(unique_hashes) / n_samples,
        "most_repeated_count": most_common,
        "avg_response_length": avg_length,
        "sample_hashes": hashes,
    }


# Comparar variabilidad
def compare_configs():
    """Compara variabilidad entre configuraciones."""
    prompt = "Escribe una metáfora sobre el tiempo."

    configs = {
        "T=0 (deterministic)": GenerationConfig(temperature=0),
        "T=0.5 (factual)": GenerationConfig(temperature=0.5),
        "T=1.0 (balanced)": GenerationConfig(temperature=1.0),
        "T=1.5 (creative)": GenerationConfig(temperature=1.5),
    }

    print(f"Prompt: '{prompt}'")
    print(f"Samples por config: 5\n")

    for name, config in configs.items():
        result = measure_variability(prompt, config, n_samples=5)
        print(f"{name}:")
        print(f"  Respuestas únicas: {result['unique_responses']}/5")
        print(f"  Ratio de unicidad: {result['uniqueness_ratio']:.0%}")
        print()


# compare_configs()
```

---

## Guía de selección de parámetros

### Matriz de recomendaciones

| Tarea | Temperature | Top-P | Top-K | Justificación |
|-------|-------------|-------|-------|---------------|
| Extracción de datos | 0 | - | 1 | Una sola respuesta correcta |
| Clasificación | 0-0.1 | 0.9 | 10 | Respuesta debe ser una categoría |
| Código | 0.1-0.3 | 0.95 | 40 | Sintaxis estricta, algo de variación |
| Q&A factual | 0.3-0.5 | 0.9 | 40 | Preciso pero natural |
| Resumen | 0.3-0.5 | 0.9 | 40 | Fiel al contenido |
| Traducción | 0.3-0.5 | 0.95 | 40 | Consistente pero fluido |
| Chat general | 0.7-0.9 | 0.95 | 60 | Natural y variado |
| Brainstorming | 1.0-1.2 | 0.98 | 100 | Máxima creatividad |
| Escritura creativa | 1.0-1.5 | 0.98 | 100+ | Originalidad importante |
| Poesía/Arte | 1.2-1.8 | 0.99 | 150 | Máxima expresividad |

### Función de selección automática

```python
from enum import Enum
from dataclasses import dataclass


class TaskType(Enum):
    EXTRACTION = "extraction"
    CLASSIFICATION = "classification"
    CODE = "code"
    FACTUAL_QA = "factual_qa"
    SUMMARIZATION = "summarization"
    TRANSLATION = "translation"
    CHAT = "chat"
    BRAINSTORMING = "brainstorming"
    CREATIVE_WRITING = "creative_writing"
    POETRY = "poetry"


@dataclass
class GenerationParams:
    """Parámetros de generación recomendados."""
    temperature: float
    top_p: float
    top_k: int
    description: str


TASK_PARAMS: dict[TaskType, GenerationParams] = {
    TaskType.EXTRACTION: GenerationParams(
        temperature=0,
        top_p=1.0,
        top_k=1,
        description="Determinístico, una respuesta correcta"
    ),
    TaskType.CLASSIFICATION: GenerationParams(
        temperature=0.1,
        top_p=0.9,
        top_k=10,
        description="Muy bajo, respuesta de categoría"
    ),
    TaskType.CODE: GenerationParams(
        temperature=0.2,
        top_p=0.95,
        top_k=40,
        description="Bajo, sintaxis estricta"
    ),
    TaskType.FACTUAL_QA: GenerationParams(
        temperature=0.4,
        top_p=0.9,
        top_k=40,
        description="Bajo-medio, preciso pero natural"
    ),
    TaskType.SUMMARIZATION: GenerationParams(
        temperature=0.4,
        top_p=0.9,
        top_k=40,
        description="Bajo-medio, fiel al contenido"
    ),
    TaskType.TRANSLATION: GenerationParams(
        temperature=0.3,
        top_p=0.95,
        top_k=40,
        description="Bajo, consistente pero fluido"
    ),
    TaskType.CHAT: GenerationParams(
        temperature=0.8,
        top_p=0.95,
        top_k=60,
        description="Medio-alto, natural y variado"
    ),
    TaskType.BRAINSTORMING: GenerationParams(
        temperature=1.1,
        top_p=0.98,
        top_k=100,
        description="Alto, máxima creatividad"
    ),
    TaskType.CREATIVE_WRITING: GenerationParams(
        temperature=1.2,
        top_p=0.98,
        top_k=100,
        description="Alto, originalidad importante"
    ),
    TaskType.POETRY: GenerationParams(
        temperature=1.5,
        top_p=0.99,
        top_k=150,
        description="Muy alto, máxima expresividad"
    ),
}


def get_config_for_task(task: TaskType) -> GenerationConfig:
    """
    Obtiene configuración óptima para un tipo de tarea.

    Args:
        task: Tipo de tarea

    Returns:
        GenerationConfig configurado
    """
    params = TASK_PARAMS[task]

    return GenerationConfig(
        temperature=params.temperature,
        top_p=params.top_p,
        top_k=params.top_k,
        max_output_tokens=2048,
    )


def recommend_params(task_description: str) -> GenerationParams:
    """
    Recomienda parámetros basándose en descripción de tarea.

    Usa heurísticas simples para sugerir configuración.
    """
    description = task_description.lower()

    # Detección por keywords
    if any(w in description for w in ["extraer", "extract", "parse", "json"]):
        return TASK_PARAMS[TaskType.EXTRACTION]

    if any(w in description for w in ["clasificar", "classify", "categorize"]):
        return TASK_PARAMS[TaskType.CLASSIFICATION]

    if any(w in description for w in ["código", "code", "programa", "function"]):
        return TASK_PARAMS[TaskType.CODE]

    if any(w in description for w in ["traducir", "translate"]):
        return TASK_PARAMS[TaskType.TRANSLATION]

    if any(w in description for w in ["resumir", "summarize", "resumen"]):
        return TASK_PARAMS[TaskType.SUMMARIZATION]

    if any(w in description for w in ["creativ", "historia", "story", "poem"]):
        return TASK_PARAMS[TaskType.CREATIVE_WRITING]

    if any(w in description for w in ["ideas", "brainstorm", "lluvia"]):
        return TASK_PARAMS[TaskType.BRAINSTORMING]

    # Default: chat general
    return TASK_PARAMS[TaskType.CHAT]


# Uso
task = "Necesito extraer nombres de personas de este texto"
params = recommend_params(task)
print(f"Para '{task}':")
print(f"  Temperature: {params.temperature}")
print(f"  Top-P: {params.top_p}")
print(f"  Top-K: {params.top_k}")
print(f"  Razón: {params.description}")
```

---

## Casos de prueba

```python
# test_generation_params.py
import pytest
import google.generativeai as genai
from google.generativeai import GenerationConfig
import os


@pytest.fixture
def api_configured():
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    return True


def test_temperature_zero_is_deterministic(api_configured):
    """Con T=0, misma entrada produce misma salida."""
    config = GenerationConfig(temperature=0)
    model = genai.GenerativeModel("gemini-1.5-flash", generation_config=config)

    prompt = "¿Cuánto es 2+2? Responde solo el número."

    responses = []
    for _ in range(3):
        r = model.generate_content(prompt)
        responses.append(r.text.strip())

    # Todas deberían ser iguales
    assert len(set(responses)) == 1, f"Respuestas diferentes: {responses}"
    print(f"✓ T=0 determinístico: '{responses[0]}'")


def test_high_temperature_produces_variation(api_configured):
    """Con T alto, esperamos variación entre respuestas."""
    config = GenerationConfig(temperature=1.5, max_output_tokens=50)
    model = genai.GenerativeModel("gemini-1.5-flash", generation_config=config)

    prompt = "Inventa un nombre para un robot."

    responses = set()
    for _ in range(5):
        r = model.generate_content(prompt)
        responses.add(r.text.strip()[:30])  # Primeros 30 chars

    # Esperamos al menos algunas diferentes
    assert len(responses) >= 2, f"Muy poca variación: {responses}"
    print(f"✓ T=1.5 variable: {len(responses)} respuestas únicas de 5")


def test_top_k_limits_vocabulary(api_configured):
    """Top-K bajo debe producir respuestas más conservadoras."""
    prompt = "Completa: El color del cielo es"

    # Top-K muy bajo
    config_low = GenerationConfig(temperature=0.5, top_k=3)
    model_low = genai.GenerativeModel("gemini-1.5-flash", generation_config=config_low)

    # Top-K alto
    config_high = GenerationConfig(temperature=0.5, top_k=100)
    model_high = genai.GenerativeModel("gemini-1.5-flash", generation_config=config_high)

    # Ambos deberían funcionar
    r_low = model_low.generate_content(prompt)
    r_high = model_high.generate_content(prompt)

    assert r_low.text is not None
    assert r_high.text is not None
    print(f"✓ Top-K bajo: {r_low.text[:30]}")
    print(f"✓ Top-K alto: {r_high.text[:30]}")


def test_config_profiles():
    """Verifica que los perfiles estén bien definidos."""
    for name, config in GENERATION_PROFILES.items():
        assert config.temperature is not None
        assert 0 <= config.temperature <= 2
        print(f"✓ Perfil '{name}': T={config.temperature}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## Errores comunes

### Error 1: Temperature muy alta para tareas precisas

```python
# ❌ Malo: temperatura alta para extracción
config = GenerationConfig(temperature=1.5)
response = model.generate_content(
    "Extrae el email de este texto: Contacta a juan@empresa.com",
    generation_config=config
)
# Puede inventar emails o dar respuestas divagantes
```

**Solución**: Usar T=0 para extracción.

### Error 2: Temperature 0 para tareas creativas

```python
# ❌ Malo: respuestas siempre idénticas
config = GenerationConfig(temperature=0)
for _ in range(5):
    response = model.generate_content(
        "Escribe un slogan creativo para una cafetería"
    )
    # Siempre el mismo slogan aburrido
```

**Solución**: Usar T=1.0+ para creatividad.

---

## Resumen del concepto

**En una frase**: Temperature controla la creatividad, Top-P adapta el vocabulario a la certeza del modelo, y Top-K pone un límite duro al vocabulario.

**Regla práctica**:
- **Tareas precisas**: T=0-0.3
- **Tareas balanceadas**: T=0.5-0.8
- **Tareas creativas**: T=1.0-1.5

**Siguiente paso**: Tema 1.1.3 - Tokens, Límites y Costos.
