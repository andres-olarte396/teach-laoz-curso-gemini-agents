# Métodos generate_content y stream

**Tiempo estimado**: 45 minutos
**Nivel**: Intermedio
**Prerrequisitos**: Primera interacción con Gemini (0.2.3)

## ¿Por qué importa este concepto?

`generate_content()` es el método principal para interactuar con Gemini. Dominar sus opciones te permite controlar completamente el comportamiento del modelo: desde respuestas simples hasta generación en streaming para interfaces en tiempo real.

---

## generate_content: El método fundamental

```python
import google.generativeai as genai
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-1.5-flash")

# Uso más simple
response = model.generate_content("¿Qué es Python?")
print(response.text)
```

### Firma completa del método

```python
def generate_content(
    contents,                    # Prompt o lista de contenidos
    generation_config=None,      # Configuración de generación
    safety_settings=None,        # Filtros de seguridad
    stream=False,                # Activar streaming
    tools=None,                  # Herramientas/funciones
    tool_config=None,            # Configuración de herramientas
    request_options=None,        # Opciones de la request HTTP
) -> GenerateContentResponse:
    ...
```

---

## Tipos de contenido de entrada

### 1. String simple

```python
response = model.generate_content("Hola, ¿cómo estás?")
```

### 2. Lista de partes (multimodal)

```python
from PIL import Image

# Texto + imagen
image = Image.open("foto.jpg")
response = model.generate_content([
    "Describe esta imagen:",
    image
])
```

### 3. Contenido estructurado

```python
from google.generativeai.types import ContentDict

# Para conversaciones o contenido complejo
contents = [
    ContentDict(
        role="user",
        parts=["Primera pregunta"]
    ),
    ContentDict(
        role="model",
        parts=["Respuesta del modelo"]
    ),
    ContentDict(
        role="user",
        parts=["Pregunta de seguimiento"]
    ),
]

response = model.generate_content(contents)
```

---

## Streaming: Respuestas en tiempo real

El streaming es crucial para interfaces de usuario responsivas. En lugar de esperar la respuesta completa, recibes fragmentos mientras se generan.

### Comparación de latencia

```
Sin streaming:
[Usuario envía] -------- 3 segundos -------- [Respuesta completa aparece]

Con streaming:
[Usuario envía] -- [chunk1] [chunk2] [chunk3] ... [chunkN]
                   100ms    200ms    300ms        3s (total)
```

### Implementación básica de streaming

```python
def generate_with_streaming(prompt: str):
    """Genera respuesta con streaming."""
    model = genai.GenerativeModel("gemini-1.5-flash")

    response = model.generate_content(prompt, stream=True)

    full_response = ""
    for chunk in response:
        # Cada chunk contiene texto incremental
        print(chunk.text, end="", flush=True)
        full_response += chunk.text

    print()  # Nueva línea al final
    return full_response


# Uso
text = generate_with_streaming("Explica la fotosíntesis en detalle")
```

### Streaming con manejo de errores

```python
import time
from typing import Generator, Optional
from google.api_core import exceptions


def robust_streaming(
    model: genai.GenerativeModel,
    prompt: str,
    timeout: float = 30.0
) -> Generator[str, None, None]:
    """
    Streaming robusto con timeout y manejo de errores.

    Args:
        model: Instancia del modelo
        prompt: Texto del prompt
        timeout: Timeout total en segundos

    Yields:
        Fragmentos de texto
    """
    start_time = time.time()

    try:
        response = model.generate_content(prompt, stream=True)

        for chunk in response:
            # Verificar timeout
            if time.time() - start_time > timeout:
                raise TimeoutError(f"Streaming excedió {timeout}s")

            if chunk.text:
                yield chunk.text

    except exceptions.ResourceExhausted:
        yield "\n[Error: Cuota excedida. Reintenta más tarde.]"
    except exceptions.InvalidArgument as e:
        yield f"\n[Error: Argumento inválido - {e}]"
    except TimeoutError as e:
        yield f"\n[{e}]"
    except Exception as e:
        yield f"\n[Error inesperado: {type(e).__name__}]"


# Uso
model = genai.GenerativeModel("gemini-1.5-flash")
for chunk in robust_streaming(model, "Escribe un cuento corto"):
    print(chunk, end="", flush=True)
```

### Streaming asíncrono

```python
import asyncio
import google.generativeai as genai


async def async_streaming(prompt: str) -> str:
    """
    Streaming asíncrono para aplicaciones async.

    Nota: El SDK actual no tiene soporte nativo async,
    pero podemos envolver la llamada síncrona.
    """
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Ejecutar en thread pool para no bloquear
    loop = asyncio.get_event_loop()

    def sync_generate():
        response = model.generate_content(prompt, stream=True)
        chunks = []
        for chunk in response:
            chunks.append(chunk.text)
        return "".join(chunks)

    result = await loop.run_in_executor(None, sync_generate)
    return result


# Uso
async def main():
    result = await async_streaming("¿Qué es async/await?")
    print(result)


# asyncio.run(main())
```

---

## Procesamiento de respuestas

### Estructura de GenerateContentResponse

```python
response = model.generate_content("Test")

# Atributos principales
response.text              # Texto de la respuesta (shortcut)
response.candidates        # Lista de respuestas candidatas
response.prompt_feedback   # Feedback sobre el prompt
response.usage_metadata    # Información de uso de tokens

# Acceso detallado a candidatos
candidate = response.candidates[0]
candidate.content          # Contenido estructurado
candidate.finish_reason    # Por qué terminó (STOP, MAX_TOKENS, SAFETY, etc.)
candidate.safety_ratings   # Calificaciones de seguridad
candidate.citation_metadata # Citas si las hay
```

### Manejo de múltiples candidatos

```python
from google.generativeai import GenerationConfig

def generate_multiple_candidates(prompt: str, n: int = 3) -> list:
    """
    Genera múltiples respuestas candidatas.

    Útil para:
    - Obtener variaciones
    - Seleccionar la mejor respuesta
    - A/B testing de prompts
    """
    model = genai.GenerativeModel("gemini-1.5-flash")

    config = GenerationConfig(
        candidate_count=n,
        temperature=1.0,  # Más variación entre candidatos
    )

    response = model.generate_content(
        prompt,
        generation_config=config
    )

    candidates = []
    for i, candidate in enumerate(response.candidates):
        candidates.append({
            "index": i,
            "text": candidate.content.parts[0].text,
            "finish_reason": candidate.finish_reason.name,
        })

    return candidates


# Uso
results = generate_multiple_candidates("Escribe un slogan para una app de fitness")
for r in results:
    print(f"Candidato {r['index']}: {r['text']}")
```

### Verificar razón de finalización

```python
from google.generativeai.types import FinishReason


def check_completion(response) -> dict:
    """
    Verifica cómo terminó la generación.

    Returns:
        Dict con estado y mensaje
    """
    if not response.candidates:
        return {
            "ok": False,
            "reason": "NO_CANDIDATES",
            "message": "No se generaron candidatos"
        }

    candidate = response.candidates[0]
    reason = candidate.finish_reason

    status = {
        FinishReason.STOP: {
            "ok": True,
            "message": "Completado normalmente"
        },
        FinishReason.MAX_TOKENS: {
            "ok": False,
            "message": "Respuesta truncada por límite de tokens"
        },
        FinishReason.SAFETY: {
            "ok": False,
            "message": "Bloqueado por filtros de seguridad"
        },
        FinishReason.RECITATION: {
            "ok": False,
            "message": "Bloqueado por posible recitación"
        },
        FinishReason.OTHER: {
            "ok": False,
            "message": "Terminado por razón no especificada"
        },
    }

    result = status.get(reason, {"ok": False, "message": f"Razón desconocida: {reason}"})
    result["reason"] = reason.name

    return result


# Uso
response = model.generate_content("Test")
status = check_completion(response)
if not status["ok"]:
    print(f"Advertencia: {status['message']}")
```

---

## Patrones comunes

### Wrapper reutilizable

```python
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
import google.generativeai as genai
from google.generativeai import GenerationConfig


@dataclass
class GenerationResult:
    """Resultado estructurado de generación."""
    text: str
    tokens_in: int
    tokens_out: int
    finish_reason: str
    is_complete: bool
    raw_response: Any = None


class GeminiClient:
    """
    Cliente wrapper para Gemini con configuración sensata.
    """

    def __init__(
        self,
        model_name: str = "gemini-1.5-flash",
        api_key: Optional[str] = None,
        default_temperature: float = 0.7,
        default_max_tokens: int = 2048,
    ):
        import os
        key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not key:
            raise ValueError("API key requerida")

        genai.configure(api_key=key)

        self.model_name = model_name
        self.default_config = GenerationConfig(
            temperature=default_temperature,
            max_output_tokens=default_max_tokens,
            top_p=0.95,
        )

        self._model = None

    @property
    def model(self) -> genai.GenerativeModel:
        """Lazy loading del modelo."""
        if self._model is None:
            self._model = genai.GenerativeModel(
                self.model_name,
                generation_config=self.default_config
            )
        return self._model

    def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_instruction: Optional[str] = None,
    ) -> GenerationResult:
        """
        Genera contenido con configuración opcional.

        Args:
            prompt: Texto del prompt
            temperature: Override de temperatura
            max_tokens: Override de max tokens
            system_instruction: Instrucción de sistema

        Returns:
            GenerationResult estructurado
        """
        # Crear config con overrides
        config = GenerationConfig(
            temperature=temperature or self.default_config.temperature,
            max_output_tokens=max_tokens or self.default_config.max_output_tokens,
            top_p=self.default_config.top_p,
        )

        # Crear modelo con system instruction si se proporciona
        if system_instruction:
            model = genai.GenerativeModel(
                self.model_name,
                generation_config=config,
                system_instruction=system_instruction
            )
        else:
            model = self.model

        # Generar
        response = model.generate_content(prompt, generation_config=config)

        # Estructurar resultado
        candidate = response.candidates[0] if response.candidates else None
        is_complete = (
            candidate and
            candidate.finish_reason.name == "STOP"
        )

        return GenerationResult(
            text=response.text if response.text else "",
            tokens_in=response.usage_metadata.prompt_token_count,
            tokens_out=response.usage_metadata.candidates_token_count,
            finish_reason=candidate.finish_reason.name if candidate else "UNKNOWN",
            is_complete=is_complete,
            raw_response=response
        )

    def stream(self, prompt: str, **kwargs):
        """
        Genera con streaming.

        Yields:
            Fragmentos de texto
        """
        model = self.model
        response = model.generate_content(prompt, stream=True, **kwargs)

        for chunk in response:
            if chunk.text:
                yield chunk.text


# Uso del cliente
client = GeminiClient(
    model_name="gemini-1.5-flash",
    default_temperature=0.5
)

# Generación simple
result = client.generate("¿Qué es machine learning?")
print(f"Respuesta: {result.text}")
print(f"Tokens: {result.tokens_in} in, {result.tokens_out} out")
print(f"Completo: {result.is_complete}")

# Con streaming
for chunk in client.stream("Cuenta una historia corta"):
    print(chunk, end="", flush=True)
```

---

## Casos de prueba

```python
# test_generate_content.py
import pytest
import google.generativeai as genai
import os


@pytest.fixture
def model():
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    return genai.GenerativeModel("gemini-1.5-flash")


def test_basic_generate(model):
    """Test generación básica."""
    response = model.generate_content("Di 'ok'")
    assert response.text is not None
    assert len(response.text) > 0
    print(f"✓ Básico: {response.text[:20]}")


def test_streaming(model):
    """Test de streaming."""
    response = model.generate_content("Cuenta 1,2,3", stream=True)
    chunks = list(response)
    assert len(chunks) > 0
    full = "".join(c.text for c in chunks if c.text)
    assert "1" in full and "2" in full and "3" in full
    print(f"✓ Streaming: {len(chunks)} chunks")


def test_usage_metadata(model):
    """Test de metadata de uso."""
    response = model.generate_content("Hola")
    usage = response.usage_metadata
    assert usage.prompt_token_count > 0
    assert usage.candidates_token_count > 0
    print(f"✓ Tokens: {usage.total_token_count}")


def test_finish_reason(model):
    """Test de razón de finalización."""
    response = model.generate_content("Di hola")
    assert response.candidates[0].finish_reason.name == "STOP"
    print("✓ Finish reason: STOP")


def test_max_tokens_truncation(model):
    """Test de truncamiento por max tokens."""
    from google.generativeai import GenerationConfig

    config = GenerationConfig(max_output_tokens=5)
    response = model.generate_content(
        "Escribe un párrafo largo sobre la historia de la computación",
        generation_config=config
    )
    # Debería truncar
    assert response.candidates[0].finish_reason.name in ["STOP", "MAX_TOKENS"]
    print(f"✓ Max tokens: {len(response.text.split())} palabras")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## Resumen del concepto

**En una frase**: `generate_content()` es el punto de entrada principal a Gemini; `stream=True` permite respuestas en tiempo real.

**Cuándo usar streaming**:
- Interfaces de chat en tiempo real
- Generación de texto largo
- Cuando la latencia percibida importa

**Cuándo NO usar streaming**:
- Procesamiento batch
- Cuando necesitas la respuesta completa antes de continuar
- APIs síncronas simples

**Siguiente paso**: Tema 1.1.2 - Configuración de Temperature, Top-P, Top-K para controlar la generación.
