# Tokens, L√≠mites y Costos

**Tiempo estimado**: 35 minutos
**Nivel**: Intermedio
**Prerrequisitos**: generate_content (1.1.1)

## ¬øPor qu√© importa este concepto?

Entender tokens es esencial para:
- **Estimar costos** antes de ejecutar
- **Evitar errores** por exceder l√≠mites de contexto
- **Optimizar prompts** para m√°xima eficiencia
- **Dise√±ar sistemas** que escalen econ√≥micamente

---

## ¬øQu√© es un token?

Un token es la unidad m√≠nima de texto que procesa el modelo. No es exactamente una palabra:

```
"Hola mundo" ‚Üí ["Hola", " mundo"] ‚Üí 2 tokens
"fotos√≠ntesis" ‚Üí ["foto", "s√≠ntesis"] ‚Üí 2 tokens
"Hello" ‚Üí ["Hello"] ‚Üí 1 token
"¬°¬°¬°" ‚Üí ["¬°", "¬°", "¬°"] ‚Üí 3 tokens
```

**Regla aproximada**: 1 token ‚âà 4 caracteres en ingl√©s, ‚âà 3 caracteres en espa√±ol.

---

## Conteo de tokens con Gemini

```python
import google.generativeai as genai
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-1.5-flash")


def count_tokens(text: str) -> dict:
    """
    Cuenta tokens en un texto.

    Returns:
        Dict con conteo y estimaciones
    """
    result = model.count_tokens(text)

    # Estimaci√≥n de costo (precios de Gemini 1.5 Flash)
    input_cost_per_1k = 0.000075  # $0.000075 por 1K tokens input

    return {
        "tokens": result.total_tokens,
        "characters": len(text),
        "ratio": len(text) / result.total_tokens if result.total_tokens > 0 else 0,
        "estimated_cost_input": (result.total_tokens / 1000) * input_cost_per_1k,
    }


# Ejemplos
texts = [
    "Hola mundo",
    "The quick brown fox jumps over the lazy dog",
    "def fibonacci(n): return n if n < 2 else fibonacci(n-1) + fibonacci(n-2)",
    "üöÄüéâüî•",  # Emojis
]

for text in texts:
    info = count_tokens(text)
    print(f"'{text[:30]}...' ‚Üí {info['tokens']} tokens ({info['ratio']:.1f} chars/token)")
```

### Conteo antes de enviar

```python
def safe_generate(model, prompt: str, max_context: int = 1_000_000) -> str:
    """
    Genera contenido verificando l√≠mites primero.

    Args:
        model: Instancia del modelo
        prompt: Texto del prompt
        max_context: L√≠mite de tokens del modelo

    Returns:
        Texto generado

    Raises:
        ValueError: Si excede el l√≠mite
    """
    # Contar tokens
    token_count = model.count_tokens(prompt).total_tokens

    if token_count > max_context * 0.9:  # 90% del l√≠mite como margen
        raise ValueError(
            f"Prompt tiene {token_count} tokens, "
            f"muy cerca del l√≠mite de {max_context}"
        )

    response = model.generate_content(prompt)
    return response.text
```

---

## L√≠mites por modelo

| Modelo | Input m√°ximo | Output m√°ximo | Contexto total |
|--------|--------------|---------------|----------------|
| Gemini 1.5 Pro | 2M tokens | 8K tokens | 2M tokens |
| Gemini 1.5 Flash | 1M tokens | 8K tokens | 1M tokens |
| Gemini 1.0 Pro | 32K tokens | 8K tokens | 32K tokens |

### Verificar l√≠mites program√°ticamente

```python
def get_model_limits(model_name: str) -> dict:
    """Obtiene l√≠mites del modelo."""
    for model in genai.list_models():
        if model_name in model.name:
            return {
                "name": model.name,
                "input_token_limit": model.input_token_limit,
                "output_token_limit": model.output_token_limit,
            }
    return None


limits = get_model_limits("gemini-1.5-flash")
print(f"Input m√°ximo: {limits['input_token_limit']:,} tokens")
print(f"Output m√°ximo: {limits['output_token_limit']:,} tokens")
```

---

## Estimaci√≥n de costos

### Precios actuales (verificar pricing oficial)

| Modelo | Input (por 1M tokens) | Output (por 1M tokens) |
|--------|----------------------|------------------------|
| Gemini 1.5 Pro | $1.25 (‚â§128K) / $2.50 (>128K) | $5.00 (‚â§128K) / $10.00 (>128K) |
| Gemini 1.5 Flash | $0.075 (‚â§128K) / $0.15 (>128K) | $0.30 (‚â§128K) / $0.60 (>128K) |

### Calculadora de costos

```python
from dataclasses import dataclass
from typing import Optional


@dataclass
class TokenUsage:
    """Uso de tokens de una llamada."""
    input_tokens: int
    output_tokens: int
    model: str


@dataclass
class CostEstimate:
    """Estimaci√≥n de costo."""
    input_cost: float
    output_cost: float
    total_cost: float
    currency: str = "USD"


class CostCalculator:
    """Calculador de costos de API."""

    # Precios por mill√≥n de tokens (actualizar seg√∫n pricing oficial)
    PRICING = {
        "gemini-1.5-pro": {
            "input_short": 1.25,    # ‚â§128K contexto
            "input_long": 2.50,     # >128K contexto
            "output_short": 5.00,
            "output_long": 10.00,
            "context_threshold": 128_000,
        },
        "gemini-1.5-flash": {
            "input_short": 0.075,
            "input_long": 0.15,
            "output_short": 0.30,
            "output_long": 0.60,
            "context_threshold": 128_000,
        },
    }

    @classmethod
    def estimate(
        cls,
        usage: TokenUsage,
        context_length: Optional[int] = None
    ) -> CostEstimate:
        """
        Estima el costo de una llamada.

        Args:
            usage: Tokens usados
            context_length: Longitud total del contexto (para pricing escalonado)

        Returns:
            Estimaci√≥n de costo
        """
        model_key = None
        for key in cls.PRICING.keys():
            if key in usage.model.lower():
                model_key = key
                break

        if not model_key:
            # Default a Flash si no se reconoce
            model_key = "gemini-1.5-flash"

        pricing = cls.PRICING[model_key]
        threshold = pricing["context_threshold"]

        # Determinar si es contexto largo o corto
        ctx = context_length or usage.input_tokens
        is_long = ctx > threshold

        input_rate = pricing["input_long"] if is_long else pricing["input_short"]
        output_rate = pricing["output_long"] if is_long else pricing["output_short"]

        # Calcular costos (rates son por mill√≥n de tokens)
        input_cost = (usage.input_tokens / 1_000_000) * input_rate
        output_cost = (usage.output_tokens / 1_000_000) * output_rate

        return CostEstimate(
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=input_cost + output_cost,
        )

    @classmethod
    def estimate_batch(
        cls,
        model: str,
        prompts: list,
        avg_output_tokens: int = 500
    ) -> CostEstimate:
        """
        Estima costo de un batch de prompts.
        """
        genai_model = genai.GenerativeModel(model)

        total_input = 0
        for prompt in prompts:
            tokens = genai_model.count_tokens(prompt).total_tokens
            total_input += tokens

        total_output = len(prompts) * avg_output_tokens

        usage = TokenUsage(
            input_tokens=total_input,
            output_tokens=total_output,
            model=model
        )

        return cls.estimate(usage)


# Uso
usage = TokenUsage(
    input_tokens=10000,
    output_tokens=500,
    model="gemini-1.5-flash"
)

cost = CostCalculator.estimate(usage)
print(f"Input: ${cost.input_cost:.6f}")
print(f"Output: ${cost.output_cost:.6f}")
print(f"Total: ${cost.total_cost:.6f}")
```

---

## Optimizaci√≥n de tokens

### T√©cnicas para reducir uso

```python
def optimize_prompt(prompt: str) -> str:
    """
    Optimiza un prompt para usar menos tokens.
    """
    optimizations = []

    # 1. Eliminar espacios redundantes
    import re
    optimized = re.sub(r'\s+', ' ', prompt)
    if len(optimized) < len(prompt):
        optimizations.append("espacios reducidos")

    # 2. Eliminar instrucciones redundantes
    redundant_phrases = [
        "por favor",
        "podr√≠as",
        "me gustar√≠a que",
        "ser√≠a genial si",
    ]
    for phrase in redundant_phrases:
        if phrase in optimized.lower():
            optimizations.append(f"'{phrase}' es redundante")

    # 3. Usar abreviaciones en system prompts
    abbreviations = {
        "responde en espa√±ol": "ES",
        "formato JSON": "JSON",
        "lista con vi√±etas": "bullets",
    }

    return optimized, optimizations


# Comparar versiones
verbose_prompt = """
Por favor, me gustar√≠a que pudieras ayudarme a escribir un resumen
del siguiente texto. Ser√≠a genial si pudieras hacerlo en espa√±ol
y en formato de lista con vi√±etas. El texto es el siguiente:
[TEXTO AQU√ç]
"""

concise_prompt = """
Resume en ES, formato bullets:
[TEXTO AQU√ç]
"""

model = genai.GenerativeModel("gemini-1.5-flash")
tokens_verbose = model.count_tokens(verbose_prompt).total_tokens
tokens_concise = model.count_tokens(concise_prompt).total_tokens

print(f"Verbose: {tokens_verbose} tokens")
print(f"Conciso: {tokens_concise} tokens")
print(f"Ahorro: {tokens_verbose - tokens_concise} tokens ({(1 - tokens_concise/tokens_verbose)*100:.0f}%)")
```

### Truncamiento inteligente

```python
def truncate_to_token_limit(
    text: str,
    max_tokens: int,
    model: genai.GenerativeModel,
    strategy: str = "end"
) -> str:
    """
    Trunca texto para no exceder l√≠mite de tokens.

    Args:
        text: Texto a truncar
        max_tokens: L√≠mite de tokens
        model: Modelo para contar tokens
        strategy: "start" (mantener inicio), "end" (mantener final), "middle" (mantener ambos extremos)

    Returns:
        Texto truncado
    """
    current_tokens = model.count_tokens(text).total_tokens

    if current_tokens <= max_tokens:
        return text

    # B√∫squeda binaria para encontrar el punto de corte
    chars = list(text)
    low, high = 0, len(chars)

    while low < high:
        mid = (low + high) // 2
        if strategy == "start":
            test_text = text[:mid]
        elif strategy == "end":
            test_text = text[-mid:]
        else:  # middle
            half = mid // 2
            test_text = text[:half] + "\n...\n" + text[-half:]

        tokens = model.count_tokens(test_text).total_tokens

        if tokens <= max_tokens:
            low = mid + 1
        else:
            high = mid

    # Retornar con margen de seguridad
    final_length = low - 10
    if strategy == "start":
        return text[:final_length] + "..."
    elif strategy == "end":
        return "..." + text[-final_length:]
    else:
        half = final_length // 2
        return text[:half] + "\n[...truncado...]\n" + text[-half:]
```

---

## Monitoreo de uso

```python
from datetime import datetime
from typing import List
import json


class UsageTracker:
    """Rastrea uso de tokens y costos."""

    def __init__(self):
        self.calls: List[dict] = []

    def log_call(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        metadata: dict = None
    ):
        """Registra una llamada a la API."""
        usage = TokenUsage(input_tokens, output_tokens, model)
        cost = CostCalculator.estimate(usage)

        self.calls.append({
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost_usd": cost.total_cost,
            "metadata": metadata or {},
        })

    def get_summary(self) -> dict:
        """Obtiene resumen de uso."""
        if not self.calls:
            return {"total_calls": 0}

        total_input = sum(c["input_tokens"] for c in self.calls)
        total_output = sum(c["output_tokens"] for c in self.calls)
        total_cost = sum(c["cost_usd"] for c in self.calls)

        return {
            "total_calls": len(self.calls),
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "total_tokens": total_input + total_output,
            "total_cost_usd": total_cost,
            "avg_input_per_call": total_input / len(self.calls),
            "avg_cost_per_call": total_cost / len(self.calls),
        }

    def export(self, filepath: str):
        """Exporta registro a JSON."""
        with open(filepath, 'w') as f:
            json.dump({
                "calls": self.calls,
                "summary": self.get_summary()
            }, f, indent=2)


# Uso
tracker = UsageTracker()

# Despu√©s de cada llamada a la API
response = model.generate_content("Test")
tracker.log_call(
    model="gemini-1.5-flash",
    input_tokens=response.usage_metadata.prompt_token_count,
    output_tokens=response.usage_metadata.candidates_token_count,
    metadata={"task": "test"}
)

print(tracker.get_summary())
```

---

## Resumen del concepto

**En una frase**: Los tokens son la moneda de los LLMs - cada palabra cuesta, y entender ese costo te permite construir sistemas eficientes y econ√≥micos.

**Reglas pr√°cticas**:
- 1 token ‚âà 4 caracteres (ingl√©s) o 3 caracteres (espa√±ol)
- Siempre verifica tokens antes de enviar prompts largos
- Flash es ~17x m√°s barato que Pro para la misma tarea
- Monitorea uso para evitar sorpresas en facturaci√≥n

**Siguiente paso**: Tema 1.2.1 - An√°lisis de Im√°genes con Gemini.
