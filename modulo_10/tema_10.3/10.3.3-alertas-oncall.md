# 10.3.3 Alertas y On-Call

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar sistemas de alertas inteligentes para agentes de IA, configurar escalamiento de incidentes y establecer prácticas de on-call efectivas.

## Introducción

Los agentes en producción requieren monitoreo activo con alertas que identifiquen problemas antes de que afecten a los usuarios. Un buen sistema de alertas distingue entre ruido y señales reales, y escala incidentes apropiadamente.

```
┌─────────────────────────────────────────────────────────────────┐
│               SISTEMA DE ALERTAS Y ON-CALL                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  METRICS ──► ALERT RULES ──► EVALUATION ──► NOTIFICATION       │
│                                                                 │
│  ┌────────────────────────────────────────────────────────┐    │
│  │                  ALERT SEVERITY                         │    │
│  │                                                         │    │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐  │    │
│  │  │   P1     │ │   P2     │ │   P3     │ │   P4     │  │    │
│  │  │ CRITICAL │ │  HIGH    │ │ MEDIUM   │ │   LOW    │  │    │
│  │  │          │ │          │ │          │ │          │  │    │
│  │  │ Page     │ │ Page     │ │ Slack    │ │ Ticket   │  │    │
│  │  │ oncall   │ │ oncall   │ │ channel  │ │ only     │  │    │
│  │  │ + mgr    │ │          │ │          │ │          │  │    │
│  │  │ < 5min   │ │ < 15min  │ │ < 1hour  │ │ Next day │  │    │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘  │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                 │
│  ┌────────────────────────────────────────────────────────┐    │
│  │              ESCALATION POLICY                          │    │
│  │                                                         │    │
│  │  0min  ──► Primary On-Call                             │    │
│  │  15min ──► Secondary On-Call                           │    │
│  │  30min ──► Team Lead                                    │    │
│  │  60min ──► Engineering Manager                          │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Sistema de Alertas

### Definición de Alertas

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
from datetime import datetime, timedelta
import asyncio
import json

class AlertSeverity(Enum):
    CRITICAL = "P1"   # Servicio completamente caído
    HIGH = "P2"       # Degradación severa
    MEDIUM = "P3"     # Degradación menor
    LOW = "P4"        # Informativo
    INFO = "P5"       # Notificación

class AlertStatus(Enum):
    FIRING = "firing"
    RESOLVED = "resolved"
    ACKNOWLEDGED = "acknowledged"
    SILENCED = "silenced"

@dataclass
class AlertRule:
    """Regla de alerta"""
    name: str
    description: str
    severity: AlertSeverity
    condition: str  # PromQL expression
    threshold: float
    comparison: str  # "gt", "lt", "eq", "gte", "lte"
    duration: timedelta  # Tiempo que debe mantenerse la condición
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)
    runbook_url: Optional[str] = None
    notification_channels: List[str] = field(default_factory=list)

@dataclass
class Alert:
    """Instancia de alerta activa"""
    alert_id: str
    rule: AlertRule
    status: AlertStatus
    value: float
    started_at: datetime
    updated_at: datetime
    resolved_at: Optional[datetime] = None
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[datetime] = None
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)

    @property
    def duration(self) -> timedelta:
        end = self.resolved_at or datetime.utcnow()
        return end - self.started_at

    def to_dict(self) -> Dict:
        return {
            "alert_id": self.alert_id,
            "name": self.rule.name,
            "severity": self.rule.severity.value,
            "status": self.status.value,
            "value": self.value,
            "started_at": self.started_at.isoformat(),
            "duration_minutes": self.duration.total_seconds() / 60,
            "description": self.rule.description,
            "runbook": self.rule.runbook_url
        }


class AgentAlertRules:
    """Reglas de alerta predefinidas para agentes"""

    @staticmethod
    def get_all_rules() -> List[AlertRule]:
        return [
            # === P1: CRITICAL ===
            AlertRule(
                name="agent_service_down",
                description="El servicio del agente no responde",
                severity=AlertSeverity.CRITICAL,
                condition='up{job="gemini-agent"} == 0',
                threshold=0,
                comparison="eq",
                duration=timedelta(minutes=2),
                notification_channels=["pagerduty", "slack_critical"],
                runbook_url="https://runbooks.example.com/agent-down",
                annotations={
                    "summary": "Servicio de agente caído",
                    "impact": "Todos los usuarios afectados",
                    "action": "Verificar pods, logs y dependencias"
                }
            ),
            AlertRule(
                name="agent_error_rate_critical",
                description="Tasa de errores > 10%",
                severity=AlertSeverity.CRITICAL,
                condition='sum(rate(agent_http_requests_total{status_code=~"5.."}[5m])) / sum(rate(agent_http_requests_total[5m]))',
                threshold=0.10,
                comparison="gt",
                duration=timedelta(minutes=3),
                notification_channels=["pagerduty", "slack_critical"],
                runbook_url="https://runbooks.example.com/high-error-rate"
            ),

            # === P2: HIGH ===
            AlertRule(
                name="agent_latency_high",
                description="Latencia P95 > 15 segundos",
                severity=AlertSeverity.HIGH,
                condition='histogram_quantile(0.95, rate(agent_http_request_duration_seconds_bucket[5m]))',
                threshold=15.0,
                comparison="gt",
                duration=timedelta(minutes=5),
                notification_channels=["pagerduty", "slack_alerts"],
                runbook_url="https://runbooks.example.com/high-latency"
            ),
            AlertRule(
                name="agent_llm_errors_high",
                description="Tasa de errores de LLM > 5%",
                severity=AlertSeverity.HIGH,
                condition='sum(rate(agent_llm_calls_total{status="error"}[5m])) / sum(rate(agent_llm_calls_total[5m]))',
                threshold=0.05,
                comparison="gt",
                duration=timedelta(minutes=5),
                notification_channels=["pagerduty", "slack_alerts"],
                annotations={
                    "action": "Verificar estado de Gemini API, rate limits, API key"
                }
            ),
            AlertRule(
                name="agent_completion_rate_low",
                description="Tasa de completitud < 70%",
                severity=AlertSeverity.HIGH,
                condition='agent_task_completion_rate{window="1h"}',
                threshold=0.70,
                comparison="lt",
                duration=timedelta(minutes=15),
                notification_channels=["slack_alerts"],
                runbook_url="https://runbooks.example.com/low-completion"
            ),

            # === P3: MEDIUM ===
            AlertRule(
                name="agent_high_token_usage",
                description="Consumo de tokens > 500K/hora",
                severity=AlertSeverity.MEDIUM,
                condition='sum(rate(agent_llm_tokens_total[1h])) * 3600',
                threshold=500000,
                comparison="gt",
                duration=timedelta(minutes=15),
                notification_channels=["slack_alerts"]
            ),
            AlertRule(
                name="agent_high_cost",
                description="Costo > $5/hora",
                severity=AlertSeverity.MEDIUM,
                condition='sum(rate(agent_cost_total_cents[1h])) * 3600 / 100',
                threshold=5.0,
                comparison="gt",
                duration=timedelta(minutes=30),
                notification_channels=["slack_alerts", "email"]
            ),
            AlertRule(
                name="agent_memory_high",
                description="Uso de memoria > 80%",
                severity=AlertSeverity.MEDIUM,
                condition='container_memory_usage_bytes{container="agent"} / container_spec_memory_limit_bytes{container="agent"}',
                threshold=0.80,
                comparison="gt",
                duration=timedelta(minutes=10),
                notification_channels=["slack_alerts"]
            ),

            # === P4: LOW ===
            AlertRule(
                name="agent_cache_miss_high",
                description="Cache miss rate > 80%",
                severity=AlertSeverity.LOW,
                condition='rate(agent_cache_total{result="miss"}[15m]) / rate(agent_cache_total[15m])',
                threshold=0.80,
                comparison="gt",
                duration=timedelta(minutes=30),
                notification_channels=["slack_monitoring"]
            ),
            AlertRule(
                name="agent_quota_warning",
                description="Quota > 80% utilizada",
                severity=AlertSeverity.LOW,
                condition='agent_quota_usage_percent',
                threshold=80,
                comparison="gt",
                duration=timedelta(minutes=0),
                notification_channels=["slack_monitoring", "email"]
            ),
        ]
```

### Motor de Evaluación de Alertas

```python
import redis.asyncio as redis
import uuid

class AlertEvaluator:
    """Evalúa condiciones de alertas"""

    def __init__(
        self,
        redis_client: redis.Redis,
        rules: List[AlertRule]
    ):
        self.redis = redis_client
        self.rules = rules
        self.active_alerts: Dict[str, Alert] = {}
        self.notifier = AlertNotifier()

    async def evaluate_all(self, metrics: Dict[str, float]):
        """Evalúa todas las reglas contra métricas actuales"""

        for rule in self.rules:
            metric_value = metrics.get(rule.name)

            if metric_value is None:
                continue

            is_firing = self._check_condition(
                metric_value,
                rule.threshold,
                rule.comparison
            )

            existing_alert = self.active_alerts.get(rule.name)

            if is_firing and not existing_alert:
                # Nueva alerta
                await self._fire_alert(rule, metric_value)

            elif is_firing and existing_alert:
                # Alerta continúa
                await self._update_alert(existing_alert, metric_value)

            elif not is_firing and existing_alert:
                # Alerta resuelta
                await self._resolve_alert(existing_alert)

    def _check_condition(
        self,
        value: float,
        threshold: float,
        comparison: str
    ) -> bool:
        """Evalúa condición"""
        comparisons = {
            "gt": lambda v, t: v > t,
            "lt": lambda v, t: v < t,
            "gte": lambda v, t: v >= t,
            "lte": lambda v, t: v <= t,
            "eq": lambda v, t: v == t,
        }
        return comparisons.get(comparison, lambda v, t: False)(value, threshold)

    async def _fire_alert(self, rule: AlertRule, value: float):
        """Dispara nueva alerta"""
        alert = Alert(
            alert_id=f"alert_{uuid.uuid4().hex[:12]}",
            rule=rule,
            status=AlertStatus.FIRING,
            value=value,
            started_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            labels=rule.labels,
            annotations=rule.annotations
        )

        self.active_alerts[rule.name] = alert

        # Guardar en Redis
        await self.redis.hset(
            "active_alerts",
            rule.name,
            json.dumps(alert.to_dict())
        )

        # Notificar
        await self.notifier.send(alert, "firing")

    async def _update_alert(self, alert: Alert, value: float):
        """Actualiza alerta existente"""
        alert.value = value
        alert.updated_at = datetime.utcnow()

        await self.redis.hset(
            "active_alerts",
            alert.rule.name,
            json.dumps(alert.to_dict())
        )

    async def _resolve_alert(self, alert: Alert):
        """Resuelve alerta"""
        alert.status = AlertStatus.RESOLVED
        alert.resolved_at = datetime.utcnow()
        alert.updated_at = datetime.utcnow()

        # Remover de activas
        del self.active_alerts[alert.rule.name]
        await self.redis.hdel("active_alerts", alert.rule.name)

        # Guardar en historial
        await self.redis.lpush(
            "alert_history",
            json.dumps(alert.to_dict())
        )

        # Notificar resolución
        await self.notifier.send(alert, "resolved")

    async def acknowledge_alert(
        self,
        rule_name: str,
        user_id: str
    ) -> bool:
        """Acknowledge de alerta por on-call"""
        alert = self.active_alerts.get(rule_name)
        if not alert:
            return False

        alert.status = AlertStatus.ACKNOWLEDGED
        alert.acknowledged_by = user_id
        alert.acknowledged_at = datetime.utcnow()

        await self.redis.hset(
            "active_alerts",
            rule_name,
            json.dumps(alert.to_dict())
        )

        await self.notifier.send(alert, "acknowledged")
        return True

    async def get_active_alerts(self) -> List[Dict]:
        """Obtiene alertas activas"""
        alerts = await self.redis.hgetall("active_alerts")
        return [json.loads(v) for v in alerts.values()]
```

### Notificador de Alertas

```python
import aiohttp
from typing import Optional

class AlertNotifier:
    """Envía notificaciones de alertas"""

    def __init__(self):
        self.channels = {
            "slack_critical": SlackNotifier(
                webhook_url=os.environ.get("SLACK_CRITICAL_WEBHOOK"),
                channel="#agent-critical"
            ),
            "slack_alerts": SlackNotifier(
                webhook_url=os.environ.get("SLACK_ALERTS_WEBHOOK"),
                channel="#agent-alerts"
            ),
            "slack_monitoring": SlackNotifier(
                webhook_url=os.environ.get("SLACK_MONITORING_WEBHOOK"),
                channel="#agent-monitoring"
            ),
            "pagerduty": PagerDutyNotifier(
                routing_key=os.environ.get("PAGERDUTY_ROUTING_KEY")
            ),
            "email": EmailNotifier(
                recipients=["oncall@example.com"]
            ),
        }

    async def send(self, alert: Alert, event: str):
        """Envía notificación a canales configurados"""
        for channel_name in alert.rule.notification_channels:
            channel = self.channels.get(channel_name)
            if channel:
                try:
                    await channel.notify(alert, event)
                except Exception as e:
                    print(f"Error notifying {channel_name}: {e}")


class SlackNotifier:
    """Notificador de Slack"""

    def __init__(self, webhook_url: str, channel: str = "#alerts"):
        self.webhook_url = webhook_url
        self.channel = channel

    async def notify(self, alert: Alert, event: str):
        """Envía alerta a Slack"""
        if not self.webhook_url:
            return

        color_map = {
            AlertSeverity.CRITICAL: "#FF0000",
            AlertSeverity.HIGH: "#FF6600",
            AlertSeverity.MEDIUM: "#FFCC00",
            AlertSeverity.LOW: "#0066FF",
            AlertSeverity.INFO: "#00CC00",
        }

        emoji_map = {
            "firing": ":rotating_light:",
            "resolved": ":white_check_mark:",
            "acknowledged": ":eyes:",
        }

        color = color_map.get(alert.rule.severity, "#808080")
        emoji = emoji_map.get(event, ":bell:")

        payload = {
            "channel": self.channel,
            "attachments": [{
                "color": color,
                "title": f"{emoji} [{alert.rule.severity.value}] {alert.rule.name}",
                "text": alert.rule.description,
                "fields": [
                    {
                        "title": "Status",
                        "value": event.upper(),
                        "short": True
                    },
                    {
                        "title": "Value",
                        "value": f"{alert.value:.4f}",
                        "short": True
                    },
                    {
                        "title": "Duration",
                        "value": str(alert.duration),
                        "short": True
                    },
                    {
                        "title": "Threshold",
                        "value": f"{alert.rule.comparison} {alert.rule.threshold}",
                        "short": True
                    }
                ],
                "footer": alert.rule.runbook_url or "",
                "ts": int(alert.started_at.timestamp())
            }]
        }

        async with aiohttp.ClientSession() as session:
            await session.post(self.webhook_url, json=payload)


class PagerDutyNotifier:
    """Notificador de PagerDuty"""

    def __init__(self, routing_key: str):
        self.routing_key = routing_key
        self.api_url = "https://events.pagerduty.com/v2/enqueue"

    async def notify(self, alert: Alert, event: str):
        """Envía evento a PagerDuty"""
        if not self.routing_key:
            return

        event_action = "trigger" if event == "firing" else "resolve"

        severity_map = {
            AlertSeverity.CRITICAL: "critical",
            AlertSeverity.HIGH: "error",
            AlertSeverity.MEDIUM: "warning",
            AlertSeverity.LOW: "info",
        }

        payload = {
            "routing_key": self.routing_key,
            "event_action": event_action,
            "dedup_key": alert.rule.name,
            "payload": {
                "summary": f"[{alert.rule.severity.value}] {alert.rule.name}: {alert.rule.description}",
                "severity": severity_map.get(alert.rule.severity, "info"),
                "source": "gemini-agent",
                "component": "agent-api",
                "custom_details": {
                    "value": alert.value,
                    "threshold": alert.rule.threshold,
                    "duration": str(alert.duration),
                    "runbook": alert.rule.runbook_url
                }
            }
        }

        async with aiohttp.ClientSession() as session:
            await session.post(self.api_url, json=payload)


class EmailNotifier:
    """Notificador por email"""

    def __init__(self, recipients: List[str]):
        self.recipients = recipients

    async def notify(self, alert: Alert, event: str):
        """Envía alerta por email (simplificado)"""
        # En producción: usar SendGrid, SES, o SMTP
        subject = f"[{alert.rule.severity.value}] {alert.rule.name} - {event.upper()}"
        body = f"""
Alert: {alert.rule.name}
Status: {event}
Severity: {alert.rule.severity.value}
Description: {alert.rule.description}
Value: {alert.value}
Threshold: {alert.rule.comparison} {alert.rule.threshold}
Started: {alert.started_at}
Duration: {alert.duration}

Runbook: {alert.rule.runbook_url or 'N/A'}
"""
        print(f"[EMAIL] To: {self.recipients}, Subject: {subject}")
        # Implementar envío real aquí
```

## Gestión de On-Call

```python
@dataclass
class OnCallSchedule:
    """Schedule de on-call"""
    team: str
    primary: str
    secondary: str
    start_date: datetime
    end_date: datetime

@dataclass
class EscalationPolicy:
    """Política de escalamiento"""
    name: str
    levels: List[Dict[str, Any]]

    @classmethod
    def default_policy(cls) -> 'EscalationPolicy':
        return cls(
            name="default",
            levels=[
                {
                    "delay_minutes": 0,
                    "target": "primary_oncall",
                    "channels": ["pagerduty", "slack_critical"]
                },
                {
                    "delay_minutes": 15,
                    "target": "secondary_oncall",
                    "channels": ["pagerduty", "slack_critical", "phone"]
                },
                {
                    "delay_minutes": 30,
                    "target": "team_lead",
                    "channels": ["pagerduty", "phone"]
                },
                {
                    "delay_minutes": 60,
                    "target": "engineering_manager",
                    "channels": ["phone", "email"]
                }
            ]
        )

class IncidentManager:
    """Gestión de incidentes"""

    def __init__(
        self,
        redis_client: redis.Redis,
        escalation_policy: EscalationPolicy
    ):
        self.redis = redis_client
        self.policy = escalation_policy
        self.notifier = AlertNotifier()

    async def create_incident(
        self,
        alert: Alert,
        description: str
    ) -> str:
        """Crea nuevo incidente"""
        incident_id = f"INC-{uuid.uuid4().hex[:8].upper()}"

        incident = {
            "id": incident_id,
            "alert_id": alert.alert_id,
            "severity": alert.rule.severity.value,
            "title": alert.rule.name,
            "description": description,
            "status": "open",
            "created_at": datetime.utcnow().isoformat(),
            "escalation_level": 0,
            "timeline": [{
                "time": datetime.utcnow().isoformat(),
                "event": "incident_created",
                "details": f"From alert: {alert.rule.name}"
            }]
        }

        await self.redis.hset(
            "incidents",
            incident_id,
            json.dumps(incident)
        )

        # Iniciar escalamiento
        asyncio.create_task(self._escalation_loop(incident_id))

        return incident_id

    async def _escalation_loop(self, incident_id: str):
        """Loop de escalamiento automático"""

        for level in self.policy.levels:
            delay = level["delay_minutes"]
            await asyncio.sleep(delay * 60)

            # Verificar si ya fue acknowledged
            incident_data = await self.redis.hget("incidents", incident_id)
            if not incident_data:
                return

            incident = json.loads(incident_data)

            if incident["status"] in ["acknowledged", "resolved"]:
                return

            # Escalar
            incident["escalation_level"] += 1
            incident["timeline"].append({
                "time": datetime.utcnow().isoformat(),
                "event": "escalated",
                "target": level["target"],
                "level": incident["escalation_level"]
            })

            await self.redis.hset(
                "incidents",
                incident_id,
                json.dumps(incident)
            )

            print(f"Incident {incident_id} escalated to {level['target']}")

    async def acknowledge_incident(
        self,
        incident_id: str,
        user_id: str
    ):
        """On-call acknowledges incidente"""
        incident_data = await self.redis.hget("incidents", incident_id)
        if not incident_data:
            return

        incident = json.loads(incident_data)
        incident["status"] = "acknowledged"
        incident["acknowledged_by"] = user_id
        incident["timeline"].append({
            "time": datetime.utcnow().isoformat(),
            "event": "acknowledged",
            "user": user_id
        })

        await self.redis.hset("incidents", incident_id, json.dumps(incident))

    async def resolve_incident(
        self,
        incident_id: str,
        user_id: str,
        resolution: str
    ):
        """Resuelve incidente"""
        incident_data = await self.redis.hget("incidents", incident_id)
        if not incident_data:
            return

        incident = json.loads(incident_data)
        incident["status"] = "resolved"
        incident["resolved_by"] = user_id
        incident["resolution"] = resolution
        incident["resolved_at"] = datetime.utcnow().isoformat()
        incident["timeline"].append({
            "time": datetime.utcnow().isoformat(),
            "event": "resolved",
            "user": user_id,
            "resolution": resolution
        })

        await self.redis.hset("incidents", incident_id, json.dumps(incident))

        # Mover a historial
        await self.redis.lpush("incident_history", json.dumps(incident))
```

## Ejemplo Práctico

```python
import asyncio

async def main():
    redis_client = redis.from_url("redis://localhost")

    # Configurar sistema de alertas
    rules = AgentAlertRules.get_all_rules()
    evaluator = AlertEvaluator(redis_client, rules)

    # Simular métricas
    metrics = {
        "agent_error_rate_critical": 0.12,     # > 10% - ALERTA
        "agent_latency_high": 18.5,            # > 15s - ALERTA
        "agent_high_token_usage": 450000,       # < 500K - OK
        "agent_high_cost": 3.2,                 # < $5 - OK
        "agent_completion_rate_low": 0.65,      # < 70% - ALERTA
        "agent_cache_miss_high": 0.55,          # < 80% - OK
    }

    print("=== Evaluando Alertas ===\n")
    await evaluator.evaluate_all(metrics)

    # Ver alertas activas
    active = await evaluator.get_active_alerts()
    print(f"Alertas activas: {len(active)}\n")

    for alert in active:
        print(f"  [{alert['severity']}] {alert['name']}")
        print(f"    Valor: {alert['value']}")
        print(f"    Duración: {alert['duration_minutes']:.1f} min")
        print()

    # Simular incidente
    print("=== Gestión de Incidente ===\n")

    policy = EscalationPolicy.default_policy()
    incident_mgr = IncidentManager(redis_client, policy)

    # Crear incidente desde alerta
    if active:
        first_alert_data = active[0]
        alert_rule = rules[0]  # Simplificado
        alert = Alert(
            alert_id=first_alert_data["alert_id"],
            rule=alert_rule,
            status=AlertStatus.FIRING,
            value=first_alert_data["value"],
            started_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )

        incident_id = await incident_mgr.create_incident(
            alert=alert,
            description="Tasa de errores alta detectada en producción"
        )
        print(f"Incidente creado: {incident_id}")

        # Simular acknowledge
        await asyncio.sleep(1)
        await incident_mgr.acknowledge_incident(incident_id, "oncall_user_1")
        print(f"Incidente acknowledged por: oncall_user_1")

        # Simular resolución
        await asyncio.sleep(1)
        await incident_mgr.resolve_incident(
            incident_id,
            "oncall_user_1",
            "Reiniciados pods del agente. Root cause: OOM por prompts largos."
        )
        print(f"Incidente resuelto")

    # Simular resolución de alertas
    print("\n=== Simulando resolución ===\n")
    resolved_metrics = {
        "agent_error_rate_critical": 0.02,
        "agent_latency_high": 3.5,
        "agent_completion_rate_low": 0.85,
    }
    await evaluator.evaluate_all(resolved_metrics)

    active_after = await evaluator.get_active_alerts()
    print(f"Alertas activas después de resolución: {len(active_after)}")

    await redis_client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Runbook Automatizado
Crea runbooks ejecutables:
- Diagnóstico automático al dispararse alerta
- Acciones de remediación automática
- Rollback automático si métricas no mejoran

### Ejercicio 2: Análisis Post-Mortem
Implementa herramientas para post-mortem:
- Recopilar timeline del incidente
- Correlacionar logs, métricas y traces
- Generar reporte automático

### Ejercicio 3: Noise Reduction
Reduce ruido en alertas:
- Agrupación de alertas relacionadas
- Supresión durante mantenimiento
- Correlación de alertas para identificar root cause

## Resumen

| Concepto | Descripción |
|----------|-------------|
| Alert Rule | Condición que dispara una alerta |
| Severity Levels | P1-P5 con diferentes SLAs de respuesta |
| Escalation Policy | Pasos de escalamiento automático |
| On-Call | Persona responsable de responder |
| Incident | Evento que requiere investigación |
| Runbook | Guía de respuesta ante incidentes |

---

**Siguiente:** [Módulo 11: Proyecto Integrador Final](../../modulo_11/tema_11.1/11.1.1-definicion-alcance.md)
