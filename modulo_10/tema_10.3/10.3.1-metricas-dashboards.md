# 10.3.1 Métricas y Dashboards

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar sistemas de métricas para agentes de IA, exportar datos a Prometheus y crear dashboards en Grafana para monitoreo en tiempo real.

## Introducción

Las métricas proporcionan visibilidad cuantitativa del comportamiento del agente en producción. A diferencia de los logs que capturan eventos individuales, las métricas permiten observar tendencias, detectar anomalías y medir SLOs.

```
┌─────────────────────────────────────────────────────────────────┐
│              SISTEMA DE MÉTRICAS Y DASHBOARDS                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  AGENT                   PROMETHEUS              GRAFANA        │
│  ┌──────────┐           ┌──────────┐          ┌──────────┐     │
│  │ /metrics │──scrape──►│  TSDB    │──query──►│Dashboard │     │
│  │          │  (15s)    │          │          │          │     │
│  │ counters │           │ storage  │          │ panels   │     │
│  │ gauges   │           │ 15 days  │          │ alerts   │     │
│  │ histograms│          │          │          │ graphs   │     │
│  └──────────┘           └──────────┘          └──────────┘     │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  MÉTRICAS CLAVE DEL AGENTE:                             │   │
│  │                                                          │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │   │
│  │  │  REQUEST     │  │  AGENT      │  │  BUSINESS   │     │   │
│  │  │  METRICS     │  │  METRICS    │  │  METRICS    │     │   │
│  │  │             │  │             │  │             │     │   │
│  │  │ • RPS       │  │ • Steps/task│  │ • Tasks OK  │     │   │
│  │  │ • Latency   │  │ • Tokens    │  │ • User sat. │     │   │
│  │  │ • Errors    │  │ • Tool calls│  │ • Cost/task │     │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘     │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Implementación de Métricas con Prometheus

### Definición de Métricas

```python
from prometheus_client import (
    Counter, Histogram, Gauge, Summary, Info,
    CollectorRegistry, generate_latest,
    CONTENT_TYPE_LATEST
)
from typing import Dict, Any, Optional
from functools import wraps
import time

# Registry personalizado
REGISTRY = CollectorRegistry()

# =============================================================================
# MÉTRICAS DE REQUEST
# =============================================================================

REQUEST_COUNT = Counter(
    'agent_http_requests_total',
    'Total de requests HTTP',
    ['method', 'endpoint', 'status_code'],
    registry=REGISTRY
)

REQUEST_LATENCY = Histogram(
    'agent_http_request_duration_seconds',
    'Duración de requests HTTP',
    ['method', 'endpoint'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0],
    registry=REGISTRY
)

REQUEST_IN_PROGRESS = Gauge(
    'agent_http_requests_in_progress',
    'Requests actualmente en progreso',
    ['method', 'endpoint'],
    registry=REGISTRY
)

# =============================================================================
# MÉTRICAS DEL AGENTE
# =============================================================================

AGENT_TASKS_TOTAL = Counter(
    'agent_tasks_total',
    'Total de tareas procesadas',
    ['task_type', 'status'],  # status: success, failure, timeout
    registry=REGISTRY
)

AGENT_TASK_DURATION = Histogram(
    'agent_task_duration_seconds',
    'Duración de tareas del agente',
    ['task_type'],
    buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0],
    registry=REGISTRY
)

AGENT_STEPS_PER_TASK = Histogram(
    'agent_steps_per_task',
    'Número de pasos por tarea',
    ['task_type'],
    buckets=[1, 2, 3, 5, 8, 10, 15, 20, 30],
    registry=REGISTRY
)

AGENT_ACTIVE_TASKS = Gauge(
    'agent_active_tasks',
    'Tareas activas actualmente',
    registry=REGISTRY
)

# =============================================================================
# MÉTRICAS DE LLM
# =============================================================================

LLM_CALLS_TOTAL = Counter(
    'agent_llm_calls_total',
    'Total de llamadas al LLM',
    ['model', 'status'],  # status: success, error, rate_limited
    registry=REGISTRY
)

LLM_CALL_DURATION = Histogram(
    'agent_llm_call_duration_seconds',
    'Duración de llamadas al LLM',
    ['model'],
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0],
    registry=REGISTRY
)

LLM_INPUT_TOKENS = Histogram(
    'agent_llm_input_tokens',
    'Tokens de entrada por llamada',
    ['model'],
    buckets=[100, 500, 1000, 2000, 5000, 10000, 20000, 50000],
    registry=REGISTRY
)

LLM_OUTPUT_TOKENS = Histogram(
    'agent_llm_output_tokens',
    'Tokens de salida por llamada',
    ['model'],
    buckets=[50, 100, 500, 1000, 2000, 5000, 10000],
    registry=REGISTRY
)

LLM_TOTAL_TOKENS = Counter(
    'agent_llm_tokens_total',
    'Total de tokens consumidos',
    ['model', 'direction'],  # direction: input, output
    registry=REGISTRY
)

# =============================================================================
# MÉTRICAS DE TOOLS
# =============================================================================

TOOL_CALLS_TOTAL = Counter(
    'agent_tool_calls_total',
    'Total de llamadas a tools',
    ['tool_name', 'status'],
    registry=REGISTRY
)

TOOL_CALL_DURATION = Histogram(
    'agent_tool_call_duration_seconds',
    'Duración de llamadas a tools',
    ['tool_name'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0],
    registry=REGISTRY
)

# =============================================================================
# MÉTRICAS DE NEGOCIO
# =============================================================================

COST_TOTAL = Counter(
    'agent_cost_total_cents',
    'Costo total acumulado en centavos',
    ['model', 'cost_type'],  # cost_type: input, output, tool
    registry=REGISTRY
)

USER_SATISFACTION = Histogram(
    'agent_user_satisfaction_score',
    'Score de satisfacción del usuario',
    buckets=[1, 2, 3, 4, 5],
    registry=REGISTRY
)

TASK_COMPLETION_RATE = Gauge(
    'agent_task_completion_rate',
    'Tasa de completitud de tareas (rolling)',
    ['window'],  # window: 1h, 24h, 7d
    registry=REGISTRY
)

# =============================================================================
# MÉTRICAS DE SISTEMA
# =============================================================================

APP_INFO = Info(
    'agent_app',
    'Información de la aplicación',
    registry=REGISTRY
)

MEMORY_ENTRIES = Gauge(
    'agent_memory_entries',
    'Entradas en memoria del agente',
    ['memory_type'],
    registry=REGISTRY
)

CACHE_HITS = Counter(
    'agent_cache_total',
    'Cache hits y misses',
    ['result'],  # result: hit, miss
    registry=REGISTRY
)
```

### Instrumentación del Agente

```python
import google.generativeai as genai
from contextlib import contextmanager
import time

class MetricsCollector:
    """Recolector de métricas para el agente"""

    def __init__(self):
        # Inicializar info de la app
        APP_INFO.info({
            'version': '1.0.0',
            'model': 'gemini-2.0-flash',
            'environment': 'production'
        })

    @contextmanager
    def track_request(self, method: str, endpoint: str):
        """Context manager para trackear requests"""
        REQUEST_IN_PROGRESS.labels(method=method, endpoint=endpoint).inc()
        start = time.time()

        status_code = 200
        try:
            yield lambda code: setattr(
                type('', (), {'code': code})(), 'code', code
            ) or None
        except Exception:
            status_code = 500
            raise
        finally:
            duration = time.time() - start
            REQUEST_COUNT.labels(
                method=method,
                endpoint=endpoint,
                status_code=status_code
            ).inc()
            REQUEST_LATENCY.labels(
                method=method,
                endpoint=endpoint
            ).observe(duration)
            REQUEST_IN_PROGRESS.labels(
                method=method,
                endpoint=endpoint
            ).dec()

    @contextmanager
    def track_task(self, task_type: str = "general"):
        """Trackea una tarea completa del agente"""
        AGENT_ACTIVE_TASKS.inc()
        start = time.time()
        status = "success"

        try:
            yield
        except Exception:
            status = "failure"
            raise
        finally:
            duration = time.time() - start
            AGENT_TASKS_TOTAL.labels(
                task_type=task_type,
                status=status
            ).inc()
            AGENT_TASK_DURATION.labels(
                task_type=task_type
            ).observe(duration)
            AGENT_ACTIVE_TASKS.dec()

    def record_llm_call(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        duration_seconds: float,
        success: bool = True
    ):
        """Registra métricas de una llamada al LLM"""
        status = "success" if success else "error"

        LLM_CALLS_TOTAL.labels(model=model, status=status).inc()
        LLM_CALL_DURATION.labels(model=model).observe(duration_seconds)
        LLM_INPUT_TOKENS.labels(model=model).observe(input_tokens)
        LLM_OUTPUT_TOKENS.labels(model=model).observe(output_tokens)

        LLM_TOTAL_TOKENS.labels(model=model, direction="input").inc(input_tokens)
        LLM_TOTAL_TOKENS.labels(model=model, direction="output").inc(output_tokens)

        # Calcular costo (Gemini Flash pricing)
        input_cost = input_tokens * 0.075 / 1_000_000 * 100  # centavos
        output_cost = output_tokens * 0.30 / 1_000_000 * 100

        COST_TOTAL.labels(model=model, cost_type="input").inc(input_cost)
        COST_TOTAL.labels(model=model, cost_type="output").inc(output_cost)

    def record_tool_call(
        self,
        tool_name: str,
        duration_seconds: float,
        success: bool = True
    ):
        """Registra métricas de una llamada a tool"""
        status = "success" if success else "error"

        TOOL_CALLS_TOTAL.labels(
            tool_name=tool_name,
            status=status
        ).inc()
        TOOL_CALL_DURATION.labels(
            tool_name=tool_name
        ).observe(duration_seconds)

    def record_agent_steps(self, task_type: str, steps: int):
        """Registra pasos por tarea"""
        AGENT_STEPS_PER_TASK.labels(task_type=task_type).observe(steps)

    def record_cache_result(self, hit: bool):
        """Registra resultado de cache"""
        CACHE_HITS.labels(result="hit" if hit else "miss").inc()

    def update_completion_rate(self, window: str, rate: float):
        """Actualiza tasa de completitud"""
        TASK_COMPLETION_RATE.labels(window=window).set(rate)


class InstrumentedGeminiModel:
    """Modelo Gemini con métricas integradas"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash",
        metrics: MetricsCollector = None
    ):
        self.model = genai.GenerativeModel(model_name)
        self.model_name = model_name
        self.metrics = metrics or MetricsCollector()

    def generate_content(self, prompt: str, **kwargs):
        """Genera contenido con métricas"""
        start = time.time()
        success = True

        try:
            response = self.model.generate_content(prompt, **kwargs)

            duration = time.time() - start

            # Registrar métricas
            self.metrics.record_llm_call(
                model=self.model_name,
                input_tokens=response.usage_metadata.prompt_token_count,
                output_tokens=response.usage_metadata.candidates_token_count,
                duration_seconds=duration,
                success=True
            )

            return response

        except Exception as e:
            duration = time.time() - start
            self.metrics.record_llm_call(
                model=self.model_name,
                input_tokens=0,
                output_tokens=0,
                duration_seconds=duration,
                success=False
            )
            raise
```

### Endpoint de Métricas

```python
from fastapi import FastAPI, Response
from starlette.middleware.base import BaseHTTPMiddleware

app = FastAPI()
metrics = MetricsCollector()

@app.get("/metrics")
async def prometheus_metrics():
    """Endpoint para Prometheus scraping"""
    return Response(
        content=generate_latest(REGISTRY),
        media_type=CONTENT_TYPE_LATEST
    )

# Middleware automático de métricas HTTP
class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        if request.url.path == "/metrics":
            return await call_next(request)

        method = request.method
        endpoint = request.url.path

        REQUEST_IN_PROGRESS.labels(method=method, endpoint=endpoint).inc()
        start = time.time()

        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception:
            status_code = 500
            raise
        finally:
            duration = time.time() - start

            REQUEST_COUNT.labels(
                method=method,
                endpoint=endpoint,
                status_code=str(status_code)
            ).inc()
            REQUEST_LATENCY.labels(
                method=method,
                endpoint=endpoint
            ).observe(duration)
            REQUEST_IN_PROGRESS.labels(
                method=method,
                endpoint=endpoint
            ).dec()

        return response

app.add_middleware(PrometheusMiddleware)
```

## Configuración de Prometheus

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'gemini-agent'
    metrics_path: '/metrics'
    scrape_interval: 10s

    # Si múltiples instancias
    static_configs:
      - targets:
        - 'agent-api:8080'
        labels:
          environment: 'production'

    # O con service discovery de Kubernetes
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - gemini-agents

    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)
        replacement: ${1}:8080
```

### Reglas de Alertas

```yaml
# alert_rules.yml
groups:
  - name: agent_alerts
    rules:
      # Alta tasa de errores
      - alert: HighErrorRate
        expr: |
          sum(rate(agent_http_requests_total{status_code=~"5.."}[5m]))
          /
          sum(rate(agent_http_requests_total[5m]))
          > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Alta tasa de errores (>5%)"
          description: "El agente tiene {{ $value | humanizePercentage }} de errores en los últimos 5 minutos"

      # Latencia alta
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, rate(agent_http_request_duration_seconds_bucket[5m]))
          > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Latencia P95 > 10s"

      # Consumo alto de tokens
      - alert: HighTokenConsumption
        expr: |
          rate(agent_llm_tokens_total[1h]) > 100000
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Consumo de tokens alto (>100K/h)"

      # Baja tasa de completitud
      - alert: LowCompletionRate
        expr: |
          agent_task_completion_rate{window="1h"} < 0.8
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Tasa de completitud < 80%"

      # Muchas tareas activas
      - alert: HighActiveTasks
        expr: agent_active_tasks > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Más de 50 tareas activas simultáneas"
```

## Dashboard de Grafana

```json
{
  "dashboard": {
    "title": "Gemini Agent - Production Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "type": "timeseries",
        "targets": [{
          "expr": "sum(rate(agent_http_requests_total[5m])) by (status_code)",
          "legendFormat": "{{status_code}}"
        }],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "title": "Latencia P50 / P95 / P99",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(agent_http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(agent_http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(agent_http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "title": "Tokens por Minuto",
        "type": "timeseries",
        "targets": [{
          "expr": "sum(rate(agent_llm_tokens_total[5m])) by (direction) * 60",
          "legendFormat": "{{direction}}"
        }],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "title": "Task Completion Rate",
        "type": "gauge",
        "targets": [{
          "expr": "agent_task_completion_rate{window='1h'}"
        }],
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 8}
      },
      {
        "title": "Costo Acumulado (USD/hora)",
        "type": "stat",
        "targets": [{
          "expr": "sum(rate(agent_cost_total_cents[1h])) * 3600 / 100"
        }],
        "gridPos": {"h": 8, "w": 6, "x": 18, "y": 8}
      },
      {
        "title": "Tool Calls por Herramienta",
        "type": "piechart",
        "targets": [{
          "expr": "sum(agent_tool_calls_total) by (tool_name)",
          "legendFormat": "{{tool_name}}"
        }],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
      },
      {
        "title": "Tareas Activas",
        "type": "timeseries",
        "targets": [{
          "expr": "agent_active_tasks"
        }],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
      }
    ]
  }
}
```

## Ejemplo Práctico

```python
import asyncio
import google.generativeai as genai
import time
import random

async def main():
    genai.configure(api_key="YOUR_API_KEY")

    # Crear modelo instrumentado
    metrics = MetricsCollector()
    model = InstrumentedGeminiModel(metrics=metrics)

    # Simular carga de trabajo
    tasks = [
        ("search", "Busca información sobre Python"),
        ("analysis", "Analiza los beneficios de microservicios"),
        ("coding", "Escribe una función para ordenar una lista"),
        ("search", "Encuentra documentación de FastAPI"),
        ("analysis", "Compara SQL vs NoSQL"),
    ]

    for task_type, prompt in tasks:
        with metrics.track_task(task_type=task_type):
            # Simular pasos del agente
            steps = random.randint(1, 5)

            for step in range(steps):
                # Llamada al LLM
                response = model.generate_content(prompt)

                # Simular tool call
                tool_start = time.time()
                await asyncio.sleep(random.uniform(0.01, 0.5))
                metrics.record_tool_call(
                    tool_name="search",
                    duration_seconds=time.time() - tool_start,
                    success=random.random() > 0.1
                )

            metrics.record_agent_steps(task_type, steps)

    # Mostrar métricas
    print("=== Métricas Recolectadas ===\n")
    output = generate_latest(REGISTRY).decode()

    # Filtrar y mostrar métricas relevantes
    for line in output.split('\n'):
        if line and not line.startswith('#'):
            print(f"  {line}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Custom Metrics
Añade métricas personalizadas:
- Tiempo de razonamiento del agente
- Calidad de respuesta (user feedback)
- Diversidad de tools usadas

### Ejercicio 2: SLO Dashboard
Crea dashboard de SLOs:
- Availability: 99.9%
- Latency P95: < 5s
- Error rate: < 1%
- Error budget tracking

### Ejercicio 3: Capacity Planning
Usa métricas para planificación:
- Proyección de tokens/mes
- Estimación de costos futuros
- Necesidades de escalado

## Resumen

| Tipo de Métrica | Ejemplo | Uso |
|-----------------|---------|-----|
| Counter | requests_total | Conteos acumulativos |
| Histogram | request_duration | Distribuciones de latencia |
| Gauge | active_tasks | Valores que suben y bajan |
| Summary | token_count | Estadísticas calculadas |
| Info | app_version | Metadata estática |

---

**Siguiente:** [10.3.2 Tracing Distribuido](./10.3.2-tracing-distribuido.md)
