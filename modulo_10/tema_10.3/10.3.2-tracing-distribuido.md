# 10.3.2 Tracing Distribuido

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar tracing distribuido para agentes de IA, rastreando el flujo completo de una solicitud a través de múltiples servicios y llamadas a LLM.

## Introducción

El tracing distribuido permite visualizar el recorrido completo de una solicitud: desde que llega al API, pasa por el agente, llama al LLM, ejecuta tools, y regresa la respuesta. Es esencial para debugging y optimización de performance.

```
┌─────────────────────────────────────────────────────────────────┐
│                    TRACE DE UN AGENTE                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Trace ID: abc-123-def-456                                      │
│                                                                 │
│  ┌──────────────────────────────────────────────────────┐      │
│  │ Span: HTTP POST /api/v1/agent                  5.2s │      │
│  │  ┌────────────────────────────────────────────────┐  │      │
│  │  │ Span: agent.process_task                 4.8s │  │      │
│  │  │  ┌──────────────────────────────────┐         │  │      │
│  │  │  │ Span: llm.generate (step 1) 1.2s│         │  │      │
│  │  │  └──────────────────────────────────┘         │  │      │
│  │  │  ┌───────────────────────┐                    │  │      │
│  │  │  │ Span: tool.search 0.8s│                    │  │      │
│  │  │  └───────────────────────┘                    │  │      │
│  │  │  ┌──────────────────────────────────┐         │  │      │
│  │  │  │ Span: llm.generate (step 2) 1.5s│         │  │      │
│  │  │  └──────────────────────────────────┘         │  │      │
│  │  │  ┌──────────────────────────────────────┐     │  │      │
│  │  │  │ Span: llm.generate (step 3)    1.1s │     │  │      │
│  │  │  └──────────────────────────────────────┘     │  │      │
│  │  └────────────────────────────────────────────────┘  │      │
│  └──────────────────────────────────────────────────────┘      │
│                                                                 │
│  Collector: OpenTelemetry ──► Jaeger / Cloud Trace             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Configuración de OpenTelemetry

### Setup Inicial

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import (
    BatchSpanProcessor,
    ConsoleSpanExporter
)
from opentelemetry.sdk.resources import Resource, SERVICE_NAME
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.propagate import set_global_textmap
from opentelemetry.propagators.b3 import B3MultiFormat
from opentelemetry.trace import StatusCode
from typing import Optional, Dict, Any
import os

def setup_tracing(
    service_name: str = "gemini-agent",
    environment: str = "production",
    otlp_endpoint: Optional[str] = None
):
    """Configura OpenTelemetry tracing"""

    # Definir recurso (identifica el servicio)
    resource = Resource.create({
        SERVICE_NAME: service_name,
        "deployment.environment": environment,
        "service.version": os.getenv("APP_VERSION", "1.0.0"),
    })

    # Crear TracerProvider
    provider = TracerProvider(resource=resource)

    # Exportador: OTLP (para Jaeger, Tempo, Cloud Trace)
    if otlp_endpoint:
        otlp_exporter = OTLPSpanExporter(endpoint=otlp_endpoint)
        provider.add_span_processor(
            BatchSpanProcessor(otlp_exporter)
        )

    # Exportador: Console (para desarrollo)
    if environment == "development":
        provider.add_span_processor(
            BatchSpanProcessor(ConsoleSpanExporter())
        )

    # Establecer como provider global
    trace.set_tracer_provider(provider)

    # Configurar propagación de contexto
    set_global_textmap(B3MultiFormat())

    # Instrumentación automática
    RequestsInstrumentor().instrument()

    return trace.get_tracer(service_name)
```

### Tracer para Agentes

```python
import google.generativeai as genai
from opentelemetry import trace, context
from opentelemetry.trace import Span, StatusCode
from contextlib import contextmanager
import time
import json
from functools import wraps

class AgentTracer:
    """Tracing especializado para agentes de IA"""

    def __init__(self, service_name: str = "gemini-agent"):
        self.tracer = trace.get_tracer(service_name)

    @contextmanager
    def trace_task(
        self,
        task_description: str,
        task_type: str = "general",
        user_id: Optional[str] = None,
        session_id: Optional[str] = None
    ):
        """
        Traza una tarea completa del agente

        Uso:
            with tracer.trace_task("Buscar info", task_type="search") as span:
                # ... lógica del agente
        """
        with self.tracer.start_as_current_span(
            "agent.task",
            attributes={
                "agent.task.description": task_description[:200],
                "agent.task.type": task_type,
                "agent.user.id": user_id or "",
                "agent.session.id": session_id or "",
            }
        ) as span:
            try:
                yield span
                span.set_status(StatusCode.OK)
            except Exception as e:
                span.set_status(StatusCode.ERROR, str(e))
                span.record_exception(e)
                raise

    @contextmanager
    def trace_llm_call(
        self,
        model: str,
        step_number: int = 0,
        prompt_preview: Optional[str] = None
    ):
        """Traza una llamada al LLM"""
        with self.tracer.start_as_current_span(
            "llm.generate",
            attributes={
                "llm.model": model,
                "llm.step_number": step_number,
                "llm.prompt_preview": (prompt_preview or "")[:100],
            }
        ) as span:
            start = time.time()
            try:
                yield span
                duration = time.time() - start
                span.set_attribute("llm.duration_ms", round(duration * 1000, 2))
                span.set_status(StatusCode.OK)
            except Exception as e:
                span.set_status(StatusCode.ERROR, str(e))
                span.record_exception(e)
                raise

    @contextmanager
    def trace_tool_call(
        self,
        tool_name: str,
        tool_args: Optional[Dict] = None
    ):
        """Traza una llamada a tool"""
        safe_args = self._sanitize_attributes(tool_args or {})

        with self.tracer.start_as_current_span(
            f"tool.{tool_name}",
            attributes={
                "tool.name": tool_name,
                "tool.args": json.dumps(safe_args)[:500],
            }
        ) as span:
            start = time.time()
            try:
                yield span
                duration = time.time() - start
                span.set_attribute("tool.duration_ms", round(duration * 1000, 2))
                span.set_attribute("tool.success", True)
                span.set_status(StatusCode.OK)
            except Exception as e:
                span.set_attribute("tool.success", False)
                span.set_status(StatusCode.ERROR, str(e))
                span.record_exception(e)
                raise

    @contextmanager
    def trace_agent_step(
        self,
        step_number: int,
        step_type: str
    ):
        """Traza un paso del agente"""
        with self.tracer.start_as_current_span(
            f"agent.step.{step_type}",
            attributes={
                "agent.step.number": step_number,
                "agent.step.type": step_type,
            }
        ) as span:
            yield span

    @contextmanager
    def trace_memory_operation(
        self,
        operation: str,  # "read", "write", "search"
        memory_type: str = "short_term"
    ):
        """Traza operación de memoria"""
        with self.tracer.start_as_current_span(
            f"memory.{operation}",
            attributes={
                "memory.operation": operation,
                "memory.type": memory_type,
            }
        ) as span:
            yield span

    def _sanitize_attributes(self, attrs: Dict) -> Dict:
        """Sanitiza atributos para tracing"""
        sensitive = {'password', 'api_key', 'secret', 'token'}
        result = {}
        for key, value in attrs.items():
            if any(s in key.lower() for s in sensitive):
                result[key] = "[REDACTED]"
            else:
                result[key] = str(value)[:200]
        return result

    def add_event(self, name: str, attributes: Optional[Dict] = None):
        """Añade evento al span actual"""
        current_span = trace.get_current_span()
        if current_span:
            current_span.add_event(name, attributes=attributes or {})

    def set_attribute(self, key: str, value: Any):
        """Añade atributo al span actual"""
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute(key, value)
```

### Agente Instrumentado

```python
class TracedGeminiAgent:
    """Agente Gemini con tracing completo"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash",
        tracer: Optional[AgentTracer] = None
    ):
        self.model = genai.GenerativeModel(model_name)
        self.model_name = model_name
        self.tracer = tracer or AgentTracer()

    async def execute_task(
        self,
        task: str,
        user_id: str = "unknown",
        max_steps: int = 10
    ) -> Dict[str, Any]:
        """Ejecuta tarea con tracing"""

        with self.tracer.trace_task(
            task_description=task,
            task_type="general",
            user_id=user_id
        ) as task_span:

            chat = self.model.start_chat()
            steps = []

            # Paso inicial
            with self.tracer.trace_llm_call(
                model=self.model_name,
                step_number=0,
                prompt_preview=task[:100]
            ) as llm_span:
                response = chat.send_message(task)

                # Registrar tokens
                llm_span.set_attribute(
                    "llm.input_tokens",
                    response.usage_metadata.prompt_token_count
                )
                llm_span.set_attribute(
                    "llm.output_tokens",
                    response.usage_metadata.candidates_token_count
                )

            # Loop de agente
            for step_num in range(1, max_steps + 1):
                with self.tracer.trace_agent_step(
                    step_number=step_num,
                    step_type="reasoning"
                ):
                    # Verificar function calls
                    function_calls = self._extract_function_calls(response)

                    if not function_calls:
                        self.tracer.add_event("agent.finished", {
                            "reason": "no_more_tool_calls",
                            "total_steps": step_num
                        })
                        break

                    # Ejecutar tools
                    tool_results = []
                    for fc in function_calls:
                        with self.tracer.trace_tool_call(
                            tool_name=fc["name"],
                            tool_args=fc["args"]
                        ) as tool_span:
                            result = await self._execute_tool(
                                fc["name"],
                                fc["args"]
                            )
                            tool_span.set_attribute(
                                "tool.result_size",
                                len(str(result))
                            )
                            tool_results.append(result)

                    # Siguiente paso del LLM
                    with self.tracer.trace_llm_call(
                        model=self.model_name,
                        step_number=step_num
                    ) as llm_span:
                        response = chat.send_message(
                            self._format_tool_results(function_calls, tool_results)
                        )
                        llm_span.set_attribute(
                            "llm.input_tokens",
                            response.usage_metadata.prompt_token_count
                        )
                        llm_span.set_attribute(
                            "llm.output_tokens",
                            response.usage_metadata.candidates_token_count
                        )

                    steps.append({
                        "step": step_num,
                        "tools_called": [fc["name"] for fc in function_calls]
                    })

            # Atributos finales del task
            task_span.set_attribute("agent.total_steps", len(steps))
            task_span.set_attribute("agent.final_response_length", len(response.text))

            return {
                "response": response.text,
                "steps": steps,
                "total_steps": len(steps)
            }

    def _extract_function_calls(self, response) -> list:
        """Extrae function calls de la respuesta"""
        calls = []
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'function_call'):
                            calls.append({
                                "name": part.function_call.name,
                                "args": dict(part.function_call.args)
                            })
        return calls

    async def _execute_tool(self, name: str, args: Dict) -> Any:
        """Ejecuta herramienta (implementar según tools disponibles)"""
        return f"Result from {name}"

    def _format_tool_results(self, calls: list, results: list) -> list:
        """Formatea resultados de tools para el LLM"""
        formatted = []
        for call, result in zip(calls, results):
            formatted.append({
                "function_response": {
                    "name": call["name"],
                    "response": {"result": str(result)}
                }
            })
        return formatted
```

## Integración con FastAPI

```python
from fastapi import FastAPI, Request
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

app = FastAPI()

# Instrumentar FastAPI automáticamente
FastAPIInstrumentor.instrument_app(app)

# Tracer del agente
agent_tracer = AgentTracer()
agent = TracedGeminiAgent(tracer=agent_tracer)

@app.post("/api/v1/agent/execute")
async def execute_agent(request: dict):
    """Endpoint con tracing automático"""

    # El span HTTP ya se crea automáticamente por FastAPIInstrumentor
    # Los spans internos del agente se anidan correctamente

    result = await agent.execute_task(
        task=request.get("task", ""),
        user_id=request.get("user_id", "anonymous"),
        max_steps=request.get("max_steps", 10)
    )

    return result


# Para propagar trace context a servicios downstream
@app.post("/api/v1/agent/delegate")
async def delegate_to_subagent(request: dict):
    """Delega a sub-agente con propagación de contexto"""
    import requests
    from opentelemetry.propagate import inject

    # Los headers de propagación se inyectan automáticamente
    # gracias a RequestsInstrumentor
    sub_agent_url = "http://sub-agent-service:8080/api/v1/execute"

    response = requests.post(
        sub_agent_url,
        json=request,
        timeout=30
    )

    return response.json()
```

## Visualización con Jaeger

### Docker Compose con Jaeger

```yaml
# docker-compose.tracing.yml
version: '3.8'

services:
  jaeger:
    image: jaegertracing/all-in-one:1.50
    ports:
      - "16686:16686"  # UI
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - agent-network

  agent:
    build: .
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OTEL_SERVICE_NAME=gemini-agent
    depends_on:
      - jaeger
    networks:
      - agent-network

networks:
  agent-network:
    driver: bridge
```

## Ejemplo Práctico

```python
import asyncio

async def main():
    genai.configure(api_key="YOUR_API_KEY")

    # Setup tracing
    tracer_instance = setup_tracing(
        service_name="gemini-agent-demo",
        environment="development",
        otlp_endpoint="http://localhost:4317"
    )

    # Crear agente con tracing
    agent_tracer = AgentTracer()
    agent = TracedGeminiAgent(tracer=agent_tracer)

    # Ejecutar tarea
    print("Ejecutando tarea con tracing...")

    result = await agent.execute_task(
        task="Explica qué es machine learning y sus aplicaciones",
        user_id="demo_user",
        max_steps=5
    )

    print(f"\nRespuesta: {result['response'][:200]}...")
    print(f"Pasos: {result['total_steps']}")
    print("\nVer traces en: http://localhost:16686")

    # Ejemplo con contexto manual
    with agent_tracer.trace_task(
        task_description="Tarea manual de ejemplo",
        task_type="demo"
    ) as span:
        # Paso 1: Memoria
        with agent_tracer.trace_memory_operation("read", "long_term"):
            await asyncio.sleep(0.1)  # Simular lectura
            agent_tracer.add_event("memory.found", {"entries": 5})

        # Paso 2: LLM
        with agent_tracer.trace_llm_call(
            model="gemini-2.0-flash",
            step_number=1,
            prompt_preview="Genera resumen basado en memoria"
        ) as llm_span:
            await asyncio.sleep(0.5)  # Simular LLM
            llm_span.set_attribute("llm.input_tokens", 1500)
            llm_span.set_attribute("llm.output_tokens", 500)

        # Paso 3: Tool
        with agent_tracer.trace_tool_call(
            tool_name="formatter",
            tool_args={"format": "markdown"}
        ):
            await asyncio.sleep(0.05)

        span.set_attribute("agent.total_steps", 3)

    print("\nTrace manual completado")

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Trace Sampling
Implementa estrategias de sampling:
- 100% en desarrollo, 10% en producción
- Always sample errores
- Sampling adaptativo por carga

### Ejercicio 2: Custom Span Processor
Crea un procesador que:
- Filtre spans por duración mínima
- Enriquezca con metadata del sistema
- Agregue correlation IDs custom

### Ejercicio 3: Tracing Cross-Service
Implementa tracing entre servicios:
- Agent API → Sub-Agent
- Agent → External Tool Service
- Verificar propagación de contexto

## Resumen

| Concepto | Descripción |
|----------|-------------|
| Trace | Recorrido completo de un request |
| Span | Unidad individual de trabajo |
| Context Propagation | Pasar trace ID entre servicios |
| Instrumentation | Añadir tracing al código |
| Jaeger/Tempo | Backends de almacenamiento de traces |
| OpenTelemetry | Estándar de observabilidad |

---

**Siguiente:** [10.3.3 Alertas y On-Call](./10.3.3-alertas-oncall.md)
