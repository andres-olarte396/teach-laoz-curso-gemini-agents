# Primera Interacción con Gemini

**Tiempo estimado**: 30 minutos
**Nivel**: Básico
**Prerrequisitos**: SDK instalado (0.2.2), API Key configurada (0.2.1)

## ¿Por qué importa este concepto?

Esta es tu primera llamada real a Gemini. Verás cómo funciona el ciclo completo: enviar prompt, recibir respuesta, procesar el resultado. Este patrón básico es la base de todo lo que construiremos en el curso.

---

## Tu primer script

```python
#!/usr/bin/env python3
"""
Primera interacción con Gemini API.
Asegúrate de tener GOOGLE_API_KEY configurada.
"""

import os
import google.generativeai as genai


def main():
    # 1. Configurar API key
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("Error: Configura GOOGLE_API_KEY")
        print("  export GOOGLE_API_KEY='tu-api-key'")
        return

    genai.configure(api_key=api_key)

    # 2. Crear instancia del modelo
    model = genai.GenerativeModel("gemini-1.5-flash")

    # 3. Enviar prompt y recibir respuesta
    print("Enviando prompt a Gemini...")
    response = model.generate_content("¿Qué es un agente de IA en una oración?")

    # 4. Mostrar respuesta
    print("\nRespuesta:")
    print(response.text)

    # 5. Ver metadata
    print("\n--- Metadata ---")
    print(f"Tokens de entrada: {response.usage_metadata.prompt_token_count}")
    print(f"Tokens de salida: {response.usage_metadata.candidates_token_count}")
    print(f"Total tokens: {response.usage_metadata.total_token_count}")


if __name__ == "__main__":
    main()
```

**Salida esperada**:
```
Enviando prompt a Gemini...

Respuesta:
Un agente de IA es un sistema autónomo que percibe su entorno, toma decisiones
y ejecuta acciones para alcanzar objetivos específicos.

--- Metadata ---
Tokens de entrada: 12
Tokens de salida: 28
Total tokens: 40
```

---

## Explorando la API

### Generación básica

```python
import google.generativeai as genai
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-1.5-flash")


# Generación simple
response = model.generate_content("Explica recursión en programación")
print(response.text)


# Con configuración de generación
from google.generativeai import GenerationConfig

config = GenerationConfig(
    temperature=0.7,        # Creatividad (0=determinístico, 2=muy creativo)
    top_p=0.9,              # Nucleus sampling
    top_k=40,               # Top-k sampling
    max_output_tokens=500,  # Límite de tokens de salida
)

response = model.generate_content(
    "Escribe un haiku sobre programación",
    generation_config=config
)
print(response.text)
```

### Streaming (respuestas en tiempo real)

```python
def stream_response(prompt: str):
    """Muestra la respuesta token por token."""
    model = genai.GenerativeModel("gemini-1.5-flash")

    print("Respuesta: ", end="", flush=True)

    # stream=True para recibir chunks incrementales
    response = model.generate_content(prompt, stream=True)

    for chunk in response:
        print(chunk.text, end="", flush=True)

    print()  # Nueva línea al final


# Uso
stream_response("Cuenta del 1 al 10, un número por línea")
```

### Conversaciones multi-turno (Chat)

```python
def chat_demo():
    """Demuestra conversación con memoria."""
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Iniciar chat (mantiene historial automáticamente)
    chat = model.start_chat(history=[])

    # Primera pregunta
    response = chat.send_message("Mi nombre es Carlos. ¿Qué es Python?")
    print(f"Gemini: {response.text}\n")

    # Segunda pregunta (el modelo recuerda el contexto)
    response = chat.send_message("¿Cómo se usa para machine learning?")
    print(f"Gemini: {response.text}\n")

    # Tercera pregunta (prueba de memoria)
    response = chat.send_message("¿Recuerdas mi nombre?")
    print(f"Gemini: {response.text}\n")

    # Ver historial completo
    print("--- Historial ---")
    for message in chat.history:
        role = "Usuario" if message.role == "user" else "Gemini"
        print(f"{role}: {message.parts[0].text[:50]}...")


chat_demo()
```

### System Instructions (Personalidad del modelo)

```python
def assistant_with_personality():
    """Crea un asistente con personalidad específica."""

    # System instruction define el comportamiento base
    system_instruction = """
    Eres un instructor de programación experto con las siguientes características:
    - Explicas conceptos de forma clara y concisa
    - Usas analogías del mundo real
    - Siempre incluyes un ejemplo de código corto
    - Respondes en español
    - Eres amigable pero profesional
    """

    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        system_instruction=system_instruction
    )

    response = model.generate_content("¿Qué es una lista enlazada?")
    print(response.text)


assistant_with_personality()
```

---

## Manejo de respuestas

```python
from google.generativeai.types import GenerateContentResponse


def analyze_response(response: GenerateContentResponse):
    """Analiza una respuesta de Gemini en detalle."""

    print("=== Análisis de Respuesta ===\n")

    # 1. Texto principal
    print("TEXTO:")
    print(response.text)
    print()

    # 2. Metadata de uso
    usage = response.usage_metadata
    print("USO DE TOKENS:")
    print(f"  Prompt: {usage.prompt_token_count}")
    print(f"  Respuesta: {usage.candidates_token_count}")
    print(f"  Total: {usage.total_token_count}")
    print()

    # 3. Candidatos (puede haber múltiples con candidate_count)
    print(f"CANDIDATOS: {len(response.candidates)}")
    for i, candidate in enumerate(response.candidates):
        print(f"\n  Candidato {i + 1}:")
        print(f"    Razón de fin: {candidate.finish_reason}")

        # Safety ratings
        print("    Ratings de seguridad:")
        for rating in candidate.safety_ratings:
            print(f"      - {rating.category.name}: {rating.probability.name}")

    # 4. Feedback del prompt (si hay problemas)
    if response.prompt_feedback:
        print("\nFEEDBACK DEL PROMPT:")
        print(f"  Block reason: {response.prompt_feedback.block_reason}")


# Ejemplo de uso
model = genai.GenerativeModel("gemini-1.5-flash")
response = model.generate_content("Hola, ¿cómo estás?")
analyze_response(response)
```

---

## Conteo de tokens

```python
def token_counting_demo():
    """Demuestra cómo contar tokens antes de enviar."""
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Texto a analizar
    text = """
    Los transformers son una arquitectura de red neuronal introducida
    en el paper 'Attention Is All You Need' en 2017. Revolucionaron
    el procesamiento de lenguaje natural y son la base de modelos
    como GPT, BERT y Gemini.
    """

    # Contar tokens
    result = model.count_tokens(text)

    print(f"Texto: {text[:50]}...")
    print(f"Tokens: {result.total_tokens}")

    # Estimar costo (aproximado)
    # Gemini 1.5 Flash: ~$0.000075/1K input tokens
    cost_per_1k = 0.000075
    estimated_cost = (result.total_tokens / 1000) * cost_per_1k
    print(f"Costo estimado: ${estimated_cost:.6f}")


token_counting_demo()
```

---

## Ejercicio práctico completo

```python
#!/usr/bin/env python3
"""
Ejercicio: Crear un asistente de código simple.
Este ejercicio integra todos los conceptos del módulo 0.
"""

import os
import google.generativeai as genai
from google.generativeai import GenerationConfig


class CodeAssistant:
    """Asistente simple para ayuda con código."""

    def __init__(self):
        # Configurar API
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY no configurada")

        genai.configure(api_key=api_key)

        # Configurar modelo con personalidad
        system_instruction = """
        Eres un asistente de programación experto. Cuando te pidan código:
        1. Primero explica brevemente el enfoque
        2. Luego proporciona el código comentado
        3. Finalmente, muestra un ejemplo de uso

        Sé conciso pero completo. Usa Python por defecto a menos que
        se especifique otro lenguaje.
        """

        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config=GenerationConfig(
                temperature=0.3,  # Más determinístico para código
                max_output_tokens=2000,
            ),
            system_instruction=system_instruction
        )

        # Iniciar chat para mantener contexto
        self.chat = self.model.start_chat(history=[])

    def ask(self, question: str) -> str:
        """Envía una pregunta y retorna la respuesta."""
        response = self.chat.send_message(question)
        return response.text

    def ask_streaming(self, question: str):
        """Envía pregunta con respuesta en streaming."""
        response = self.chat.send_message(question, stream=True)
        for chunk in response:
            yield chunk.text

    def get_token_count(self, text: str) -> int:
        """Cuenta tokens en un texto."""
        result = self.model.count_tokens(text)
        return result.total_tokens

    def get_history(self) -> list:
        """Retorna el historial de la conversación."""
        history = []
        for msg in self.chat.history:
            history.append({
                "role": msg.role,
                "content": msg.parts[0].text
            })
        return history


def main():
    """Demo interactivo del asistente."""
    print("=" * 50)
    print("Asistente de Código con Gemini")
    print("Escribe 'salir' para terminar")
    print("=" * 50)
    print()

    try:
        assistant = CodeAssistant()
    except ValueError as e:
        print(f"Error: {e}")
        return

    while True:
        # Obtener input del usuario
        try:
            user_input = input("\nTú: ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if not user_input:
            continue

        if user_input.lower() == "salir":
            break

        if user_input.lower() == "historial":
            print("\n--- Historial ---")
            for msg in assistant.get_history():
                role = "Tú" if msg["role"] == "user" else "Gemini"
                print(f"{role}: {msg['content'][:100]}...")
            continue

        # Mostrar respuesta con streaming
        print("\nGemini: ", end="", flush=True)
        for chunk in assistant.ask_streaming(user_input):
            print(chunk, end="", flush=True)
        print()

    print("\n¡Hasta luego!")


if __name__ == "__main__":
    main()
```

---

## Casos de prueba

```python
# test_first_interaction.py
import os
import pytest
import google.generativeai as genai


@pytest.fixture(scope="module")
def model():
    """Configura modelo para tests."""
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        pytest.skip("GOOGLE_API_KEY no configurada")

    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash")


def test_basic_generation(model):
    """Test de generación básica."""
    response = model.generate_content("Di 'hola'")
    assert response.text is not None
    assert len(response.text) > 0
    print(f"✓ Generación básica: {response.text[:30]}")


def test_generation_config(model):
    """Test de configuración de generación."""
    config = genai.GenerationConfig(
        temperature=0,  # Determinístico
        max_output_tokens=10
    )
    response = model.generate_content(
        "¿Cuánto es 2+2?",
        generation_config=config
    )
    assert "4" in response.text
    print(f"✓ Con config: {response.text}")


def test_token_counting(model):
    """Test de conteo de tokens."""
    result = model.count_tokens("Hola mundo")
    assert result.total_tokens > 0
    assert result.total_tokens < 10  # "Hola mundo" debería ser pocos tokens
    print(f"✓ Tokens contados: {result.total_tokens}")


def test_chat_memory(model):
    """Test de memoria en chat."""
    chat = model.start_chat(history=[])

    # Primera mensaje
    chat.send_message("Mi color favorito es el azul")

    # Verificar que recuerda
    response = chat.send_message("¿Cuál es mi color favorito?")
    assert "azul" in response.text.lower()
    print(f"✓ Chat con memoria: {response.text[:50]}")


def test_streaming(model):
    """Test de streaming."""
    response = model.generate_content("Cuenta del 1 al 3", stream=True)

    chunks = list(response)
    assert len(chunks) > 0

    full_text = "".join(chunk.text for chunk in chunks)
    assert "1" in full_text
    print(f"✓ Streaming: {len(chunks)} chunks recibidos")


def test_system_instruction():
    """Test de system instruction."""
    model = genai.GenerativeModel(
        "gemini-1.5-flash",
        system_instruction="Responde siempre en mayúsculas"
    )

    response = model.generate_content("Di hola")
    # Al menos parte debería estar en mayúsculas
    assert any(c.isupper() for c in response.text)
    print(f"✓ System instruction: {response.text[:30]}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## Errores comunes y soluciones

### Error 1: API key inválida
```
google.api_core.exceptions.PermissionDenied: 403 API key not valid
```
**Solución**: Verifica que la API key esté correcta y que la API esté habilitada en GCP.

### Error 2: Cuota excedida
```
google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded
```
**Solución**: Espera o aumenta tus límites de cuota en GCP Console.

### Error 3: Contenido bloqueado
```
response.candidates[0].finish_reason = SAFETY
```
**Solución**: El contenido fue bloqueado por filtros de seguridad. Reformula tu prompt.

---

## Resumen del concepto

**En una frase**: Interactuar con Gemini es tan simple como crear un modelo y llamar `generate_content()` con tu prompt.

**Patrón básico**:
```python
import google.generativeai as genai
genai.configure(api_key="...")
model = genai.GenerativeModel("gemini-1.5-flash")
response = model.generate_content("tu prompt")
print(response.text)
```

**Siguiente paso**: Módulo 1 - Dominio de la API de Gemini, donde exploraremos todas las capacidades avanzadas de la API.

---

## Checkpoint del Módulo 0

Has completado la nivelación. Deberías poder:

1. ✅ Explicar qué son los Transformers y el mecanismo de atención
2. ✅ Describir cómo los LLMs generan texto token por token
3. ✅ Diferenciar entre modelos Gemini (Pro, Flash)
4. ✅ Configurar GCP y obtener una API key de forma segura
5. ✅ Instalar y configurar el SDK de Python
6. ✅ Hacer llamadas básicas a la API y procesar respuestas

**Entregable del Módulo 0**: Script funcional que:
- Se conecta a Gemini
- Mantiene una conversación de 3 turnos
- Muestra conteo de tokens
- Maneja errores apropiadamente
