# Modelos de Lenguaje y Generación de Texto

**Tiempo estimado**: 40 minutos
**Nivel**: Básico
**Prerrequisitos**: Arquitectura Transformer (0.1.1), probabilidad básica

## ¿Por qué importa este concepto?

Los modelos de lenguaje son el corazón de Gemini y todos los LLMs modernos. Entender cómo funcionan te permite predecir su comportamiento, optimizar tus prompts, y comprender por qué a veces "alucinan" o generan respuestas inesperadas.

Un modelo de lenguaje hace una cosa: predecir la probabilidad del siguiente token dada una secuencia. Esta tarea aparentemente simple, cuando se escala masivamente, produce comportamiento emergente que parece "inteligencia": razonamiento, creatividad, seguimiento de instrucciones.

## Conexión con conocimientos previos

En el subtema anterior vimos cómo los Transformers procesan secuencias usando atención. Ahora veremos cómo esa arquitectura se entrena para la tarea específica de predecir texto, y cómo esa predicción se convierte en generación de respuestas coherentes.

---

## Comprensión intuitiva

Imagina que estás jugando a completar oraciones:
- "El cielo es ___" → probablemente "azul"
- "2 + 2 = ___" → probablemente "4"
- "def fibonacci(n): ___" → probablemente código Python válido

Un modelo de lenguaje hace exactamente esto, pero con miles de millones de parámetros entrenados en billones de palabras. Ha visto tantos patrones que puede predecir no solo palabras comunes, sino también código, razonamiento matemático, y respuestas a preguntas complejas.

### Ejemplo motivador

Cuando escribes a Gemini: "Explica la fotosíntesis", el modelo no "sabe" fotosíntesis como un humano. En su lugar:
1. Procesa tu pregunta
2. Predice que el siguiente token más probable es algo como "La"
3. Dado "La", predice "fotosíntesis"
4. Dado "La fotosíntesis", predice "es"
5. Continúa token por token hasta generar una respuesta completa

Cada predicción está informada por todo el contexto anterior y los patrones aprendidos durante el entrenamiento.

---

## Definición formal

### Modelo de Lenguaje Autoregresivo

Un modelo de lenguaje autoregresivo modela la distribución de probabilidad:

```
P(x₁, x₂, ..., xₙ) = P(x₁) × P(x₂|x₁) × P(x₃|x₁,x₂) × ... × P(xₙ|x₁,...,xₙ₋₁)
```

Usando la regla de la cadena, descompone la probabilidad conjunta en probabilidades condicionales. El modelo aprende a estimar `P(xₜ | x₁, ..., xₜ₋₁)` para cualquier secuencia.

### Tokenización

Antes de procesar texto, este se divide en **tokens**:
- Un token puede ser una palabra completa: "gato" → [gato]
- O subpalabras: "fotosíntesis" → [foto, síntesis]
- O caracteres individuales en casos especiales

El vocabulario típico tiene 32K-100K tokens. Gemini usa un tokenizador basado en SentencePiece.

### Función de Pérdida: Cross-Entropy

El modelo se entrena minimizando la cross-entropy loss:

```
L = -1/N × Σᵢ log P(xᵢ | x₁, ..., xᵢ₋₁)
```

Esto equivale a maximizar la log-probabilidad del siguiente token correcto. Un modelo perfecto tendría loss = 0 (siempre predice el token correcto con probabilidad 1).

### Propiedades fundamentales

1. **Autoregresivo**: Genera un token a la vez, condicionado en todos los anteriores
2. **Causal**: Solo puede ver el pasado, no el futuro (máscara causal en atención)
3. **Probabilístico**: Produce una distribución de probabilidad sobre el vocabulario
4. **Stateless**: No tiene memoria entre llamadas diferentes (solo el contexto dado)

---

## Implementación práctica

### Proceso de generación

```
función generar_texto(modelo, prompt, max_tokens, temperatura):
    tokens = tokenizar(prompt)

    para i en rango(max_tokens):
        # Obtener probabilidades del siguiente token
        logits = modelo.forward(tokens)
        probs = softmax(logits[-1] / temperatura)

        # Muestrear siguiente token
        siguiente_token = muestrear(probs)

        # Agregar a la secuencia
        tokens.append(siguiente_token)

        # Parar si es token de fin
        si siguiente_token == TOKEN_FIN:
            break

    return destokenizar(tokens)
```

### Implementación en Python

```python
import numpy as np
from typing import List, Callable
from dataclasses import dataclass


@dataclass
class GenerationConfig:
    """Configuración para generación de texto."""
    max_tokens: int = 100
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.9
    stop_tokens: List[int] = None


def softmax(logits: np.ndarray) -> np.ndarray:
    """Convierte logits a probabilidades."""
    exp_logits = np.exp(logits - np.max(logits))
    return exp_logits / np.sum(exp_logits)


def apply_temperature(logits: np.ndarray, temperature: float) -> np.ndarray:
    """
    Aplica temperatura a los logits.

    - temperature < 1: Más determinístico (tokens probables más probables)
    - temperature = 1: Distribución original
    - temperature > 1: Más aleatorio (distribución más uniforme)
    """
    if temperature == 0:
        # Greedy: solo el token más probable
        return logits
    return logits / temperature


def top_k_filtering(logits: np.ndarray, k: int) -> np.ndarray:
    """
    Mantiene solo los top-k tokens más probables.
    Pone -inf en el resto para que softmax los ignore.
    """
    if k <= 0:
        return logits

    indices_to_remove = logits < np.partition(logits, -k)[-k]
    logits[indices_to_remove] = -np.inf
    return logits


def top_p_filtering(logits: np.ndarray, p: float) -> np.ndarray:
    """
    Nucleus sampling: mantiene tokens cuya probabilidad acumulada <= p.
    Permite vocabulario dinámico según la certeza del modelo.
    """
    if p >= 1.0:
        return logits

    sorted_indices = np.argsort(logits)[::-1]
    sorted_logits = logits[sorted_indices]
    cumulative_probs = np.cumsum(softmax(sorted_logits))

    # Encontrar punto de corte
    sorted_indices_to_remove = cumulative_probs > p
    # Siempre mantener al menos el token más probable
    sorted_indices_to_remove[0] = False

    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    logits[indices_to_remove] = -np.inf
    return logits


def sample_token(probs: np.ndarray) -> int:
    """Muestrea un token de la distribución de probabilidad."""
    return np.random.choice(len(probs), p=probs)


def greedy_decode(probs: np.ndarray) -> int:
    """Selecciona el token más probable (determinístico)."""
    return np.argmax(probs)


class LanguageModelGenerator:
    """
    Simulador de generación de texto para un modelo de lenguaje.
    En producción, esto sería reemplazado por llamadas reales a Gemini.
    """

    def __init__(
        self,
        vocab_size: int = 32000,
        model_fn: Callable = None
    ):
        """
        Args:
            vocab_size: Tamaño del vocabulario
            model_fn: Función que toma tokens y retorna logits
                      Si None, usa distribución simulada
        """
        self.vocab_size = vocab_size
        self.model_fn = model_fn or self._dummy_model

    def _dummy_model(self, tokens: List[int]) -> np.ndarray:
        """Modelo dummy que retorna logits aleatorios (solo para demo)."""
        # En la realidad, aquí iría el forward pass del transformer
        np.random.seed(sum(tokens) % 1000)  # Reproducibilidad por contexto
        return np.random.randn(self.vocab_size)

    def generate(
        self,
        input_tokens: List[int],
        config: GenerationConfig
    ) -> List[int]:
        """
        Genera tokens de manera autoregresiva.

        Args:
            input_tokens: Tokens del prompt
            config: Configuración de generación

        Returns:
            Lista de tokens generados (incluyendo el prompt)
        """
        tokens = input_tokens.copy()
        stop_tokens = config.stop_tokens or []

        for _ in range(config.max_tokens):
            # 1. Forward pass: obtener logits para el siguiente token
            logits = self.model_fn(tokens)

            # 2. Aplicar temperatura
            logits = apply_temperature(logits, config.temperature)

            # 3. Aplicar filtrado top-k
            if config.top_k > 0:
                logits = top_k_filtering(logits.copy(), config.top_k)

            # 4. Aplicar filtrado top-p (nucleus sampling)
            if config.top_p < 1.0:
                logits = top_p_filtering(logits.copy(), config.top_p)

            # 5. Convertir a probabilidades
            probs = softmax(logits)

            # 6. Muestrear siguiente token
            if config.temperature == 0:
                next_token = greedy_decode(probs)
            else:
                next_token = sample_token(probs)

            tokens.append(next_token)

            # 7. Verificar condición de parada
            if next_token in stop_tokens:
                break

        return tokens

    def generate_with_beam_search(
        self,
        input_tokens: List[int],
        beam_width: int = 5,
        max_tokens: int = 50
    ) -> List[List[int]]:
        """
        Búsqueda por haz: mantiene múltiples hipótesis en paralelo.
        Encuentra secuencias más probables que greedy.

        Args:
            input_tokens: Tokens del prompt
            beam_width: Número de hipótesis a mantener
            max_tokens: Máximo de tokens a generar

        Returns:
            Lista de las mejores secuencias encontradas
        """
        # Cada beam es (tokens, log_prob_acumulada)
        beams = [(input_tokens.copy(), 0.0)]

        for _ in range(max_tokens):
            all_candidates = []

            for tokens, score in beams:
                logits = self.model_fn(tokens)
                log_probs = np.log(softmax(logits) + 1e-10)

                # Expandir con los top-k tokens
                top_indices = np.argsort(log_probs)[-beam_width:]

                for idx in top_indices:
                    new_tokens = tokens + [idx]
                    new_score = score + log_probs[idx]
                    all_candidates.append((new_tokens, new_score))

            # Mantener solo los mejores beams
            all_candidates.sort(key=lambda x: x[1], reverse=True)
            beams = all_candidates[:beam_width]

        return [tokens for tokens, _ in beams]


# Ejemplo de uso con tokenizador simulado
class SimpleTokenizer:
    """Tokenizador simplificado para demostración."""

    def __init__(self):
        # Vocabulario mínimo para demo
        self.vocab = {
            "<pad>": 0, "<eos>": 1, "<unk>": 2,
            "el": 3, "la": 4, "es": 5, "un": 6, "una": 7,
            "gato": 8, "perro": 9, "casa": 10, "grande": 11,
            "pequeño": 12, "rojo": 13, "azul": 14, ".": 15
        }
        self.inverse_vocab = {v: k for k, v in self.vocab.items()}

    def encode(self, text: str) -> List[int]:
        """Convierte texto a tokens."""
        words = text.lower().split()
        return [self.vocab.get(w, self.vocab["<unk>"]) for w in words]

    def decode(self, tokens: List[int]) -> str:
        """Convierte tokens a texto."""
        words = [self.inverse_vocab.get(t, "<unk>") for t in tokens]
        return " ".join(words)
```

### Casos de prueba

```python
# Test 1: Verificar que softmax produce distribución válida
print("Test 1: Softmax produce probabilidades válidas")
logits = np.array([2.0, 1.0, 0.1])
probs = softmax(logits)
assert np.isclose(probs.sum(), 1.0), f"Suma no es 1: {probs.sum()}"
assert all(p >= 0 for p in probs), "Probabilidades negativas"
print(f"✓ Softmax correcto: {probs}")

# Test 2: Temperatura afecta distribución
print("\nTest 2: Temperatura modifica distribución")
logits = np.array([5.0, 2.0, 1.0])
probs_t1 = softmax(apply_temperature(logits.copy(), 1.0))
probs_t01 = softmax(apply_temperature(logits.copy(), 0.1))
probs_t2 = softmax(apply_temperature(logits.copy(), 2.0))

# Temperatura baja = más concentrado en el máximo
assert probs_t01[0] > probs_t1[0] > probs_t2[0], "Temperatura no funciona correctamente"
print(f"✓ T=0.1: {probs_t01[0]:.3f}, T=1: {probs_t1[0]:.3f}, T=2: {probs_t2[0]:.3f}")

# Test 3: Top-k filtering
print("\nTest 3: Top-k filtering")
logits = np.array([5.0, 3.0, 2.0, 1.0, 0.5])
filtered = top_k_filtering(logits.copy(), k=2)
valid_count = np.sum(filtered > -np.inf)
assert valid_count == 2, f"Top-k no filtró correctamente: {valid_count} válidos"
print(f"✓ Top-k=2 mantiene exactamente 2 tokens")

# Test 4: Generación produce tokens
print("\nTest 4: Generación produce secuencia")
generator = LanguageModelGenerator(vocab_size=100)
config = GenerationConfig(max_tokens=10, temperature=0.8)
result = generator.generate([1, 2, 3], config)
assert len(result) > 3, "No generó tokens nuevos"
assert len(result) <= 13, "Generó demasiados tokens"
print(f"✓ Generó {len(result) - 3} tokens nuevos")

# Test 5: Beam search produce múltiples secuencias
print("\nTest 5: Beam search")
beams = generator.generate_with_beam_search([1, 2], beam_width=3, max_tokens=5)
assert len(beams) == 3, f"Beam search no retornó 3 beams: {len(beams)}"
print(f"✓ Beam search retornó {len(beams)} secuencias")

print("\n✅ Todos los tests pasaron")
```

### Análisis de complejidad

**Generación autoregresiva**:
- **Temporal**: O(n × T × V) donde n = longitud contexto, T = tokens generados, V = tamaño vocabulario
- Cada token requiere un forward pass completo del modelo
- **Espacial**: O(n × d) para el KV-cache (optimización común)

**Beam search**:
- **Temporal**: O(n × T × V × B²) donde B = beam width
- Más costoso pero produce secuencias de mayor calidad

---

## Estrategias de muestreo

### Greedy Decoding
Siempre elige el token más probable. Rápido pero puede producir texto repetitivo.

```python
next_token = np.argmax(probs)
```

**Cuándo usar**: Tareas con respuesta única correcta (ej: clasificación, extracción)

### Sampling con Temperatura
Muestrea de la distribución ajustada por temperatura.

```python
probs = softmax(logits / temperature)
next_token = np.random.choice(vocab_size, p=probs)
```

**Cuándo usar**: Tareas creativas donde queremos variedad

### Top-k Sampling
Restringe el muestreo a los k tokens más probables.

```python
top_k_indices = np.argsort(logits)[-k:]
# Muestrear solo de estos k tokens
```

**Cuándo usar**: Evitar tokens muy improbables mientras se mantiene diversidad

### Nucleus (Top-p) Sampling
Muestrea del conjunto mínimo de tokens cuya probabilidad acumulada excede p.

**Cuándo usar**: Adaptarse dinámicamente a la certeza del modelo

---

## Errores frecuentes

### Error 1: No usar KV-cache en producción

```python
# INCORRECTO - recalcula todo cada token
def slow_generate(model, tokens, max_new):
    for _ in range(max_new):
        # Forward pass completo cada vez
        logits = model.forward(tokens)
        next_token = sample(logits[-1])
        tokens.append(next_token)
    return tokens
```

**Por qué falla**: El costo crece cuadráticamente con la longitud. Con KV-cache, las keys y values de tokens anteriores se reusan.

### Solución correcta

```python
# CORRECTO - usa caché incremental
def fast_generate(model, tokens, max_new):
    # Primer forward: procesa todo el prompt
    logits, kv_cache = model.forward(tokens, use_cache=True)

    for _ in range(max_new):
        next_token = sample(logits[-1])
        tokens.append(next_token)
        # Forward incremental: solo procesa el nuevo token
        logits, kv_cache = model.forward(
            [next_token],
            past_kv=kv_cache,
            use_cache=True
        )
    return tokens
```

### Error 2: Temperatura 0 con sampling

```python
# INCORRECTO - división por cero o comportamiento indefinido
probs = softmax(logits / 0)  # ¡Error!
```

**Por qué falla**: Temperatura 0 requiere manejo especial (greedy decoding).

### Solución correcta

```python
# CORRECTO - caso especial para temperatura 0
if temperature == 0:
    next_token = np.argmax(logits)
else:
    probs = softmax(logits / temperature)
    next_token = np.random.choice(len(probs), p=probs)
```

---

## Visualización del concepto

### Proceso de generación token por token

```
Prompt: "El gato"
         ↓
┌─────────────────────────────────────────────┐
│  Modelo predice distribución sobre vocab:   │
│  "es": 0.3, "negro": 0.2, "duerme": 0.15... │
└─────────────────────────────────────────────┘
         ↓ (muestrear con temperatura)
Token seleccionado: "es"
         ↓
Nueva secuencia: "El gato es"
         ↓
┌─────────────────────────────────────────────┐
│  Modelo predice siguiente token:            │
│  "un": 0.25, "muy": 0.2, "negro": 0.18...   │
└─────────────────────────────────────────────┘
         ↓
Token seleccionado: "muy"
         ↓
... continúa hasta max_tokens o token de fin
```

### Efecto de temperatura

```
Logits originales: [gato: 5.0, perro: 2.0, mesa: 0.1]

T=0.1 (muy determinístico):
  gato: 99.9%, perro: 0.1%, mesa: ~0%

T=1.0 (original):
  gato: 88%, perro: 10%, mesa: 2%

T=2.0 (más aleatorio):
  gato: 60%, perro: 28%, mesa: 12%
```

---

## Casos de uso en producción

### Aplicación 1: Chatbots conversacionales
**Contexto**: Responder preguntas de usuarios de manera natural
**Configuración típica**: temperature=0.7, top_p=0.9
**Por qué**: Balance entre coherencia y naturalidad

### Aplicación 2: Generación de código
**Contexto**: Autocompletado y generación de funciones
**Configuración típica**: temperature=0.2, top_p=0.95
**Por qué**: Código debe ser sintácticamente correcto, poco margen para creatividad

### Aplicación 3: Escritura creativa
**Contexto**: Generar historias, poemas, ideas
**Configuración típica**: temperature=1.0-1.2, top_k=100
**Por qué**: Maximizar diversidad y creatividad

---

## ¿Cuándo usar cada estrategia?

| Tarea | Temperatura | Top-p | Top-k | Justificación |
|-------|-------------|-------|-------|---------------|
| Extracción de datos | 0 | - | - | Respuesta única correcta |
| Código | 0.1-0.3 | 0.95 | - | Sintaxis estricta |
| Q&A factual | 0.3-0.5 | 0.9 | - | Precisión con variación |
| Chat general | 0.7-0.9 | 0.9 | - | Natural pero coherente |
| Escritura creativa | 1.0-1.2 | 0.95 | 100 | Máxima diversidad |

---

## Para ir más allá

### Papers fundamentales

1. Holtzman et al., 2020 - "The Curious Case of Neural Text Degeneration" - Introdujo nucleus sampling
2. Fan et al., 2018 - "Hierarchical Neural Story Generation" - Top-k sampling
3. Radford et al., 2019 - "Language Models are Unsupervised Multitask Learners" (GPT-2) - Demostró generación de alta calidad

### Implementaciones de referencia

- **HuggingFace generate()**: Implementación completa con todas las estrategias
- **vLLM**: Servidor de inferencia optimizado con batching continuo

### Preguntas abiertas

- **Especulación**: ¿Se puede acelerar generación prediciendo múltiples tokens?
- **Coherencia a largo plazo**: ¿Cómo mantener consistencia en textos muy largos?
- **Control fino**: ¿Cómo guiar la generación hacia estilos o temas específicos?

---

## Resumen del concepto

**En una frase**: Los modelos de lenguaje generan texto prediciendo el siguiente token más probable dado el contexto, token por token.

**Cuándo ajustar temperatura**: Baja para tareas precisas, alta para tareas creativas.

**Complejidad de generación**: Tiempo O(n × T) por token generado (con KV-cache).

**Prerequisito crítico**: Entender que el modelo solo predice probabilidades; el muestreo determina la salida final.

**Siguiente paso**: Módulo 0.1.3 - Familia de Modelos Gemini, donde veremos las variantes específicas de Gemini y sus capacidades.
