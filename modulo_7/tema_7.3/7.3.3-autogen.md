# 7.3.3 AutoGen para Conversaciones Multi-Agente

## Objetivo de Aprendizaje

Implementar sistemas de conversación multi-agente usando AutoGen (Microsoft), creando agentes que interactúan mediante diálogos naturales, con integración de Google Gemini como modelo de razonamiento.

## Introducción

AutoGen es un framework de Microsoft que permite crear agentes conversacionales que colaboran mediante diálogos. Su enfoque en la conversación natural lo hace ideal para sistemas donde los agentes necesitan debatir, negociar o resolver problemas colaborativamente.

```
┌─────────────────────────────────────────────────────────────────┐
│                    AUTOGEN ARCHITECTURE                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                   CONVERSATION                           │   │
│  │                                                          │   │
│  │   Agent A ──────────────────────────────> Agent B       │   │
│  │      │     "Propongo solución X"              │         │   │
│  │      │                                        │         │   │
│  │      │ <─────────────────────────────────────┘         │   │
│  │      │  "Considero que Y sería mejor porque..."        │   │
│  │      │                                                  │   │
│  │      v                                                  │   │
│  │   Agent A ──────────────────────────────> Agent B       │   │
│  │           "Acepto Y, procedamos con..."                │   │
│  │                                                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  TIPOS DE AGENTES:                                              │
│  ├─ AssistantAgent: Agente con LLM                             │
│  ├─ UserProxyAgent: Proxy de usuario (puede ejecutar código)  │
│  ├─ GroupChat: Conversación grupal                             │
│  └─ ConversableAgent: Agente base customizable                 │
└─────────────────────────────────────────────────────────────────┘
```

## Configuración con Google Gemini

```python
"""
Configuración de AutoGen con Google Gemini.
"""
# pip install pyautogen google-generativeai

import autogen
from autogen import AssistantAgent, UserProxyAgent, ConversableAgent
import os
from typing import Dict, List, Any, Optional

# Configuración para Gemini
def get_gemini_config() -> Dict[str, Any]:
    """Obtiene configuración para usar Gemini con AutoGen."""
    return {
        "config_list": [
            {
                "model": "gemini-1.5-flash",
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "api_type": "google"
            }
        ],
        "temperature": 0.7,
        "timeout": 120
    }


# Configuración alternativa usando OpenAI-compatible endpoint
def get_gemini_openai_config() -> Dict[str, Any]:
    """Configuración usando endpoint compatible con OpenAI."""
    return {
        "config_list": [
            {
                "model": "gemini-1.5-flash",
                "base_url": "https://generativelanguage.googleapis.com/v1beta/openai/",
                "api_key": os.getenv("GOOGLE_API_KEY"),
            }
        ],
        "temperature": 0.7
    }


# Wrapper para usar Gemini directamente
import google.generativeai as genai

class GeminiWrapper:
    """Wrapper para usar Gemini con AutoGen."""

    def __init__(self, model_name: str = "gemini-1.5-flash"):
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.model = genai.GenerativeModel(model_name)
        self.chat = None

    def create_chat(self):
        """Crea nueva sesión de chat."""
        self.chat = self.model.start_chat(history=[])
        return self.chat

    def send_message(self, message: str) -> str:
        """Envía mensaje y obtiene respuesta."""
        if self.chat is None:
            self.create_chat()
        response = self.chat.send_message(message)
        return response.text

    def generate(self, prompt: str) -> str:
        """Genera respuesta sin contexto de chat."""
        response = self.model.generate_content(prompt)
        return response.text
```

## Agentes Básicos con AutoGen

```python
"""
Implementación de agentes básicos con AutoGen y Gemini.
"""
import autogen
from autogen import AssistantAgent, UserProxyAgent, ConversableAgent
import google.generativeai as genai
from typing import Dict, List, Any, Optional, Callable, Union
import os


class GeminiAgent(ConversableAgent):
    """Agente que usa Gemini directamente."""

    def __init__(
        self,
        name: str,
        system_message: str = "",
        model_name: str = "gemini-1.5-flash",
        **kwargs
    ):
        super().__init__(name=name, **kwargs)
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.model = genai.GenerativeModel(
            model_name,
            system_instruction=system_message if system_message else None
        )
        self.chat_session = None
        self._system_message = system_message

    def generate_reply(
        self,
        messages: Optional[List[Dict]] = None,
        sender: Optional["ConversableAgent"] = None,
        **kwargs
    ) -> Union[str, Dict, None]:
        """Genera respuesta usando Gemini."""
        if messages is None or len(messages) == 0:
            return None

        # Obtener último mensaje
        last_message = messages[-1]
        content = last_message.get("content", "")

        # Construir contexto de conversación
        if self.chat_session is None:
            self.chat_session = self.model.start_chat(history=[])

        # Generar respuesta
        try:
            response = self.chat_session.send_message(content)
            return response.text
        except Exception as e:
            return f"Error generando respuesta: {str(e)}"

    def reset(self):
        """Resetea el agente."""
        super().reset()
        self.chat_session = None


class SpecializedGeminiAgent(GeminiAgent):
    """Agente Gemini con especialización."""

    def __init__(
        self,
        name: str,
        role: str,
        expertise: List[str],
        model_name: str = "gemini-1.5-flash",
        **kwargs
    ):
        expertise_text = ", ".join(expertise)
        system_message = f"""Eres un {role} especializado en {expertise_text}.

Tu comportamiento:
- Responde siempre desde tu área de expertise
- Sé conciso pero informativo
- Si algo está fuera de tu expertise, indícalo claramente
- Colabora constructivamente con otros agentes"""

        super().__init__(
            name=name,
            system_message=system_message,
            model_name=model_name,
            **kwargs
        )
        self.role = role
        self.expertise = expertise


def create_assistant_agent(
    name: str,
    system_message: str,
    llm_config: Dict[str, Any] = None
) -> AssistantAgent:
    """Crea un AssistantAgent configurado."""
    config = llm_config or get_gemini_openai_config()

    return AssistantAgent(
        name=name,
        system_message=system_message,
        llm_config=config
    )


def create_user_proxy(
    name: str = "user_proxy",
    human_input_mode: str = "NEVER",
    code_execution_config: Dict = None
) -> UserProxyAgent:
    """Crea un UserProxyAgent."""
    return UserProxyAgent(
        name=name,
        human_input_mode=human_input_mode,
        code_execution_config=code_execution_config or {"use_docker": False},
        max_consecutive_auto_reply=10
    )


# Ejemplo de uso básico
if __name__ == "__main__":
    # Crear agentes especializados
    researcher = SpecializedGeminiAgent(
        name="Researcher",
        role="Investigador Senior",
        expertise=["análisis de datos", "investigación de mercado", "tendencias tecnológicas"]
    )

    analyst = SpecializedGeminiAgent(
        name="Analyst",
        role="Analista de Negocios",
        expertise=["análisis financiero", "estrategia", "modelado de negocios"]
    )

    # Simular conversación
    print("=== Conversación entre agentes ===")

    # Mensaje inicial
    initial_message = "¿Cuáles son las principales tendencias en IA para 2024?"

    # Researcher responde
    researcher_response = researcher.generate_reply(
        messages=[{"role": "user", "content": initial_message}]
    )
    print(f"\nResearcher: {researcher_response[:500]}...")

    # Analyst responde al researcher
    analyst_response = analyst.generate_reply(
        messages=[
            {"role": "user", "content": initial_message},
            {"role": "assistant", "content": researcher_response}
        ]
    )
    print(f"\nAnalyst: {analyst_response[:500]}...")
```

## Conversaciones Grupales

```python
"""
Implementación de chats grupales con múltiples agentes.
"""
import autogen
from autogen import GroupChat, GroupChatManager
import google.generativeai as genai
from typing import Dict, List, Any, Optional
import os


class GeminiGroupChat:
    """Chat grupal usando agentes Gemini."""

    def __init__(self, agents: List[GeminiAgent], max_rounds: int = 10):
        self.agents = {agent.name: agent for agent in agents}
        self.max_rounds = max_rounds
        self.conversation_history: List[Dict[str, Any]] = []
        self.current_speaker_idx = 0

        # Modelo coordinador para seleccionar siguiente hablante
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.coordinator = genai.GenerativeModel("gemini-1.5-flash")

    def select_next_speaker(self) -> str:
        """Selecciona el siguiente agente para hablar."""
        if not self.conversation_history:
            return list(self.agents.keys())[0]

        # Usar Gemini para seleccionar siguiente hablante
        agent_names = list(self.agents.keys())
        history_summary = "\n".join([
            f"{msg['speaker']}: {msg['content'][:100]}..."
            for msg in self.conversation_history[-5:]
        ])

        prompt = f"""Dado el siguiente historial de conversación entre agentes:

{history_summary}

Agentes disponibles: {agent_names}

¿Quién debería hablar a continuación para hacer avanzar la conversación?
Responde SOLO con el nombre del agente."""

        response = self.coordinator.generate_content(prompt)
        suggested = response.text.strip()

        # Validar que el agente existe
        if suggested in self.agents:
            return suggested

        # Fallback: round-robin
        self.current_speaker_idx = (self.current_speaker_idx + 1) % len(agent_names)
        return agent_names[self.current_speaker_idx]

    def run_conversation(
        self,
        initial_message: str,
        initial_speaker: str = None
    ) -> List[Dict[str, Any]]:
        """Ejecuta una conversación grupal."""
        self.conversation_history = []

        # Mensaje inicial
        current_speaker = initial_speaker or list(self.agents.keys())[0]
        current_message = initial_message

        for round_num in range(self.max_rounds):
            print(f"\n--- Ronda {round_num + 1} ---")

            # Obtener respuesta del agente actual
            agent = self.agents[current_speaker]

            # Construir mensajes para el agente
            messages = [{"role": "user", "content": current_message}]
            if self.conversation_history:
                messages = [
                    {"role": "user" if m["speaker"] != current_speaker else "assistant",
                     "content": f"[{m['speaker']}]: {m['content']}"}
                    for m in self.conversation_history[-5:]
                ] + messages

            response = agent.generate_reply(messages=messages)

            # Registrar en historial
            self.conversation_history.append({
                "round": round_num + 1,
                "speaker": current_speaker,
                "content": response
            })

            print(f"{current_speaker}: {response[:300]}...")

            # Verificar si la conversación debe terminar
            if self._should_end_conversation(response):
                print("\n[Conversación finalizada por consenso]")
                break

            # Seleccionar siguiente hablante
            current_speaker = self.select_next_speaker()
            current_message = response

        return self.conversation_history

    def _should_end_conversation(self, last_response: str) -> bool:
        """Determina si la conversación debe terminar."""
        end_signals = [
            "consenso alcanzado",
            "todos de acuerdo",
            "conclusión final",
            "tarea completada",
            "no hay más que discutir"
        ]
        return any(signal in last_response.lower() for signal in end_signals)

    def get_summary(self) -> str:
        """Genera resumen de la conversación."""
        if not self.conversation_history:
            return "No hay conversación que resumir."

        full_conversation = "\n".join([
            f"{msg['speaker']}: {msg['content']}"
            for msg in self.conversation_history
        ])

        prompt = f"""Resume la siguiente conversación entre agentes:

{full_conversation}

Proporciona:
1. Tema principal discutido
2. Puntos clave de cada participante
3. Conclusiones o acuerdos alcanzados
4. Temas pendientes (si hay)"""

        response = self.coordinator.generate_content(prompt)
        return response.text


# Implementación con AutoGen nativo
def create_autogen_group_chat(
    agents: List,
    max_round: int = 10,
    speaker_selection_method: str = "auto"
) -> GroupChat:
    """Crea un GroupChat de AutoGen."""
    return GroupChat(
        agents=agents,
        messages=[],
        max_round=max_round,
        speaker_selection_method=speaker_selection_method
    )


def run_autogen_group_discussion(
    topic: str,
    agents: List,
    max_rounds: int = 10
):
    """Ejecuta discusión grupal con AutoGen."""
    llm_config = get_gemini_openai_config()

    # Crear GroupChat
    group_chat = GroupChat(
        agents=agents,
        messages=[],
        max_round=max_rounds
    )

    # Crear manager
    manager = GroupChatManager(
        groupchat=group_chat,
        llm_config=llm_config
    )

    # Iniciar discusión
    user_proxy = UserProxyAgent(
        name="User",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=0
    )

    user_proxy.initiate_chat(
        manager,
        message=topic
    )

    return group_chat.messages


# Ejemplo de uso
if __name__ == "__main__":
    # Crear agentes
    tech_expert = SpecializedGeminiAgent(
        name="TechExpert",
        role="Experto en Tecnología",
        expertise=["IA", "cloud computing", "arquitectura de software"]
    )

    business_analyst = SpecializedGeminiAgent(
        name="BusinessAnalyst",
        role="Analista de Negocios",
        expertise=["estrategia", "ROI", "análisis de mercado"]
    )

    risk_manager = SpecializedGeminiAgent(
        name="RiskManager",
        role="Gestor de Riesgos",
        expertise=["evaluación de riesgos", "compliance", "seguridad"]
    )

    # Crear chat grupal
    group_chat = GeminiGroupChat(
        agents=[tech_expert, business_analyst, risk_manager],
        max_rounds=6
    )

    # Ejecutar conversación
    history = group_chat.run_conversation(
        initial_message="Debatamos sobre la adopción de IA generativa en una empresa de servicios financieros. ¿Cuáles son los beneficios, riesgos y recomendaciones?",
        initial_speaker="TechExpert"
    )

    # Obtener resumen
    print("\n=== RESUMEN ===")
    print(group_chat.get_summary())
```

## Agentes con Capacidad de Código

```python
"""
Agentes que pueden escribir y ejecutar código.
"""
import autogen
from autogen import AssistantAgent, UserProxyAgent
import google.generativeai as genai
from typing import Dict, List, Any, Optional
import subprocess
import tempfile
import os


class CodeExecutorAgent(GeminiAgent):
    """Agente que puede ejecutar código Python."""

    def __init__(
        self,
        name: str,
        allowed_modules: List[str] = None,
        **kwargs
    ):
        system_message = """Eres un asistente de programación experto.
Cuando necesites ejecutar código:
1. Escribe el código entre ```python y ```
2. El código será ejecutado automáticamente
3. Recibirás el resultado de la ejecución
4. Analiza el resultado y proporciona insights

Solo usa módulos seguros y estándar de Python."""

        super().__init__(
            name=name,
            system_message=system_message,
            **kwargs
        )
        self.allowed_modules = allowed_modules or [
            "math", "json", "datetime", "collections",
            "itertools", "functools", "re", "statistics"
        ]
        self.execution_history: List[Dict[str, Any]] = []

    def execute_code(self, code: str) -> Dict[str, Any]:
        """Ejecuta código Python de forma segura."""
        # Verificar módulos
        for module in self._extract_imports(code):
            if module not in self.allowed_modules:
                return {
                    "success": False,
                    "output": f"Módulo no permitido: {module}",
                    "error": "SecurityError"
                }

        # Crear archivo temporal
        with tempfile.NamedTemporaryFile(
            mode="w",
            suffix=".py",
            delete=False
        ) as f:
            f.write(code)
            temp_file = f.name

        try:
            # Ejecutar
            result = subprocess.run(
                ["python", temp_file],
                capture_output=True,
                text=True,
                timeout=30
            )

            execution_result = {
                "success": result.returncode == 0,
                "output": result.stdout,
                "error": result.stderr if result.returncode != 0 else None
            }

        except subprocess.TimeoutExpired:
            execution_result = {
                "success": False,
                "output": "",
                "error": "Timeout: ejecución excedió 30 segundos"
            }
        except Exception as e:
            execution_result = {
                "success": False,
                "output": "",
                "error": str(e)
            }
        finally:
            os.unlink(temp_file)

        self.execution_history.append({
            "code": code,
            "result": execution_result
        })

        return execution_result

    def _extract_imports(self, code: str) -> List[str]:
        """Extrae nombres de módulos importados."""
        imports = []
        for line in code.split("\n"):
            line = line.strip()
            if line.startswith("import "):
                module = line.split()[1].split(".")[0]
                imports.append(module)
            elif line.startswith("from "):
                module = line.split()[1].split(".")[0]
                imports.append(module)
        return imports

    def generate_reply(
        self,
        messages: Optional[List[Dict]] = None,
        sender: Optional["ConversableAgent"] = None,
        **kwargs
    ) -> str:
        """Genera respuesta y ejecuta código si está presente."""
        # Obtener respuesta base
        response = super().generate_reply(messages, sender, **kwargs)

        # Buscar bloques de código
        if "```python" in response:
            code_blocks = self._extract_code_blocks(response)

            for code in code_blocks:
                result = self.execute_code(code)

                if result["success"]:
                    response += f"\n\n**Resultado de ejecución:**\n```\n{result['output']}\n```"
                else:
                    response += f"\n\n**Error en ejecución:**\n```\n{result['error']}\n```"

        return response

    def _extract_code_blocks(self, text: str) -> List[str]:
        """Extrae bloques de código Python."""
        blocks = []
        parts = text.split("```python")

        for part in parts[1:]:
            if "```" in part:
                code = part.split("```")[0].strip()
                blocks.append(code)

        return blocks


class CodingTeam:
    """Equipo de agentes para desarrollo de código."""

    def __init__(self):
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

        self.architect = SpecializedGeminiAgent(
            name="Architect",
            role="Arquitecto de Software",
            expertise=["diseño de sistemas", "patrones de diseño", "arquitectura"]
        )

        self.developer = CodeExecutorAgent(
            name="Developer",
            allowed_modules=[
                "math", "json", "datetime", "collections",
                "itertools", "functools", "re", "typing"
            ]
        )

        self.reviewer = SpecializedGeminiAgent(
            name="Reviewer",
            role="Code Reviewer",
            expertise=["code review", "mejores prácticas", "optimización"]
        )

    def develop_solution(self, requirement: str) -> Dict[str, Any]:
        """Desarrolla una solución en equipo."""
        results = {"phases": []}

        # Fase 1: Arquitecto diseña
        print("\n=== Fase 1: Diseño ===")
        design_prompt = f"""Diseña la arquitectura para: {requirement}

Incluye:
1. Componentes principales
2. Interfaces
3. Flujo de datos
4. Pseudocódigo de alto nivel"""

        design = self.architect.generate_reply(
            messages=[{"role": "user", "content": design_prompt}]
        )
        results["phases"].append({"phase": "design", "output": design})
        print(f"Architect: {design[:500]}...")

        # Fase 2: Developer implementa
        print("\n=== Fase 2: Implementación ===")
        impl_prompt = f"""Basándote en este diseño, implementa el código Python:

{design}

Requisito original: {requirement}

Proporciona código funcional y bien documentado."""

        implementation = self.developer.generate_reply(
            messages=[{"role": "user", "content": impl_prompt}]
        )
        results["phases"].append({"phase": "implementation", "output": implementation})
        print(f"Developer: {implementation[:500]}...")

        # Fase 3: Reviewer evalúa
        print("\n=== Fase 3: Revisión ===")
        review_prompt = f"""Revisa el siguiente código:

{implementation}

Evalúa:
1. Calidad del código
2. Mejores prácticas
3. Posibles bugs
4. Sugerencias de mejora"""

        review = self.reviewer.generate_reply(
            messages=[{"role": "user", "content": review_prompt}]
        )
        results["phases"].append({"phase": "review", "output": review})
        print(f"Reviewer: {review[:500]}...")

        return results


# Ejemplo de uso
if __name__ == "__main__":
    team = CodingTeam()

    result = team.develop_solution(
        "Crear una función que implemente el algoritmo de búsqueda binaria con manejo de errores"
    )

    print("\n=== Resultado Final ===")
    for phase in result["phases"]:
        print(f"\n--- {phase['phase'].upper()} ---")
        print(phase["output"][:300])
```

## Patrones de Conversación Avanzados

```python
"""
Patrones avanzados de conversación multi-agente.
"""
import google.generativeai as genai
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import os


class ConversationPattern(Enum):
    """Patrones de conversación."""
    ROUND_ROBIN = "round_robin"
    EXPERT_CONSULTATION = "expert_consultation"
    DEBATE = "debate"
    BRAINSTORM = "brainstorm"
    CONSENSUS = "consensus"
    HIERARCHICAL = "hierarchical"


@dataclass
class ConversationConfig:
    """Configuración de conversación."""
    pattern: ConversationPattern
    max_rounds: int = 10
    require_consensus: bool = False
    voting_threshold: float = 0.6
    time_limit_seconds: Optional[int] = None


class AdvancedConversationManager:
    """Gestor de conversaciones con patrones avanzados."""

    def __init__(self, agents: List[GeminiAgent]):
        self.agents = {agent.name: agent for agent in agents}
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.moderator = genai.GenerativeModel("gemini-1.5-flash")

    async def run_debate(
        self,
        topic: str,
        pro_agent: str,
        con_agent: str,
        rounds: int = 3
    ) -> Dict[str, Any]:
        """Ejecuta un debate estructurado."""
        debate_history = []

        for round_num in range(rounds):
            # Argumento a favor
            pro_prompt = f"""Debate - Ronda {round_num + 1}
Tema: {topic}
Tu posición: A FAVOR

{"Historial previo:" + str(debate_history[-2:]) if debate_history else "Primera ronda."}

Presenta tu argumento a favor:"""

            pro_response = self.agents[pro_agent].generate_reply(
                messages=[{"role": "user", "content": pro_prompt}]
            )
            debate_history.append({
                "round": round_num + 1,
                "position": "pro",
                "agent": pro_agent,
                "argument": pro_response
            })

            # Argumento en contra
            con_prompt = f"""Debate - Ronda {round_num + 1}
Tema: {topic}
Tu posición: EN CONTRA

Argumento del oponente: {pro_response}

Presenta tu contraargumento:"""

            con_response = self.agents[con_agent].generate_reply(
                messages=[{"role": "user", "content": con_prompt}]
            )
            debate_history.append({
                "round": round_num + 1,
                "position": "con",
                "agent": con_agent,
                "argument": con_response
            })

        # Evaluar debate
        evaluation = await self._evaluate_debate(topic, debate_history)

        return {
            "topic": topic,
            "rounds": rounds,
            "history": debate_history,
            "evaluation": evaluation
        }

    async def _evaluate_debate(
        self,
        topic: str,
        history: List[Dict]
    ) -> Dict[str, Any]:
        """Evalúa el resultado del debate."""
        history_text = "\n".join([
            f"[{h['position'].upper()}] {h['agent']}: {h['argument'][:200]}..."
            for h in history
        ])

        prompt = f"""Evalúa este debate sobre: {topic}

{history_text}

Proporciona evaluación en JSON:
{{
    "winner": "pro|con|tie",
    "pro_score": 0-10,
    "con_score": 0-10,
    "key_arguments": {{"pro": ["arg1"], "con": ["arg1"]}},
    "reasoning": "explicación"
}}"""

        response = self.moderator.generate_content(prompt)

        try:
            import json
            return json.loads(response.text)
        except:
            return {"winner": "tie", "reasoning": "No se pudo evaluar"}

    async def run_brainstorm(
        self,
        challenge: str,
        rounds: int = 3
    ) -> Dict[str, Any]:
        """Ejecuta sesión de brainstorming."""
        ideas = []
        agent_names = list(self.agents.keys())

        for round_num in range(rounds):
            round_ideas = []

            for agent_name in agent_names:
                previous_ideas = "\n".join([
                    f"- {idea['content'][:100]}"
                    for idea in ideas
                ])

                prompt = f"""Brainstorming - Ronda {round_num + 1}
Desafío: {challenge}

Ideas previas:
{previous_ideas if previous_ideas else "Ninguna aún."}

Genera UNA idea innovadora y diferente a las anteriores:"""

                response = self.agents[agent_name].generate_reply(
                    messages=[{"role": "user", "content": prompt}]
                )

                round_ideas.append({
                    "round": round_num + 1,
                    "agent": agent_name,
                    "content": response
                })

            ideas.extend(round_ideas)

        # Consolidar y rankear ideas
        ranked_ideas = await self._rank_ideas(challenge, ideas)

        return {
            "challenge": challenge,
            "total_ideas": len(ideas),
            "ideas": ideas,
            "ranked": ranked_ideas
        }

    async def _rank_ideas(
        self,
        challenge: str,
        ideas: List[Dict]
    ) -> List[Dict]:
        """Rankea ideas por relevancia e innovación."""
        ideas_text = "\n".join([
            f"{i+1}. [{idea['agent']}]: {idea['content'][:150]}"
            for i, idea in enumerate(ideas)
        ])

        prompt = f"""Rankea estas ideas para el desafío: {challenge}

{ideas_text}

Criterios: innovación, viabilidad, impacto

Responde con JSON:
{{
    "ranking": [
        {{"idea_num": 1, "score": 0-10, "reason": "..."}},
        ...
    ]
}}"""

        response = self.moderator.generate_content(prompt)

        try:
            import json
            result = json.loads(response.text)
            return sorted(
                result.get("ranking", []),
                key=lambda x: x.get("score", 0),
                reverse=True
            )
        except:
            return []

    async def run_consensus_building(
        self,
        topic: str,
        options: List[str],
        max_rounds: int = 5
    ) -> Dict[str, Any]:
        """Construye consenso sobre opciones."""
        votes_history = []
        agent_names = list(self.agents.keys())

        for round_num in range(max_rounds):
            round_votes = {}

            # Cada agente vota
            for agent_name in agent_names:
                vote_history_text = ""
                if votes_history:
                    vote_history_text = f"Votos anteriores: {votes_history[-1]}"

                prompt = f"""Consenso - Ronda {round_num + 1}
Tema: {topic}
Opciones: {options}
{vote_history_text}

Vota por UNA opción y justifica brevemente.
Responde: "VOTO: [opción] - [justificación]" """

                response = self.agents[agent_name].generate_reply(
                    messages=[{"role": "user", "content": prompt}]
                )

                # Extraer voto
                vote = self._extract_vote(response, options)
                round_votes[agent_name] = {
                    "vote": vote,
                    "justification": response
                }

            votes_history.append(round_votes)

            # Verificar consenso
            vote_counts = {}
            for v in round_votes.values():
                vote_val = v["vote"]
                vote_counts[vote_val] = vote_counts.get(vote_val, 0) + 1

            max_votes = max(vote_counts.values())
            if max_votes >= len(agent_names) * 0.7:  # 70% consenso
                winner = [k for k, v in vote_counts.items() if v == max_votes][0]
                return {
                    "consensus_reached": True,
                    "winner": winner,
                    "rounds": round_num + 1,
                    "final_votes": round_votes,
                    "history": votes_history
                }

        return {
            "consensus_reached": False,
            "rounds": max_rounds,
            "final_votes": votes_history[-1],
            "history": votes_history
        }

    def _extract_vote(self, response: str, options: List[str]) -> str:
        """Extrae el voto de la respuesta."""
        response_lower = response.lower()
        for option in options:
            if option.lower() in response_lower:
                return option
        return options[0]  # Default


# Ejemplo de uso
async def demo_advanced_patterns():
    # Crear agentes
    agents = [
        SpecializedGeminiAgent(
            name="Innovador",
            role="Innovation Lead",
            expertise=["innovación", "tecnología emergente"]
        ),
        SpecializedGeminiAgent(
            name="Pragmatico",
            role="Operations Manager",
            expertise=["operaciones", "eficiencia", "costos"]
        ),
        SpecializedGeminiAgent(
            name="Estratega",
            role="Strategy Director",
            expertise=["estrategia", "visión a largo plazo"]
        )
    ]

    manager = AdvancedConversationManager(agents)

    # Brainstorming
    print("=== BRAINSTORMING ===")
    brainstorm_result = await manager.run_brainstorm(
        challenge="¿Cómo podemos mejorar la experiencia del cliente usando IA?",
        rounds=2
    )
    print(f"Ideas generadas: {brainstorm_result['total_ideas']}")

    # Consenso
    print("\n=== CONSENSO ===")
    consensus_result = await manager.run_consensus_building(
        topic="Prioridad de implementación de IA",
        options=["Chatbot", "Recomendaciones", "Automatización"],
        max_rounds=3
    )
    print(f"Consenso alcanzado: {consensus_result['consensus_reached']}")


if __name__ == "__main__":
    asyncio.run(demo_advanced_patterns())
```

## Ejercicios Prácticos

### Ejercicio 1: Sistema de Soporte Multi-Nivel
```python
"""
Ejercicio: Crear sistema de soporte con escalamiento.
"""

class SupportSystem:
    """
    TODO: Implementar sistema con:
    1. Agente de clasificación inicial
    2. Agente de nivel 1 (FAQ)
    3. Agente de nivel 2 (técnico)
    4. Agente de nivel 3 (especialista)

    Debe:
    - Clasificar tickets automáticamente
    - Escalar cuando sea necesario
    - Mantener contexto entre niveles
    - Generar resumen al cerrar ticket
    """
    pass
```

### Ejercicio 2: Panel de Expertos
```python
"""
Ejercicio: Crear panel de expertos para consultas.
"""

class ExpertPanel:
    """
    TODO: Implementar panel donde:
    1. Usuario hace pregunta
    2. Moderador asigna a expertos relevantes
    3. Expertos responden desde su perspectiva
    4. Moderador sintetiza respuestas
    5. Panel puede pedir clarificaciones
    """
    pass
```

## Resumen

| Componente | Descripción | Uso Principal |
|------------|-------------|---------------|
| **ConversableAgent** | Agente base conversacional | Crear agentes custom |
| **AssistantAgent** | Agente con LLM | Respuestas inteligentes |
| **UserProxyAgent** | Proxy de usuario | Ejecutar código, input humano |
| **GroupChat** | Chat grupal | Múltiples agentes |
| **GroupChatManager** | Gestor de grupo | Coordinar conversación |
| **Patrones** | Debate, Brainstorm, Consenso | Colaboración estructurada |

---

**Siguiente Módulo:** [Módulo 8: Agentes Autónomos](../modulo_8/README.md)
