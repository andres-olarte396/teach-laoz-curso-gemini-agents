# 7.3.1 LangGraph para Grafos de Agentes

## Objetivo de Aprendizaje

Implementar sistemas multi-agente usando LangGraph para crear flujos de trabajo basados en grafos con estados, transiciones condicionales y ciclos, integrando Google Gemini como motor de razonamiento.

## Introducción

LangGraph es un framework que extiende LangChain para crear agentes como grafos de estados. Permite definir flujos complejos con ciclos, bifurcaciones y estados persistentes, ideal para workflows multi-agente sofisticados.

```
┌─────────────────────────────────────────────────────────────────┐
│                    LANGGRAPH ARCHITECTURE                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────┐      ┌─────────┐      ┌─────────┐                │
│   │  START  │─────>│ Node A  │─────>│ Node B  │                │
│   └─────────┘      └────┬────┘      └────┬────┘                │
│                         │                │                      │
│                         v                v                      │
│                    ┌─────────┐      ┌─────────┐                │
│                    │  Edge   │      │Condition│                │
│                    │(always) │      │  Edge   │                │
│                    └────┬────┘      └────┬────┘                │
│                         │           ┌────┴────┐                │
│                         v           v         v                 │
│                    ┌─────────┐ ┌─────────┐ ┌─────────┐         │
│                    │ Node C  │ │ Node D  │ │ Node E  │         │
│                    └────┬────┘ └────┬────┘ └────┬────┘         │
│                         │           │           │               │
│                         └───────────┴───────────┘               │
│                                     │                           │
│                                     v                           │
│                               ┌─────────┐                       │
│                               │   END   │                       │
│                               └─────────┘                       │
│                                                                 │
│   State: Diccionario que fluye entre nodos                     │
│   Nodes: Funciones que transforman el estado                   │
│   Edges: Conexiones (condicionales o directas)                 │
└─────────────────────────────────────────────────────────────────┘
```

## Configuración e Instalación

```python
"""
Configuración de LangGraph con Google Gemini.
"""
# pip install langgraph langchain-google-genai

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from typing import TypedDict, Annotated, Sequence, Literal
import operator
import os


# Configurar Gemini
os.environ["GOOGLE_API_KEY"] = "tu-api-key"

# Inicializar modelo
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.7
)


# Definir estado del grafo
class AgentState(TypedDict):
    """Estado compartido entre nodos del grafo."""
    messages: Annotated[Sequence[dict], operator.add]
    current_agent: str
    task: str
    results: dict
    iteration: int
    should_continue: bool
```

## Grafo Básico de Agentes

```python
"""
Implementación de grafo básico multi-agente.
"""
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import TypedDict, Annotated, Sequence, Literal, Dict, Any, List
import operator
import json


class SimpleAgentState(TypedDict):
    """Estado para grafo simple."""
    messages: Annotated[List[dict], operator.add]
    task: str
    analysis: str
    draft: str
    final_output: str
    current_step: str


def create_analyst_node(llm: ChatGoogleGenerativeAI):
    """Crea nodo analista."""
    def analyst(state: SimpleAgentState) -> Dict[str, Any]:
        task = state["task"]

        prompt = f"""Eres un analista experto. Analiza la siguiente tarea y extrae los puntos clave.

Tarea: {task}

Proporciona un análisis estructurado con:
1. Objetivo principal
2. Requisitos identificados
3. Posibles desafíos
4. Enfoque recomendado

Análisis:"""

        response = llm.invoke(prompt)

        return {
            "analysis": response.content,
            "messages": [{"role": "analyst", "content": response.content}],
            "current_step": "analysis_complete"
        }

    return analyst


def create_writer_node(llm: ChatGoogleGenerativeAI):
    """Crea nodo escritor."""
    def writer(state: SimpleAgentState) -> Dict[str, Any]:
        task = state["task"]
        analysis = state["analysis"]

        prompt = f"""Eres un escritor experto. Basándote en el análisis, crea un borrador.

Tarea original: {task}

Análisis previo:
{analysis}

Crea un borrador completo y bien estructurado:"""

        response = llm.invoke(prompt)

        return {
            "draft": response.content,
            "messages": [{"role": "writer", "content": response.content}],
            "current_step": "draft_complete"
        }

    return writer


def create_reviewer_node(llm: ChatGoogleGenerativeAI):
    """Crea nodo revisor."""
    def reviewer(state: SimpleAgentState) -> Dict[str, Any]:
        draft = state["draft"]

        prompt = f"""Eres un revisor experto. Revisa y mejora el siguiente borrador.

Borrador:
{draft}

Proporciona:
1. Evaluación de calidad (1-10)
2. Puntos fuertes
3. Áreas de mejora
4. Versión final mejorada

Revisión:"""

        response = llm.invoke(prompt)

        return {
            "final_output": response.content,
            "messages": [{"role": "reviewer", "content": response.content}],
            "current_step": "review_complete"
        }

    return reviewer


def build_simple_agent_graph():
    """Construye grafo simple de 3 agentes."""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

    # Crear grafo
    workflow = StateGraph(SimpleAgentState)

    # Agregar nodos
    workflow.add_node("analyst", create_analyst_node(llm))
    workflow.add_node("writer", create_writer_node(llm))
    workflow.add_node("reviewer", create_reviewer_node(llm))

    # Definir flujo
    workflow.set_entry_point("analyst")
    workflow.add_edge("analyst", "writer")
    workflow.add_edge("writer", "reviewer")
    workflow.add_edge("reviewer", END)

    # Compilar
    app = workflow.compile()

    return app


# Uso
if __name__ == "__main__":
    app = build_simple_agent_graph()

    # Ejecutar
    result = app.invoke({
        "messages": [],
        "task": "Crear un plan de marketing para una startup de tecnología educativa",
        "analysis": "",
        "draft": "",
        "final_output": "",
        "current_step": "start"
    })

    print("=== Resultado Final ===")
    print(result["final_output"])
```

## Grafo con Bifurcaciones Condicionales

```python
"""
Grafo con decisiones condicionales entre agentes.
"""
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import TypedDict, Annotated, List, Dict, Any, Literal
import operator
import json


class ConditionalAgentState(TypedDict):
    """Estado con soporte para bifurcaciones."""
    messages: Annotated[List[dict], operator.add]
    task: str
    task_type: str  # "technical", "creative", "research"
    complexity: str  # "simple", "complex"
    output: str
    requires_review: bool
    iteration: int


def classifier_node(llm: ChatGoogleGenerativeAI):
    """Nodo que clasifica el tipo de tarea."""
    def classify(state: ConditionalAgentState) -> Dict[str, Any]:
        task = state["task"]

        prompt = f"""Clasifica la siguiente tarea.

Tarea: {task}

Responde SOLO con JSON:
{{
    "task_type": "technical|creative|research",
    "complexity": "simple|complex",
    "requires_review": true|false,
    "reasoning": "breve explicación"
}}"""

        response = llm.invoke(prompt)

        try:
            # Limpiar respuesta para obtener JSON
            text = response.content
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]

            classification = json.loads(text.strip())

            return {
                "task_type": classification.get("task_type", "technical"),
                "complexity": classification.get("complexity", "simple"),
                "requires_review": classification.get("requires_review", True),
                "messages": [{"role": "classifier", "content": str(classification)}]
            }
        except:
            return {
                "task_type": "technical",
                "complexity": "simple",
                "requires_review": True,
                "messages": [{"role": "classifier", "content": "Default classification"}]
            }

    return classify


def technical_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente para tareas técnicas."""
    def process(state: ConditionalAgentState) -> Dict[str, Any]:
        task = state["task"]
        complexity = state["complexity"]

        prompt = f"""Eres un experto técnico. Resuelve esta tarea técnica.

Tarea: {task}
Complejidad: {complexity}

Proporciona una solución técnica detallada con código si es necesario:"""

        response = llm.invoke(prompt)

        return {
            "output": response.content,
            "messages": [{"role": "technical_agent", "content": response.content}]
        }

    return process


def creative_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente para tareas creativas."""
    def process(state: ConditionalAgentState) -> Dict[str, Any]:
        task = state["task"]

        prompt = f"""Eres un experto creativo. Aborda esta tarea con creatividad e innovación.

Tarea: {task}

Proporciona una respuesta creativa y original:"""

        response = llm.invoke(prompt)

        return {
            "output": response.content,
            "messages": [{"role": "creative_agent", "content": response.content}]
        }

    return process


def research_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente para investigación."""
    def process(state: ConditionalAgentState) -> Dict[str, Any]:
        task = state["task"]

        prompt = f"""Eres un investigador experto. Investiga este tema a profundidad.

Tarea: {task}

Proporciona un análisis investigativo con:
1. Contexto y antecedentes
2. Hallazgos principales
3. Fuentes relevantes
4. Conclusiones"""

        response = llm.invoke(prompt)

        return {
            "output": response.content,
            "messages": [{"role": "research_agent", "content": response.content}]
        }

    return process


def review_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente revisor opcional."""
    def review(state: ConditionalAgentState) -> Dict[str, Any]:
        output = state["output"]

        prompt = f"""Revisa y mejora el siguiente contenido:

{output}

Proporciona versión mejorada:"""

        response = llm.invoke(prompt)

        return {
            "output": response.content,
            "messages": [{"role": "reviewer", "content": "Reviewed and improved"}]
        }

    return review


def route_by_task_type(state: ConditionalAgentState) -> Literal["technical", "creative", "research"]:
    """Función de enrutamiento por tipo de tarea."""
    task_type = state.get("task_type", "technical")

    if task_type == "creative":
        return "creative"
    elif task_type == "research":
        return "research"
    else:
        return "technical"


def should_review(state: ConditionalAgentState) -> Literal["review", "end"]:
    """Decide si se necesita revisión."""
    if state.get("requires_review", False):
        return "review"
    return "end"


def build_conditional_graph():
    """Construye grafo con bifurcaciones condicionales."""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

    workflow = StateGraph(ConditionalAgentState)

    # Agregar nodos
    workflow.add_node("classifier", classifier_node(llm))
    workflow.add_node("technical", technical_agent_node(llm))
    workflow.add_node("creative", creative_agent_node(llm))
    workflow.add_node("research", research_agent_node(llm))
    workflow.add_node("review", review_agent_node(llm))

    # Punto de entrada
    workflow.set_entry_point("classifier")

    # Bifurcación condicional después de clasificación
    workflow.add_conditional_edges(
        "classifier",
        route_by_task_type,
        {
            "technical": "technical",
            "creative": "creative",
            "research": "research"
        }
    )

    # Después de cada agente especializado, decidir si revisar
    for agent in ["technical", "creative", "research"]:
        workflow.add_conditional_edges(
            agent,
            should_review,
            {
                "review": "review",
                "end": END
            }
        )

    # Después de revisión, terminar
    workflow.add_edge("review", END)

    return workflow.compile()


# Uso
if __name__ == "__main__":
    app = build_conditional_graph()

    # Tarea técnica
    result = app.invoke({
        "messages": [],
        "task": "Implementar una función de ordenamiento quicksort en Python",
        "task_type": "",
        "complexity": "",
        "output": "",
        "requires_review": False,
        "iteration": 0
    })

    print("=== Resultado Tarea Técnica ===")
    print(result["output"][:500])
```

## Grafo con Ciclos (Iterativo)

```python
"""
Grafo con ciclos para refinamiento iterativo.
"""
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import TypedDict, Annotated, List, Dict, Any, Literal
import operator
import json


class IterativeAgentState(TypedDict):
    """Estado para refinamiento iterativo."""
    messages: Annotated[List[dict], operator.add]
    task: str
    current_draft: str
    feedback: str
    quality_score: float
    iteration: int
    max_iterations: int


def drafter_node(llm: ChatGoogleGenerativeAI):
    """Nodo que genera/refina borrador."""
    def draft(state: IterativeAgentState) -> Dict[str, Any]:
        task = state["task"]
        current_draft = state.get("current_draft", "")
        feedback = state.get("feedback", "")
        iteration = state.get("iteration", 0)

        if iteration == 0:
            prompt = f"""Crea un borrador inicial para:

Tarea: {task}

Borrador:"""
        else:
            prompt = f"""Mejora el siguiente borrador basándote en el feedback.

Borrador anterior:
{current_draft}

Feedback recibido:
{feedback}

Borrador mejorado:"""

        response = llm.invoke(prompt)

        return {
            "current_draft": response.content,
            "iteration": iteration + 1,
            "messages": [{"role": "drafter", "content": f"Iteration {iteration + 1}"}]
        }

    return draft


def evaluator_node(llm: ChatGoogleGenerativeAI):
    """Nodo que evalúa calidad del borrador."""
    def evaluate(state: IterativeAgentState) -> Dict[str, Any]:
        draft = state["current_draft"]
        task = state["task"]

        prompt = f"""Evalúa la calidad de este borrador en relación a la tarea.

Tarea: {task}

Borrador:
{draft}

Responde SOLO con JSON:
{{
    "quality_score": 0.0-1.0,
    "strengths": ["punto1", "punto2"],
    "improvements_needed": ["mejora1", "mejora2"],
    "specific_feedback": "feedback detallado para mejora"
}}"""

        response = llm.invoke(prompt)

        try:
            text = response.content
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]

            evaluation = json.loads(text.strip())

            return {
                "quality_score": float(evaluation.get("quality_score", 0.5)),
                "feedback": evaluation.get("specific_feedback", ""),
                "messages": [{"role": "evaluator", "content": str(evaluation)}]
            }
        except:
            return {
                "quality_score": 0.5,
                "feedback": "Por favor mejora la estructura y claridad.",
                "messages": [{"role": "evaluator", "content": "Default evaluation"}]
            }

    return evaluate


def should_continue_iteration(state: IterativeAgentState) -> Literal["continue", "finish"]:
    """Decide si continuar iterando."""
    quality_score = state.get("quality_score", 0)
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)

    # Terminar si calidad suficiente o máximo de iteraciones
    if quality_score >= 0.85:
        return "finish"
    if iteration >= max_iterations:
        return "finish"

    return "continue"


def finalizer_node(llm: ChatGoogleGenerativeAI):
    """Nodo que finaliza el output."""
    def finalize(state: IterativeAgentState) -> Dict[str, Any]:
        draft = state["current_draft"]
        iterations = state["iteration"]
        quality = state["quality_score"]

        prompt = f"""Finaliza y pule el siguiente contenido.

Borrador final (después de {iterations} iteraciones, calidad: {quality:.2f}):
{draft}

Versión final pulida:"""

        response = llm.invoke(prompt)

        return {
            "current_draft": response.content,
            "messages": [{"role": "finalizer", "content": "Finalized"}]
        }

    return finalize


def build_iterative_graph():
    """Construye grafo con ciclo iterativo."""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

    workflow = StateGraph(IterativeAgentState)

    # Agregar nodos
    workflow.add_node("drafter", drafter_node(llm))
    workflow.add_node("evaluator", evaluator_node(llm))
    workflow.add_node("finalizer", finalizer_node(llm))

    # Punto de entrada
    workflow.set_entry_point("drafter")

    # Flujo: drafter -> evaluator -> (continue -> drafter | finish -> finalizer)
    workflow.add_edge("drafter", "evaluator")

    workflow.add_conditional_edges(
        "evaluator",
        should_continue_iteration,
        {
            "continue": "drafter",  # Ciclo de vuelta
            "finish": "finalizer"
        }
    )

    workflow.add_edge("finalizer", END)

    return workflow.compile()


# Uso
if __name__ == "__main__":
    app = build_iterative_graph()

    result = app.invoke({
        "messages": [],
        "task": "Escribe un artículo breve sobre los beneficios de la IA en educación",
        "current_draft": "",
        "feedback": "",
        "quality_score": 0.0,
        "iteration": 0,
        "max_iterations": 3
    })

    print(f"=== Resultado después de {result['iteration']} iteraciones ===")
    print(f"Calidad final: {result['quality_score']:.2f}")
    print(f"\n{result['current_draft'][:800]}")
```

## Grafo Multi-Agente con Supervisor

```python
"""
Grafo con patrón supervisor que coordina múltiples agentes.
"""
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Optional
import operator
import json


class SupervisedAgentState(TypedDict):
    """Estado para grafo supervisado."""
    messages: Annotated[List[dict], operator.add]
    task: str
    subtasks: List[Dict[str, Any]]
    current_subtask_idx: int
    agent_outputs: Dict[str, str]
    final_result: str
    next_agent: str


def supervisor_node(llm: ChatGoogleGenerativeAI):
    """Supervisor que descompone tareas y coordina agentes."""
    def supervise(state: SupervisedAgentState) -> Dict[str, Any]:
        task = state["task"]
        subtasks = state.get("subtasks", [])
        current_idx = state.get("current_subtask_idx", -1)
        agent_outputs = state.get("agent_outputs", {})

        # Primera invocación: descomponer tarea
        if current_idx == -1:
            prompt = f"""Eres un supervisor de agentes. Descompón esta tarea en subtareas.

Tarea: {task}

Responde SOLO con JSON:
{{
    "subtasks": [
        {{"id": 1, "description": "subtarea 1", "agent": "researcher|writer|coder"}},
        {{"id": 2, "description": "subtarea 2", "agent": "researcher|writer|coder"}},
        {{"id": 3, "description": "subtarea 3", "agent": "researcher|writer|coder"}}
    ]
}}"""

            response = llm.invoke(prompt)

            try:
                text = response.content
                if "```json" in text:
                    text = text.split("```json")[1].split("```")[0]
                elif "```" in text:
                    text = text.split("```")[1].split("```")[0]

                result = json.loads(text.strip())
                new_subtasks = result.get("subtasks", [])

                if new_subtasks:
                    return {
                        "subtasks": new_subtasks,
                        "current_subtask_idx": 0,
                        "next_agent": new_subtasks[0].get("agent", "researcher"),
                        "messages": [{"role": "supervisor", "content": f"Created {len(new_subtasks)} subtasks"}]
                    }
            except:
                pass

            # Fallback: una sola subtarea
            return {
                "subtasks": [{"id": 1, "description": task, "agent": "writer"}],
                "current_subtask_idx": 0,
                "next_agent": "writer",
                "messages": [{"role": "supervisor", "content": "Single subtask created"}]
            }

        # Siguientes invocaciones: avanzar al siguiente subtask
        else:
            next_idx = current_idx + 1

            if next_idx >= len(subtasks):
                return {
                    "current_subtask_idx": next_idx,
                    "next_agent": "synthesizer",
                    "messages": [{"role": "supervisor", "content": "All subtasks complete, synthesizing"}]
                }
            else:
                next_agent = subtasks[next_idx].get("agent", "writer")
                return {
                    "current_subtask_idx": next_idx,
                    "next_agent": next_agent,
                    "messages": [{"role": "supervisor", "content": f"Moving to subtask {next_idx + 1}"}]
                }

    return supervise


def researcher_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente investigador."""
    def research(state: SupervisedAgentState) -> Dict[str, Any]:
        subtasks = state["subtasks"]
        idx = state["current_subtask_idx"]
        subtask = subtasks[idx]["description"]

        prompt = f"""Investiga sobre: {subtask}

Proporciona información relevante y bien organizada:"""

        response = llm.invoke(prompt)
        agent_outputs = state.get("agent_outputs", {})
        agent_outputs[f"subtask_{idx}"] = response.content

        return {
            "agent_outputs": agent_outputs,
            "messages": [{"role": "researcher", "content": f"Completed subtask {idx}"}]
        }

    return research


def writer_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente escritor."""
    def write(state: SupervisedAgentState) -> Dict[str, Any]:
        subtasks = state["subtasks"]
        idx = state["current_subtask_idx"]
        subtask = subtasks[idx]["description"]
        prev_outputs = state.get("agent_outputs", {})

        context = "\n".join([f"- {v[:200]}" for v in prev_outputs.values()])

        prompt = f"""Escribe sobre: {subtask}

Contexto previo:
{context if context else "Ninguno"}

Contenido:"""

        response = llm.invoke(prompt)
        agent_outputs = state.get("agent_outputs", {})
        agent_outputs[f"subtask_{idx}"] = response.content

        return {
            "agent_outputs": agent_outputs,
            "messages": [{"role": "writer", "content": f"Completed subtask {idx}"}]
        }

    return write


def coder_agent_node(llm: ChatGoogleGenerativeAI):
    """Agente programador."""
    def code(state: SupervisedAgentState) -> Dict[str, Any]:
        subtasks = state["subtasks"]
        idx = state["current_subtask_idx"]
        subtask = subtasks[idx]["description"]

        prompt = f"""Implementa código para: {subtask}

Proporciona código Python funcional:"""

        response = llm.invoke(prompt)
        agent_outputs = state.get("agent_outputs", {})
        agent_outputs[f"subtask_{idx}"] = response.content

        return {
            "agent_outputs": agent_outputs,
            "messages": [{"role": "coder", "content": f"Completed subtask {idx}"}]
        }

    return code


def synthesizer_node(llm: ChatGoogleGenerativeAI):
    """Nodo que sintetiza todos los outputs."""
    def synthesize(state: SupervisedAgentState) -> Dict[str, Any]:
        task = state["task"]
        agent_outputs = state.get("agent_outputs", {})

        all_outputs = "\n\n---\n\n".join([
            f"**Subtarea {k}:**\n{v}"
            for k, v in agent_outputs.items()
        ])

        prompt = f"""Sintetiza los siguientes resultados en un documento coherente.

Tarea original: {task}

Resultados de subtareas:
{all_outputs}

Documento final integrado:"""

        response = llm.invoke(prompt)

        return {
            "final_result": response.content,
            "messages": [{"role": "synthesizer", "content": "Final synthesis complete"}]
        }

    return synthesize


def route_to_agent(state: SupervisedAgentState) -> Literal["researcher", "writer", "coder", "synthesizer", "supervisor"]:
    """Enruta al siguiente agente según decisión del supervisor."""
    next_agent = state.get("next_agent", "supervisor")

    if next_agent == "synthesizer":
        return "synthesizer"
    elif next_agent == "researcher":
        return "researcher"
    elif next_agent == "coder":
        return "coder"
    elif next_agent == "writer":
        return "writer"
    else:
        return "supervisor"


def build_supervised_graph():
    """Construye grafo con supervisor."""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

    workflow = StateGraph(SupervisedAgentState)

    # Agregar nodos
    workflow.add_node("supervisor", supervisor_node(llm))
    workflow.add_node("researcher", researcher_agent_node(llm))
    workflow.add_node("writer", writer_agent_node(llm))
    workflow.add_node("coder", coder_agent_node(llm))
    workflow.add_node("synthesizer", synthesizer_node(llm))

    # Punto de entrada
    workflow.set_entry_point("supervisor")

    # Después del supervisor, enrutar al agente correspondiente
    workflow.add_conditional_edges(
        "supervisor",
        route_to_agent,
        {
            "researcher": "researcher",
            "writer": "writer",
            "coder": "coder",
            "synthesizer": "synthesizer"
        }
    )

    # Después de cada agente, volver al supervisor
    for agent in ["researcher", "writer", "coder"]:
        workflow.add_edge(agent, "supervisor")

    # Después del sintetizador, terminar
    workflow.add_edge("synthesizer", END)

    return workflow.compile()


# Uso
if __name__ == "__main__":
    app = build_supervised_graph()

    result = app.invoke({
        "messages": [],
        "task": "Crear una guía para implementar autenticación JWT en una API REST con Python",
        "subtasks": [],
        "current_subtask_idx": -1,
        "agent_outputs": {},
        "final_result": "",
        "next_agent": ""
    })

    print("=== Resultado Final ===")
    print(result["final_result"][:1000])
```

## Ejercicios Prácticos

### Ejercicio 1: Grafo con Memoria Persistente
```python
"""
Ejercicio: Agregar memoria persistente al grafo.
"""
from langgraph.checkpoint.sqlite import SqliteSaver

def build_graph_with_memory():
    """
    TODO: Crear grafo que:
    1. Use SqliteSaver para persistir estado
    2. Permita reanudar conversaciones
    3. Mantenga historial entre sesiones
    """
    # Hint: memory = SqliteSaver.from_conn_string("checkpoints.db")
    # app = workflow.compile(checkpointer=memory)
    pass
```

### Ejercicio 2: Grafo con Herramientas
```python
"""
Ejercicio: Integrar herramientas en nodos del grafo.
"""
from langchain_core.tools import tool

@tool
def search_web(query: str) -> str:
    """Busca información en la web."""
    # TODO: Implementar búsqueda real
    return f"Resultados para: {query}"

def build_graph_with_tools():
    """
    TODO: Crear grafo donde los agentes pueden:
    1. Decidir si usar herramientas
    2. Ejecutar herramientas
    3. Procesar resultados
    """
    pass
```

## Resumen

| Componente | Descripción | Uso Principal |
|------------|-------------|---------------|
| **StateGraph** | Contenedor del grafo | Definir estructura del workflow |
| **Nodes** | Funciones que procesan estado | Lógica de cada agente |
| **Edges** | Conexiones entre nodos | Flujo secuencial |
| **Conditional Edges** | Bifurcaciones dinámicas | Routing por condición |
| **Cycles** | Conexiones de vuelta | Refinamiento iterativo |
| **Checkpointer** | Persistencia de estado | Memoria entre sesiones |

---

**Siguiente:** [7.3.2 CrewAI para Equipos de Agentes](./7.3.2-crewai.md)
