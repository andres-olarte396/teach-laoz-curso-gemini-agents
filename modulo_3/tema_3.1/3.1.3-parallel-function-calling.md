# Parallel Function Calling

**Tiempo estimado**: 45 minutos
**Nivel**: Intermedio
**Prerrequisitos**: Ciclo de Vida de Function Calling (3.1.2)

## ¿Por qué importa este concepto?

Parallel Function Calling permite que el modelo invoque múltiples funciones en una sola respuesta. Esta capacidad es crucial para:

- **Reducir latencia**: Ejecutar llamadas independientes simultáneamente
- **Optimizar tokens**: Una sola respuesta del modelo vs múltiples turnos
- **Mejorar UX**: Respuestas más rápidas y completas
- **Eficiencia de costos**: Menos llamadas a la API = menor costo

Gemini soporta parallel function calling de forma nativa, permitiendo que el modelo decida inteligentemente cuándo paralelizar.

---

## Anatomía del Parallel Function Calling

```
┌─────────────────────────────────────────────────────────────────┐
│ FUNCTION CALLING SECUENCIAL                                     │
│ ───────────────────────────                                     │
│                                                                 │
│ Usuario ──► Modelo ──► Tool A ──► Modelo ──► Tool B ──► Resp.  │
│                                                                 │
│ Tiempo total: T_modelo + T_A + T_modelo + T_B + T_modelo       │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│ PARALLEL FUNCTION CALLING                                       │
│ ─────────────────────────                                       │
│                                                                 │
│                        ┌──► Tool A ──┐                          │
│ Usuario ──► Modelo ────┤             ├──► Modelo ──► Respuesta │
│                        └──► Tool B ──┘                          │
│                                                                 │
│ Tiempo total: T_modelo + max(T_A, T_B) + T_modelo              │
│                                                                 │
│ Ahorro: T_modelo + min(T_A, T_B)                               │
└─────────────────────────────────────────────────────────────────┘
```

---

## Implementación práctica

### Configuración básica con múltiples tools

```python
import google.generativeai as genai
from typing import List, Dict, Any, Callable
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json
import time


# Configurar API
genai.configure(api_key="TU_API_KEY")


# Definir funciones/tools
def get_weather(location: str, unit: str = "celsius") -> Dict:
    """Obtiene el clima actual de una ubicación."""
    # Simulación de API call
    time.sleep(0.5)  # Simular latencia
    weathers = {
        "madrid": {"temp": 22, "condition": "soleado"},
        "london": {"temp": 15, "condition": "nublado"},
        "tokyo": {"temp": 28, "condition": "húmedo"},
        "new york": {"temp": 18, "condition": "parcialmente nublado"}
    }
    weather = weathers.get(location.lower(), {"temp": 20, "condition": "desconocido"})

    if unit == "fahrenheit":
        weather["temp"] = weather["temp"] * 9/5 + 32

    return {
        "location": location,
        "temperature": weather["temp"],
        "unit": unit,
        "condition": weather["condition"]
    }


def get_time(timezone: str) -> Dict:
    """Obtiene la hora actual en una zona horaria."""
    time.sleep(0.3)  # Simular latencia
    from datetime import datetime, timedelta

    offsets = {
        "UTC": 0, "EST": -5, "PST": -8, "CET": 1, "JST": 9
    }
    offset = offsets.get(timezone.upper(), 0)
    current = datetime.utcnow() + timedelta(hours=offset)

    return {
        "timezone": timezone,
        "time": current.strftime("%H:%M:%S"),
        "date": current.strftime("%Y-%m-%d")
    }


def get_stock_price(symbol: str) -> Dict:
    """Obtiene el precio de una acción."""
    time.sleep(0.4)  # Simular latencia
    prices = {
        "GOOGL": 142.50, "AAPL": 178.20, "MSFT": 378.90,
        "AMZN": 178.50, "META": 505.75
    }
    return {
        "symbol": symbol.upper(),
        "price": prices.get(symbol.upper(), 100.00),
        "currency": "USD"
    }


# Declaraciones de tools para Gemini
weather_tool = {
    "name": "get_weather",
    "description": "Obtiene el clima actual de una ubicación específica",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "Ciudad o ubicación"
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "Unidad de temperatura"
            }
        },
        "required": ["location"]
    }
}

time_tool = {
    "name": "get_time",
    "description": "Obtiene la hora actual en una zona horaria",
    "parameters": {
        "type": "object",
        "properties": {
            "timezone": {
                "type": "string",
                "description": "Zona horaria (UTC, EST, PST, CET, JST)"
            }
        },
        "required": ["timezone"]
    }
}

stock_tool = {
    "name": "get_stock_price",
    "description": "Obtiene el precio actual de una acción",
    "parameters": {
        "type": "object",
        "properties": {
            "symbol": {
                "type": "string",
                "description": "Símbolo de la acción (ej: GOOGL, AAPL)"
            }
        },
        "required": ["symbol"]
    }
}


# Crear modelo con todas las tools
model = genai.GenerativeModel(
    "gemini-1.5-flash",
    tools=[weather_tool, time_tool, stock_tool]
)
```

### Ejecutor de parallel function calls

```python
class ParallelFunctionExecutor:
    """Ejecuta múltiples function calls en paralelo."""

    def __init__(self, tool_registry: Dict[str, Callable]):
        """
        Args:
            tool_registry: Mapeo de nombre de tool a función
        """
        self.tools = tool_registry
        self.executor = ThreadPoolExecutor(max_workers=10)

    def execute_single(self, function_call) -> Dict:
        """Ejecuta una sola function call."""
        func_name = function_call.name
        args = dict(function_call.args)

        if func_name not in self.tools:
            return {
                "error": f"Function {func_name} not found",
                "function_name": func_name
            }

        try:
            result = self.tools[func_name](**args)
            return {
                "function_name": func_name,
                "result": result
            }
        except Exception as e:
            return {
                "function_name": func_name,
                "error": str(e)
            }

    def execute_parallel(self, function_calls: List) -> List[Dict]:
        """
        Ejecuta múltiples function calls en paralelo.

        Args:
            function_calls: Lista de function calls del modelo

        Returns:
            Lista de resultados en el mismo orden
        """
        # Usar ThreadPoolExecutor para paralelizar
        futures = [
            self.executor.submit(self.execute_single, fc)
            for fc in function_calls
        ]

        # Recolectar resultados manteniendo orden
        results = [future.result() for future in futures]
        return results

    async def execute_parallel_async(self, function_calls: List) -> List[Dict]:
        """Versión asíncrona del ejecutor paralelo."""
        loop = asyncio.get_event_loop()

        tasks = [
            loop.run_in_executor(self.executor, self.execute_single, fc)
            for fc in function_calls
        ]

        results = await asyncio.gather(*tasks)
        return list(results)


# Crear ejecutor con tools registrados
executor = ParallelFunctionExecutor({
    "get_weather": get_weather,
    "get_time": get_time,
    "get_stock_price": get_stock_price
})
```

### Flujo completo de parallel function calling

```python
def handle_parallel_function_calls(
    user_query: str,
    model: genai.GenerativeModel,
    executor: ParallelFunctionExecutor
) -> str:
    """
    Maneja una query que puede resultar en parallel function calls.

    Args:
        user_query: Pregunta del usuario
        model: Modelo de Gemini configurado con tools
        executor: Ejecutor de funciones paralelas

    Returns:
        Respuesta final del modelo
    """
    # Paso 1: Enviar query al modelo
    response = model.generate_content(user_query)

    # Paso 2: Verificar si hay function calls
    if not response.candidates[0].content.parts:
        return response.text

    function_calls = []
    for part in response.candidates[0].content.parts:
        if hasattr(part, 'function_call'):
            function_calls.append(part.function_call)

    if not function_calls:
        return response.text

    print(f"Detectadas {len(function_calls)} function calls")
    for fc in function_calls:
        print(f"  - {fc.name}({dict(fc.args)})")

    # Paso 3: Ejecutar en paralelo
    start_time = time.time()
    results = executor.execute_parallel(function_calls)
    execution_time = time.time() - start_time

    print(f"Ejecución paralela completada en {execution_time:.2f}s")

    # Paso 4: Construir respuesta con resultados
    function_response_parts = []
    for fc, result in zip(function_calls, results):
        function_response_parts.append(
            genai.protos.Part(
                function_response=genai.protos.FunctionResponse(
                    name=fc.name,
                    response={"result": result}
                )
            )
        )

    # Paso 5: Enviar resultados al modelo para respuesta final
    final_response = model.generate_content([
        genai.protos.Content(
            parts=[genai.protos.Part(text=user_query)],
            role="user"
        ),
        response.candidates[0].content,  # La respuesta con function calls
        genai.protos.Content(
            parts=function_response_parts,
            role="function"
        )
    ])

    return final_response.text


# Uso
query = """
Dame información completa sobre:
1. El clima en Madrid y Londres
2. La hora en Tokyo (JST)
3. El precio de las acciones de Google y Apple
"""

result = handle_parallel_function_calls(query, model, executor)
print("\n" + "="*50)
print("RESPUESTA FINAL:")
print(result)
```

---

## Patrones avanzados

### 1. Agrupación inteligente de function calls

```python
class SmartFunctionGrouper:
    """
    Agrupa function calls por dependencias y prioridad
    para optimizar la ejecución.
    """

    def __init__(self, dependency_graph: Dict[str, List[str]] = None):
        """
        Args:
            dependency_graph: {func_name: [funciones de las que depende]}
        """
        self.dependencies = dependency_graph or {}

    def group_by_dependency(
        self,
        function_calls: List
    ) -> List[List]:
        """
        Agrupa function calls en niveles de dependencia.
        Funciones en el mismo nivel pueden ejecutarse en paralelo.
        """
        # Extraer nombres
        calls_by_name = {fc.name: fc for fc in function_calls}
        remaining = set(calls_by_name.keys())
        executed = set()
        groups = []

        while remaining:
            # Encontrar funciones sin dependencias pendientes
            ready = []
            for name in remaining:
                deps = self.dependencies.get(name, [])
                if all(d in executed or d not in calls_by_name for d in deps):
                    ready.append(name)

            if not ready:
                # Ciclo detectado o funciones sin dependencias declaradas
                ready = list(remaining)

            groups.append([calls_by_name[name] for name in ready])
            executed.update(ready)
            remaining -= set(ready)

        return groups

    def execute_grouped(
        self,
        function_calls: List,
        executor: ParallelFunctionExecutor
    ) -> List[Dict]:
        """Ejecuta respetando dependencias."""
        groups = self.group_by_dependency(function_calls)
        all_results = []

        for i, group in enumerate(groups):
            print(f"Ejecutando grupo {i+1}/{len(groups)}: {[fc.name for fc in group]}")
            results = executor.execute_parallel(group)
            all_results.extend(results)

        return all_results


# Ejemplo con dependencias
# get_flight_status depende de get_airport_code
grouper = SmartFunctionGrouper({
    "get_flight_status": ["get_airport_code"],
    "get_hotel_availability": ["get_weather"]  # Ejemplo: solo buscar hotel si clima OK
})
```

### 2. Timeout y fallback para parallel calls

```python
import asyncio
from typing import Optional


class ResilientParallelExecutor(ParallelFunctionExecutor):
    """
    Ejecutor con timeouts, fallbacks y manejo de errores robusto.
    """

    def __init__(
        self,
        tool_registry: Dict[str, Callable],
        default_timeout: float = 5.0,
        fallback_values: Dict[str, Any] = None
    ):
        super().__init__(tool_registry)
        self.default_timeout = default_timeout
        self.fallbacks = fallback_values or {}

    async def execute_with_timeout(
        self,
        function_call,
        timeout: Optional[float] = None
    ) -> Dict:
        """Ejecuta una función con timeout."""
        timeout = timeout or self.default_timeout

        try:
            loop = asyncio.get_event_loop()
            result = await asyncio.wait_for(
                loop.run_in_executor(
                    self.executor,
                    self.execute_single,
                    function_call
                ),
                timeout=timeout
            )
            return result

        except asyncio.TimeoutError:
            func_name = function_call.name
            fallback = self.fallbacks.get(func_name)

            if fallback is not None:
                return {
                    "function_name": func_name,
                    "result": fallback,
                    "warning": f"Timeout after {timeout}s, using fallback"
                }

            return {
                "function_name": func_name,
                "error": f"Timeout after {timeout}s",
                "timeout": True
            }

    async def execute_parallel_resilient(
        self,
        function_calls: List,
        timeout: Optional[float] = None
    ) -> List[Dict]:
        """
        Ejecuta en paralelo con manejo de errores individual.
        Una función fallida no afecta a las demás.
        """
        tasks = [
            self.execute_with_timeout(fc, timeout)
            for fc in function_calls
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Convertir excepciones a errores estructurados
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "function_name": function_calls[i].name,
                    "error": str(result)
                })
            else:
                processed_results.append(result)

        return processed_results


# Uso
resilient_executor = ResilientParallelExecutor(
    tool_registry={
        "get_weather": get_weather,
        "get_time": get_time,
        "get_stock_price": get_stock_price
    },
    default_timeout=3.0,
    fallback_values={
        "get_weather": {"temp": "N/A", "condition": "unavailable"},
        "get_stock_price": {"price": "N/A", "note": "Market data unavailable"}
    }
)

# Ejecutar
async def main():
    # Simular function calls
    class MockFC:
        def __init__(self, name, args):
            self.name = name
            self.args = args

    calls = [
        MockFC("get_weather", {"location": "Madrid"}),
        MockFC("get_time", {"timezone": "CET"}),
        MockFC("get_stock_price", {"symbol": "GOOGL"})
    ]

    results = await resilient_executor.execute_parallel_resilient(calls)
    for r in results:
        print(r)

# asyncio.run(main())
```

### 3. Batching y rate limiting

```python
import time
from collections import deque


class RateLimitedParallelExecutor(ParallelFunctionExecutor):
    """
    Ejecutor que respeta rate limits de APIs externas.
    """

    def __init__(
        self,
        tool_registry: Dict[str, Callable],
        rate_limits: Dict[str, Dict] = None
    ):
        """
        Args:
            tool_registry: Funciones disponibles
            rate_limits: {func_name: {"calls": N, "period": seconds}}
        """
        super().__init__(tool_registry)
        self.rate_limits = rate_limits or {}
        self.call_history: Dict[str, deque] = {}

    def _check_rate_limit(self, func_name: str) -> bool:
        """Verifica si podemos hacer la llamada."""
        if func_name not in self.rate_limits:
            return True

        limit = self.rate_limits[func_name]
        max_calls = limit["calls"]
        period = limit["period"]

        if func_name not in self.call_history:
            self.call_history[func_name] = deque()

        history = self.call_history[func_name]
        now = time.time()

        # Limpiar llamadas antiguas
        while history and now - history[0] > period:
            history.popleft()

        return len(history) < max_calls

    def _record_call(self, func_name: str):
        """Registra una llamada."""
        if func_name not in self.call_history:
            self.call_history[func_name] = deque()
        self.call_history[func_name].append(time.time())

    def _wait_for_rate_limit(self, func_name: str):
        """Espera si es necesario por rate limit."""
        if func_name not in self.rate_limits:
            return

        limit = self.rate_limits[func_name]
        period = limit["period"]

        while not self._check_rate_limit(func_name):
            history = self.call_history[func_name]
            if history:
                wait_time = period - (time.time() - history[0]) + 0.1
                if wait_time > 0:
                    print(f"Rate limit: esperando {wait_time:.1f}s para {func_name}")
                    time.sleep(wait_time)

    def execute_single_rate_limited(self, function_call) -> Dict:
        """Ejecuta respetando rate limits."""
        func_name = function_call.name
        self._wait_for_rate_limit(func_name)
        self._record_call(func_name)
        return self.execute_single(function_call)

    def execute_parallel_batched(
        self,
        function_calls: List,
        batch_size: int = 5
    ) -> List[Dict]:
        """
        Ejecuta en batches para respetar rate limits globales.
        """
        all_results = []

        for i in range(0, len(function_calls), batch_size):
            batch = function_calls[i:i + batch_size]
            print(f"Ejecutando batch {i//batch_size + 1}: {len(batch)} calls")

            # Ejecutar batch en paralelo
            futures = [
                self.executor.submit(self.execute_single_rate_limited, fc)
                for fc in batch
            ]

            results = [f.result() for f in futures]
            all_results.extend(results)

        return all_results


# Uso
rate_limited_executor = RateLimitedParallelExecutor(
    tool_registry={
        "get_weather": get_weather,
        "get_stock_price": get_stock_price
    },
    rate_limits={
        "get_weather": {"calls": 5, "period": 60},  # 5 calls por minuto
        "get_stock_price": {"calls": 10, "period": 60}  # 10 calls por minuto
    }
)
```

---

## Errores frecuentes

### Error 1: No manejar resultados parciales

```python
# ❌ Asumir que todas las calls son exitosas
def bad_handler(function_calls, executor):
    results = executor.execute_parallel(function_calls)
    # Si una falla, el procesamiento puede fallar
    return [r["result"]["value"] for r in results]  # KeyError si hay error

# ✓ Manejar errores individuales
def good_handler(function_calls, executor):
    results = executor.execute_parallel(function_calls)
    processed = []

    for r in results:
        if "error" in r:
            processed.append({
                "function": r["function_name"],
                "status": "failed",
                "error": r["error"]
            })
        else:
            processed.append({
                "function": r["function_name"],
                "status": "success",
                "data": r["result"]
            })

    return processed
```

### Error 2: No considerar dependencias

```python
# ❌ Ejecutar todo en paralelo sin considerar dependencias
def bad_parallel():
    # get_user_preferences necesita user_id de get_current_user
    calls = [get_current_user(), get_user_preferences(user_id=???)]
    execute_parallel(calls)  # ¡user_id no disponible!

# ✓ Detectar y respetar dependencias
def good_parallel():
    # Primero ejecutar calls sin dependencias
    user = get_current_user()

    # Luego las que dependen de los resultados
    preferences = get_user_preferences(user_id=user["id"])
```

### Error 3: Ignorar rate limits de APIs

```python
# ❌ Bombardear API sin límites
def bad_burst():
    # 100 llamadas simultáneas a una API con límite de 10/segundo
    results = execute_parallel([call() for _ in range(100)])
    # Resultado: 90 errores 429 (rate limit)

# ✓ Respetar límites
def good_burst():
    results = rate_limited_executor.execute_parallel_batched(
        calls,
        batch_size=10  # Respetar límite de API
    )
```

---

## Métricas de rendimiento

| Escenario | Secuencial | Paralelo | Mejora |
|-----------|------------|----------|--------|
| 3 APIs (500ms cada una) | ~1500ms | ~500ms | 3x |
| 5 APIs (200-800ms) | ~2500ms | ~800ms | 3x |
| 10 APIs (100-500ms) | ~3000ms | ~500ms | 6x |
| 10 APIs + rate limit | ~3000ms | ~1200ms | 2.5x |

---

## Resumen del concepto

**En una frase**: Parallel Function Calling permite ejecutar múltiples tools simultáneamente, reduciendo latencia y mejorando eficiencia.

**Cuándo usar**:
- Múltiples APIs independientes
- Datos de diferentes fuentes para una respuesta
- Operaciones I/O-bound (red, disco)

**Cuándo NO usar**:
- Funciones con dependencias entre sí
- APIs con rate limits estrictos (sin batching)
- Operaciones CPU-bound (no hay beneficio real)

**Consideraciones clave**:
- Manejar errores individualmente
- Respetar rate limits
- Considerar timeouts

**Siguiente paso**: Tema 3.2.1 - Patrones de Diseño de Tools.
