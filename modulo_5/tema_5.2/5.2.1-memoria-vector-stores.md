# 5.2.1 Memoria con Vector Stores

## Tiempo estimado: 90 minutos
## Nivel: Intermedio-Avanzado

## Prerrequisitos
- Tipos de memoria (Tema 5.1)
- Embeddings con Gemini (6.1.2)
- Conceptos b√°sicos de bases de datos

## ¬øPor qu√© es importante?

Los **Vector Stores** son el est√°ndar de facto para implementar memoria a largo plazo en agentes de IA:
- Permiten b√∫squeda sem√°ntica eficiente
- Escalan a millones de documentos
- Soportan filtrado por metadata
- Se integran f√°cilmente con pipelines de RAG

> "Los Vector Stores son la 'base de datos nativa' para IA - optimizados para encontrar informaci√≥n por significado, no solo por coincidencia exacta."

## Arquitectura de Memoria con Vector Store

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MEMORIA CON VECTOR STORE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                    FLUJO DE ALMACENAMIENTO                       ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Texto ‚îÄ‚îÄ‚ñ∂ Chunking ‚îÄ‚îÄ‚ñ∂ Embedding ‚îÄ‚îÄ‚ñ∂ Vector Store             ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                              ‚îÇ                                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                              ‚ñº                                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                         [768 dims]                               ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                    VECTOR STORE                                  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ                    √çNDICE VECTORIAL                      ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ                                                           ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ID    ‚îÇ  Vector [768]       ‚îÇ  Metadata               ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   doc_1 ‚îÇ  [0.12, -0.34, ...] ‚îÇ  {type: "email"}        ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   doc_2 ‚îÇ  [0.56, 0.78, ...]  ‚îÇ  {type: "task"}         ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   doc_3 ‚îÇ  [-0.23, 0.45, ...] ‚îÇ  {type: "insight"}      ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ...   ‚îÇ  ...                ‚îÇ  ...                     ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ                                                           ‚îÇ   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Algoritmos de B√∫squeda:                                        ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ HNSW (Hierarchical Navigable Small World)                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ IVF (Inverted File Index)                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Flat (Brute Force para datasets peque√±os)                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                    FLUJO DE CONSULTA                             ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Query ‚îÄ‚îÄ‚ñ∂ Embedding ‚îÄ‚îÄ‚ñ∂ ANN Search ‚îÄ‚îÄ‚ñ∂ Top-K Results          ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                              ‚îÇ                                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                              ‚ñº                                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                      Similarity Score                            ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                   ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Comparaci√≥n de Vector Stores

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VECTOR STORES POPULARES                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                        ‚îÇ
‚îÇ  CHROMADB (Local/Embebido)                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ ‚úÖ F√°cil de usar, no requiere servidor                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Ideal para desarrollo y prototipos                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Persistencia local simple                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚ùå No escala horizontalmente                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ üìä Hasta ~1M vectores en una m√°quina                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ  PINECONE (Cloud/Managed)                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Escalabilidad autom√°tica                                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Alta disponibilidad                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Metadata filtering potente                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚ùå Costo por uso                                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ üìä Billones de vectores                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ  WEAVIATE (Self-hosted/Cloud)                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ ‚úÖ B√∫squeda h√≠brida (vector + keyword)                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ GraphQL API                                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ M√≥dulos de ML integrados                                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚ùå Mayor complejidad de setup                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ üìä Millones de vectores                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ  QDRANT (Self-hosted/Cloud)                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Alto rendimiento (Rust)                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Filtrado rico                                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Soporte para m√∫ltiples vectores por documento                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚ùå Comunidad m√°s peque√±a                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ üìä Millones de vectores                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Implementaci√≥n con ChromaDB

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import Optional, Any
from datetime import datetime
import chromadb
from chromadb.config import Settings
import json
import os
import uuid

@dataclass
class MemoryDocument:
    """Documento para almacenar en memoria."""
    id: str
    content: str
    metadata: dict = field(default_factory=dict)
    embedding: Optional[list[float]] = None

    @classmethod
    def create(
        cls,
        content: str,
        memory_type: str = "general",
        user_id: str = None,
        **extra_metadata
    ) -> 'MemoryDocument':
        """Crea un documento con metadata est√°ndar."""
        return cls(
            id=str(uuid.uuid4()),
            content=content,
            metadata={
                "memory_type": memory_type,
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                **extra_metadata
            }
        )


class VectorStoreMemory:
    """
    Sistema de memoria basado en Vector Store (ChromaDB).
    """

    def __init__(
        self,
        collection_name: str = "agent_memory",
        persist_directory: str = "./chroma_db",
        model_name: str = "gemini-2.0-flash"
    ):
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        self.model = genai.GenerativeModel(model_name)

        # Inicializar ChromaDB
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # Crear o recuperar colecci√≥n
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Usar similitud coseno
        )

        print(f"üì¶ Vector Store inicializado: {collection_name}")
        print(f"   Documentos existentes: {self.collection.count()}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ALMACENAMIENTO
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def store(
        self,
        content: str,
        memory_type: str = "general",
        user_id: str = None,
        **metadata
    ) -> str:
        """
        Almacena un documento en la memoria.

        Args:
            content: Texto a almacenar
            memory_type: Tipo de memoria (interaction, task, insight, etc.)
            user_id: ID del usuario (opcional)
            **metadata: Metadata adicional

        Returns:
            ID del documento almacenado
        """
        doc = MemoryDocument.create(
            content=content,
            memory_type=memory_type,
            user_id=user_id,
            **metadata
        )

        # Generar embedding
        embedding = self._generate_embedding(content)

        # Almacenar en ChromaDB
        self.collection.add(
            ids=[doc.id],
            embeddings=[embedding],
            documents=[content],
            metadatas=[doc.metadata]
        )

        print(f"üíæ Almacenado: {doc.id[:8]}... ({memory_type})")
        return doc.id

    def store_batch(self, documents: list[MemoryDocument]) -> list[str]:
        """Almacena m√∫ltiples documentos en batch."""
        ids = []
        embeddings = []
        contents = []
        metadatas = []

        for doc in documents:
            ids.append(doc.id)
            embeddings.append(self._generate_embedding(doc.content))
            contents.append(doc.content)
            metadatas.append(doc.metadata)

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=contents,
            metadatas=metadatas
        )

        print(f"üíæ Almacenados {len(documents)} documentos en batch")
        return ids

    def store_interaction(
        self,
        user_message: str,
        agent_response: str,
        user_id: str = None,
        outcome: str = "neutral"
    ) -> str:
        """Almacena una interacci√≥n completa."""
        content = f"Usuario: {user_message}\nAsistente: {agent_response}"
        return self.store(
            content=content,
            memory_type="interaction",
            user_id=user_id,
            user_message=user_message,
            agent_response=agent_response,
            outcome=outcome
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # RECUPERACI√ìN
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def search(
        self,
        query: str,
        k: int = 5,
        memory_type: str = None,
        user_id: str = None,
        min_score: float = 0.0,
        **filter_kwargs
    ) -> list[dict]:
        """
        Busca documentos similares a la consulta.

        Args:
            query: Texto de b√∫squeda
            k: N√∫mero de resultados
            memory_type: Filtrar por tipo de memoria
            user_id: Filtrar por usuario
            min_score: Score m√≠nimo de similitud
            **filter_kwargs: Filtros adicionales de metadata

        Returns:
            Lista de documentos con scores
        """
        # Construir filtro
        where_filter = self._build_filter(memory_type, user_id, **filter_kwargs)

        # Generar embedding de la consulta
        query_embedding = self._generate_embedding(query)

        # Buscar
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            where=where_filter if where_filter else None,
            include=["documents", "metadatas", "distances"]
        )

        # Formatear resultados
        formatted = []
        for i in range(len(results["ids"][0])):
            # Convertir distancia a score (ChromaDB usa distancia, no similitud)
            distance = results["distances"][0][i]
            score = 1 - distance  # Para coseno: similitud = 1 - distancia

            if score >= min_score:
                formatted.append({
                    "id": results["ids"][0][i],
                    "content": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                    "score": score
                })

        return formatted

    def search_by_type(
        self,
        query: str,
        memory_type: str,
        k: int = 5
    ) -> list[dict]:
        """Busca por tipo espec√≠fico de memoria."""
        return self.search(query, k=k, memory_type=memory_type)

    def search_user_history(
        self,
        user_id: str,
        query: str = None,
        k: int = 10
    ) -> list[dict]:
        """Recupera historial de un usuario."""
        if query:
            return self.search(query, k=k, user_id=user_id)

        # Sin query, obtener los m√°s recientes
        results = self.collection.get(
            where={"user_id": user_id},
            include=["documents", "metadatas"]
        )

        # Ordenar por timestamp
        items = []
        for i in range(len(results["ids"])):
            items.append({
                "id": results["ids"][i],
                "content": results["documents"][i],
                "metadata": results["metadatas"][i]
            })

        items.sort(
            key=lambda x: x["metadata"].get("timestamp", ""),
            reverse=True
        )

        return items[:k]

    def get_similar_interactions(
        self,
        current_message: str,
        user_id: str = None,
        k: int = 3
    ) -> list[dict]:
        """Encuentra interacciones similares pasadas."""
        return self.search(
            query=current_message,
            k=k,
            memory_type="interaction",
            user_id=user_id,
            min_score=0.7
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GESTI√ìN
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def delete(self, doc_id: str):
        """Elimina un documento por ID."""
        self.collection.delete(ids=[doc_id])
        print(f"üóëÔ∏è Eliminado: {doc_id[:8]}...")

    def delete_by_filter(
        self,
        memory_type: str = None,
        user_id: str = None,
        **filter_kwargs
    ) -> int:
        """Elimina documentos que coincidan con el filtro."""
        where_filter = self._build_filter(memory_type, user_id, **filter_kwargs)

        if not where_filter:
            raise ValueError("Debe especificar al menos un filtro")

        # Obtener IDs que coinciden
        results = self.collection.get(
            where=where_filter,
            include=[]
        )

        if results["ids"]:
            self.collection.delete(ids=results["ids"])
            print(f"üóëÔ∏è Eliminados {len(results['ids'])} documentos")
            return len(results["ids"])

        return 0

    def update_metadata(self, doc_id: str, **new_metadata):
        """Actualiza metadata de un documento."""
        # ChromaDB no tiene update directo, hay que obtener y reinsertar
        result = self.collection.get(
            ids=[doc_id],
            include=["documents", "metadatas", "embeddings"]
        )

        if not result["ids"]:
            raise ValueError(f"Documento {doc_id} no encontrado")

        # Actualizar metadata
        current_metadata = result["metadatas"][0]
        current_metadata.update(new_metadata)

        # Actualizar
        self.collection.update(
            ids=[doc_id],
            metadatas=[current_metadata]
        )

    def get_statistics(self) -> dict:
        """Obtiene estad√≠sticas de la memoria."""
        total = self.collection.count()

        if total == 0:
            return {"total": 0}

        # Obtener todos para estad√≠sticas
        all_docs = self.collection.get(include=["metadatas"])

        by_type = {}
        by_user = {}
        for meta in all_docs["metadatas"]:
            mem_type = meta.get("memory_type", "unknown")
            user = meta.get("user_id", "anonymous")

            by_type[mem_type] = by_type.get(mem_type, 0) + 1
            by_user[user] = by_user.get(user, 0) + 1

        return {
            "total": total,
            "by_type": by_type,
            "by_user": by_user,
            "unique_users": len([u for u in by_user if u != "anonymous"])
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # UTILIDADES
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _generate_embedding(self, text: str) -> list[float]:
        """Genera embedding usando Gemini."""
        try:
            result = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            print(f"Error generando embedding: {e}")
            # Fallback
            import numpy as np
            return list(np.random.randn(768))

    def _build_filter(
        self,
        memory_type: str = None,
        user_id: str = None,
        **kwargs
    ) -> Optional[dict]:
        """Construye filtro para ChromaDB."""
        conditions = []

        if memory_type:
            conditions.append({"memory_type": memory_type})
        if user_id:
            conditions.append({"user_id": user_id})
        for key, value in kwargs.items():
            if value is not None:
                conditions.append({key: value})

        if not conditions:
            return None
        if len(conditions) == 1:
            return conditions[0]
        return {"$and": conditions}
```

## Agente con Vector Store Memory

```python
class AgentWithVectorMemory:
    """
    Agente que usa Vector Store para memoria persistente.
    """

    def __init__(self, model_name: str = "gemini-2.0-flash"):
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        self.model = genai.GenerativeModel(model_name)
        self.memory = VectorStoreMemory()
        self.current_user: Optional[str] = None

    def set_user(self, user_id: str):
        """Establece el usuario actual."""
        self.current_user = user_id
        print(f"üë§ Usuario: {user_id}")

    def respond(self, user_message: str) -> str:
        """Genera respuesta con contexto de memoria."""
        # 1. Buscar interacciones similares
        similar = self.memory.get_similar_interactions(
            user_message,
            user_id=self.current_user,
            k=3
        )

        # 2. Buscar insights relevantes
        insights = self.memory.search(
            user_message,
            k=2,
            memory_type="insight"
        )

        # 3. Construir contexto
        context = self._build_context(similar, insights)

        # 4. Generar respuesta
        prompt = f"""Eres un asistente con memoria a largo plazo.

## Memoria Relevante
{context}

## Mensaje del Usuario
{user_message}

## Instrucciones
- Si hay interacciones similares pasadas, aprende de ellas
- Si hay insights relevantes, √∫salos para informar tu respuesta
- S√© consistente con el historial del usuario
- Si no hay memoria relevante, responde normalmente

Respuesta:"""

        response = self.model.generate_content(prompt)
        agent_response = response.text

        # 5. Almacenar esta interacci√≥n
        self.memory.store_interaction(
            user_message=user_message,
            agent_response=agent_response,
            user_id=self.current_user
        )

        return agent_response

    def _build_context(
        self,
        similar: list[dict],
        insights: list[dict]
    ) -> str:
        """Construye el contexto de memoria."""
        parts = []

        if similar:
            parts.append("### Interacciones Similares Pasadas")
            for item in similar:
                score = item.get("score", 0)
                parts.append(f"[Similitud: {score:.0%}] {item['content'][:300]}")

        if insights:
            parts.append("\n### Insights Relevantes")
            for item in insights:
                parts.append(f"- {item['content']}")

        return "\n".join(parts) if parts else "No hay memoria relevante."

    def learn(self, insight: str, source: str = "observation"):
        """Almacena un aprendizaje."""
        self.memory.store(
            content=insight,
            memory_type="insight",
            user_id=self.current_user,
            source=source
        )
        print(f"üí° Insight almacenado: {insight[:50]}...")

    def forget_user(self, user_id: str):
        """Elimina toda la memoria de un usuario."""
        count = self.memory.delete_by_filter(user_id=user_id)
        print(f"üóëÔ∏è Eliminada memoria de {user_id}: {count} documentos")


# Uso
agent = AgentWithVectorMemory()
agent.set_user("user_123")

# Interacciones
response = agent.respond("¬øC√≥mo puedo aprender Python?")
print(response)

# El agente aprende
agent.learn("El usuario user_123 prefiere ejemplos pr√°cticos de c√≥digo")

# Siguiente interacci√≥n (usar√° el insight)
response = agent.respond("Expl√≠came las listas en Python")
print(response)

# Estad√≠sticas
print(agent.memory.get_statistics())
```

## Mejores Pr√°cticas

```python
class OptimizedVectorMemory(VectorStoreMemory):
    """
    Memoria optimizada con mejores pr√°cticas.
    """

    def store_with_chunking(
        self,
        content: str,
        chunk_size: int = 500,
        overlap: int = 50,
        **metadata
    ) -> list[str]:
        """
        Almacena texto largo dividi√©ndolo en chunks.
        """
        chunks = self._create_chunks(content, chunk_size, overlap)
        ids = []

        for i, chunk in enumerate(chunks):
            doc = MemoryDocument.create(
                content=chunk,
                chunk_index=i,
                total_chunks=len(chunks),
                **metadata
            )
            self.store(chunk, **doc.metadata)
            ids.append(doc.id)

        return ids

    def _create_chunks(
        self,
        text: str,
        chunk_size: int,
        overlap: int
    ) -> list[str]:
        """Divide texto en chunks con overlap."""
        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # Intentar cortar en l√≠mite de oraci√≥n
            if end < len(text):
                last_period = chunk.rfind('.')
                if last_period > chunk_size * 0.7:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1

            chunks.append(chunk)
            start = end - overlap

        return chunks

    def search_with_reranking(
        self,
        query: str,
        k: int = 5,
        initial_k: int = 20
    ) -> list[dict]:
        """
        B√∫squeda con re-ranking para mejor relevancia.
        """
        # 1. B√∫squeda inicial m√°s amplia
        initial_results = self.search(query, k=initial_k)

        if not initial_results:
            return []

        # 2. Re-ranking con LLM
        prompt = f"""Reordena estos documentos por relevancia para la consulta.

Consulta: {query}

Documentos:
{self._format_for_reranking(initial_results)}

Responde con los IDs ordenados del m√°s al menos relevante.
Formato: id1, id2, id3, ...
Solo incluye los {k} m√°s relevantes."""

        response = self.model.generate_content(prompt)

        # 3. Parsear orden
        ordered_ids = [id.strip() for id in response.text.split(',')]

        # 4. Retornar en nuevo orden
        id_to_result = {r['id']: r for r in initial_results}
        reranked = []
        for doc_id in ordered_ids[:k]:
            if doc_id in id_to_result:
                reranked.append(id_to_result[doc_id])

        return reranked

    def _format_for_reranking(self, results: list[dict]) -> str:
        lines = []
        for r in results:
            lines.append(f"ID: {r['id']}")
            lines.append(f"Content: {r['content'][:200]}...")
            lines.append("")
        return "\n".join(lines)
```

## Errores Comunes y Soluciones

### 1. Embeddings Inconsistentes

```python
# ‚ùå MAL: Diferentes modelos para store y query
def store(self, text):
    embedding = model_a.embed(text)

def search(self, query):
    query_emb = model_b.embed(query)  # ¬°Diferente modelo!

# ‚úÖ BIEN: Mismo modelo siempre
class ConsistentMemory:
    def __init__(self, embedding_model="models/embedding-001"):
        self.embedding_model = embedding_model

    def _embed(self, text: str) -> list[float]:
        return genai.embed_content(
            model=self.embedding_model,
            content=text
        )['embedding']
```

### 2. Metadata No Indexada

```python
# ‚ùå MAL: Filtrar en c√≥digo despu√©s de b√∫squeda
results = memory.search(query, k=1000)  # Traer muchos
filtered = [r for r in results if r['metadata']['user_id'] == user]  # Filtrar

# ‚úÖ BIEN: Usar filtros nativos del vector store
results = memory.search(
    query,
    k=10,
    where={"user_id": user}  # Filtro en la base de datos
)
```

## Resumen

Los **Vector Stores** son la base de la memoria a largo plazo:

**Componentes**:
- Embeddings para representaci√≥n sem√°ntica
- √çndice vectorial para b√∫squeda eficiente
- Metadata para filtrado

**Operaciones**:
1. **Store**: Convertir texto a embedding y almacenar
2. **Search**: B√∫squeda por similitud sem√°ntica
3. **Filter**: Filtrar por metadata
4. **Manage**: CRUD de documentos

**Mejores pr√°cticas**:
- Usar chunking para textos largos
- Mismo modelo de embedding siempre
- Filtrar en la base de datos, no en c√≥digo
- Re-ranking para mejorar relevancia

---

## Navegaci√≥n

- **Anterior**: [5.1.3 Memoria Sem√°ntica](../tema_5.1/5.1.3-memoria-semantica.md)
- **Siguiente**: [5.2.2 Memoria Estructurada (Grafos)](./5.2.2-memoria-grafos-conocimiento.md)
- **√çndice**: [README del Curso](../../README.md)
