# Role Prompting y Personas

**Tiempo estimado**: 40 minutos
**Nivel**: Intermedio
**Prerrequisitos**: Zero-Shot, One-Shot y Few-Shot Learning (2.1.2)

## ¿Por qué importa este concepto?

Role prompting permite cambiar fundamentalmente cómo el modelo aborda una tarea. Al asignar una "persona" o rol específico, el modelo:

- Adopta vocabulario y tono apropiados al contexto
- Aplica conocimiento especializado de forma más consistente
- Genera respuestas más coherentes con expectativas del rol
- Puede simular diferentes perspectivas para análisis multidimensional

Empresas como Google, Microsoft y startups de IA usan role prompting extensivamente para crear asistentes especializados sin fine-tuning.

---

## Anatomía de un Role Prompt

```
┌─────────────────────────────────────────────────────────────┐
│ DEFINICIÓN DEL ROL                                          │
│ ──────────────────                                          │
│ • Identidad: ¿Quién eres?                                   │
│ • Expertise: ¿Qué conocimientos tienes?                     │
│ • Experiencia: ¿Cuántos años/proyectos?                     │
│ • Contexto: ¿Dónde trabajas/operas?                         │
├─────────────────────────────────────────────────────────────┤
│ COMPORTAMIENTO ESPERADO                                     │
│ ──────────────────────                                      │
│ • Tono: ¿Formal, casual, técnico?                           │
│ • Estilo: ¿Conciso, detallado, socrático?                   │
│ • Limitaciones: ¿Qué NO debes hacer/saber?                  │
│ • Prioridades: ¿Qué es más importante?                      │
├─────────────────────────────────────────────────────────────┤
│ INSTRUCCIONES ESPECÍFICAS                                   │
│ ─────────────────────────                                   │
│ • Formato de respuestas                                     │
│ • Información a incluir/excluir                             │
│ • Cómo manejar incertidumbre                                │
└─────────────────────────────────────────────────────────────┘
```

---

## Implementación práctica

### Clase base para gestión de personas

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import Optional, List, Dict
from enum import Enum


class ToneStyle(Enum):
    FORMAL = "formal y profesional"
    CASUAL = "casual y amigable"
    TECHNICAL = "técnico y preciso"
    EDUCATIONAL = "educativo y paciente"
    EXECUTIVE = "ejecutivo y conciso"


@dataclass
class Persona:
    """Define una persona/rol para el modelo."""

    name: str
    role: str
    expertise: List[str]
    years_experience: int
    tone: ToneStyle
    context: str = ""
    constraints: List[str] = field(default_factory=list)
    priorities: List[str] = field(default_factory=list)

    def to_system_prompt(self) -> str:
        """Convierte la persona en un system prompt estructurado."""
        prompt_parts = []

        # Identidad
        prompt_parts.append(f"Eres {self.name}, {self.role}.")

        # Expertise
        expertise_str = ", ".join(self.expertise)
        prompt_parts.append(
            f"Tienes {self.years_experience} años de experiencia en: {expertise_str}."
        )

        # Contexto
        if self.context:
            prompt_parts.append(f"Contexto: {self.context}")

        # Tono
        prompt_parts.append(f"Tu comunicación es {self.tone.value}.")

        # Prioridades
        if self.priorities:
            priorities_str = "\n".join(f"- {p}" for p in self.priorities)
            prompt_parts.append(f"Tus prioridades son:\n{priorities_str}")

        # Restricciones
        if self.constraints:
            constraints_str = "\n".join(f"- {c}" for c in self.constraints)
            prompt_parts.append(f"Restricciones:\n{constraints_str}")

        return "\n\n".join(prompt_parts)


# Configurar modelo
genai.configure(api_key="TU_API_KEY")


class PersonaChat:
    """Chat con una persona específica."""

    def __init__(self, persona: Persona):
        self.persona = persona
        self.model = genai.GenerativeModel(
            "gemini-1.5-flash",
            system_instruction=persona.to_system_prompt()
        )
        self.chat = self.model.start_chat()

    def send(self, message: str) -> str:
        """Envía mensaje y recibe respuesta en el contexto del rol."""
        response = self.chat.send_message(message)
        return response.text

    def reset(self):
        """Reinicia la conversación manteniendo la persona."""
        self.chat = self.model.start_chat()
```

### Biblioteca de personas predefinidas

```python
class PersonaLibrary:
    """Biblioteca de personas comunes para diferentes casos de uso."""

    @staticmethod
    def senior_developer(language: str = "Python") -> Persona:
        return Persona(
            name="Alex",
            role=f"Desarrollador Senior de {language}",
            expertise=[
                f"Desarrollo en {language}",
                "Arquitectura de software",
                "Code review",
                "Testing y CI/CD",
                "Optimización de rendimiento"
            ],
            years_experience=15,
            tone=ToneStyle.TECHNICAL,
            context=f"Trabajas en una empresa tech Fortune 500",
            constraints=[
                "No escribir código inseguro o con vulnerabilidades conocidas",
                "Siempre considerar mantenibilidad a largo plazo",
                "Explicar el 'por qué' detrás de las decisiones técnicas"
            ],
            priorities=[
                "Código limpio y legible",
                "Rendimiento cuando es crítico",
                "Testabilidad",
                "Documentación útil (no excesiva)"
            ]
        )

    @staticmethod
    def product_manager() -> Persona:
        return Persona(
            name="María",
            role="Product Manager Senior",
            expertise=[
                "Gestión de producto",
                "Metodologías ágiles",
                "Análisis de mercado",
                "UX/UI",
                "Métricas de producto"
            ],
            years_experience=10,
            tone=ToneStyle.EXECUTIVE,
            context="Lideras productos B2B SaaS con millones de usuarios",
            constraints=[
                "Basar decisiones en datos, no en opiniones",
                "Considerar siempre el impacto en usuarios y negocio",
                "No prometer features sin validar viabilidad técnica"
            ],
            priorities=[
                "Valor para el usuario",
                "ROI del negocio",
                "Viabilidad técnica",
                "Time to market"
            ]
        )

    @staticmethod
    def security_expert() -> Persona:
        return Persona(
            name="Carlos",
            role="Experto en Seguridad Informática (CISSP, CEH)",
            expertise=[
                "Penetration testing",
                "Seguridad de aplicaciones",
                "Criptografía",
                "Compliance (SOC2, GDPR, HIPAA)",
                "Incident response"
            ],
            years_experience=12,
            tone=ToneStyle.FORMAL,
            context="Consultor de seguridad para empresas financieras y de salud",
            constraints=[
                "Nunca proporcionar exploits funcionales para uso malicioso",
                "Siempre recomendar prácticas seguras por defecto",
                "Advertir sobre implicaciones legales cuando aplique"
            ],
            priorities=[
                "Seguridad del usuario final",
                "Principio de mínimo privilegio",
                "Defense in depth",
                "Cumplimiento normativo"
            ]
        )

    @staticmethod
    def data_scientist() -> Persona:
        return Persona(
            name="Elena",
            role="Data Scientist / ML Engineer",
            expertise=[
                "Machine Learning",
                "Estadística aplicada",
                "Python (pandas, scikit-learn, PyTorch)",
                "Visualización de datos",
                "MLOps"
            ],
            years_experience=8,
            tone=ToneStyle.EDUCATIONAL,
            context="Trabajas en un equipo de IA aplicada a problemas de negocio",
            constraints=[
                "No hacer afirmaciones estadísticas sin evidencia",
                "Advertir sobre sesgos potenciales en modelos",
                "Explicar limitaciones de los enfoques propuestos"
            ],
            priorities=[
                "Rigor metodológico",
                "Interpretabilidad cuando es posible",
                "Reproducibilidad",
                "Escalabilidad de soluciones"
            ]
        )

    @staticmethod
    def tech_writer() -> Persona:
        return Persona(
            name="Daniel",
            role="Technical Writer Senior",
            expertise=[
                "Documentación técnica",
                "API documentation",
                "Tutoriales y guías",
                "Docs-as-code",
                "Accesibilidad en documentación"
            ],
            years_experience=7,
            tone=ToneStyle.EDUCATIONAL,
            context="Escribes documentación para desarrolladores en empresas tech",
            constraints=[
                "Usar ejemplos ejecutables y verificados",
                "Evitar jerga innecesaria",
                "Mantener consistencia en terminología"
            ],
            priorities=[
                "Claridad sobre completitud",
                "Ejemplos prácticos",
                "Estructura navegable",
                "Actualización constante"
            ]
        )
```

### Uso de personas en conversaciones

```python
# Crear chat con desarrollador senior
dev_persona = PersonaLibrary.senior_developer("Python")
dev_chat = PersonaChat(dev_persona)

# Consulta técnica
response = dev_chat.send("""
Tengo este código que procesa archivos CSV grandes:

for line in open('big_file.csv'):
    data = line.split(',')
    process(data)

¿Qué mejoras sugieres?
""")
print("Desarrollador Senior:", response)

# Crear chat con experto en seguridad
sec_persona = PersonaLibrary.security_expert()
sec_chat = PersonaChat(sec_persona)

# Consulta de seguridad
response = sec_chat.send("""
Estoy almacenando contraseñas con MD5 en mi aplicación.
¿Hay algún problema con este enfoque?
""")
print("\nExperto en Seguridad:", response)
```

---

## Técnicas avanzadas de Role Prompting

### 1. Múltiples perspectivas (Panel de expertos)

```python
class ExpertPanel:
    """
    Simula un panel de expertos que analizan
    un problema desde diferentes perspectivas.
    """

    def __init__(self, experts: List[Persona]):
        self.experts = experts
        self.model = genai.GenerativeModel("gemini-1.5-flash")

    def analyze(self, problem: str) -> Dict[str, str]:
        """Obtiene análisis de cada experto."""
        responses = {}

        for expert in self.experts:
            prompt = f"""
{expert.to_system_prompt()}

PROBLEMA A ANALIZAR:
{problem}

Proporciona tu análisis desde tu perspectiva profesional.
Sé específico y basa tu respuesta en tu expertise.
"""
            response = self.model.generate_content(prompt)
            responses[f"{expert.name} ({expert.role})"] = response.text

        return responses

    def synthesize(self, problem: str) -> str:
        """Analiza y sintetiza las perspectivas."""
        analyses = self.analyze(problem)

        synthesis_prompt = f"""
Eres un moderador experto que sintetiza múltiples perspectivas.

PROBLEMA ORIGINAL:
{problem}

ANÁLISIS DE EXPERTOS:
"""
        for expert, analysis in analyses.items():
            synthesis_prompt += f"\n\n--- {expert} ---\n{analysis}"

        synthesis_prompt += """

TAREA:
1. Identifica los puntos de acuerdo entre expertos
2. Señala las discrepancias o tensiones
3. Proporciona una recomendación balanceada
4. Lista los próximos pasos concretos
"""
        response = self.model.generate_content(synthesis_prompt)
        return response.text


# Uso
panel = ExpertPanel([
    PersonaLibrary.senior_developer(),
    PersonaLibrary.security_expert(),
    PersonaLibrary.product_manager()
])

problem = """
Necesitamos implementar autenticación para nuestra API.
Tenemos 100K usuarios activos y el tiempo de desarrollo es limitado (2 semanas).
¿Qué enfoque deberíamos tomar?
"""

# Análisis individual de cada experto
for expert, analysis in panel.analyze(problem).items():
    print(f"\n{'='*50}")
    print(f"EXPERTO: {expert}")
    print(f"{'='*50}")
    print(analysis)

# Síntesis
print("\n" + "="*50)
print("SÍNTESIS DEL PANEL")
print("="*50)
print(panel.synthesize(problem))
```

### 2. Persona dinámica basada en contexto

```python
class DynamicPersonaSelector:
    """Selecciona la persona más apropiada basándose en el contexto."""

    def __init__(self, personas: Dict[str, Persona]):
        self.personas = personas
        self.model = genai.GenerativeModel("gemini-1.5-flash")

    def classify_query(self, query: str) -> str:
        """Clasifica el tipo de query para seleccionar persona."""
        prompt = f"""
Clasifica la siguiente consulta en una de estas categorías:
- DEVELOPMENT: Preguntas sobre código, arquitectura, debugging
- SECURITY: Preguntas sobre vulnerabilidades, autenticación, compliance
- PRODUCT: Preguntas sobre features, roadmap, priorización
- DATA: Preguntas sobre ML, estadística, análisis de datos
- DOCUMENTATION: Preguntas sobre cómo documentar o explicar

CONSULTA: {query}

Responde SOLO con la categoría (una palabra).
"""
        response = self.model.generate_content(prompt)
        return response.text.strip().upper()

    def get_persona(self, query: str) -> Persona:
        """Obtiene la persona más apropiada para la consulta."""
        category = self.classify_query(query)
        return self.personas.get(category, list(self.personas.values())[0])

    def respond(self, query: str) -> Dict[str, str]:
        """Responde usando la persona más apropiada."""
        persona = self.get_persona(query)
        chat = PersonaChat(persona)

        return {
            "persona": f"{persona.name} ({persona.role})",
            "response": chat.send(query)
        }


# Configurar selector
selector = DynamicPersonaSelector({
    "DEVELOPMENT": PersonaLibrary.senior_developer(),
    "SECURITY": PersonaLibrary.security_expert(),
    "PRODUCT": PersonaLibrary.product_manager(),
    "DATA": PersonaLibrary.data_scientist(),
    "DOCUMENTATION": PersonaLibrary.tech_writer()
})

# Probar con diferentes queries
queries = [
    "¿Cómo optimizo esta query SQL que tarda 30 segundos?",
    "¿Es seguro usar JWT sin refresh tokens?",
    "¿Cómo priorizamos entre estas 5 features?",
    "¿Qué métricas debería trackear para mi modelo de churn?",
    "¿Cómo documento una API REST de forma efectiva?"
]

for query in queries:
    result = selector.respond(query)
    print(f"\nQuery: {query}")
    print(f"Persona: {result['persona']}")
    print(f"Respuesta: {result['response'][:200]}...")
```

### 3. Persona con memoria de contexto

```python
@dataclass
class ContextualPersona(Persona):
    """Persona que mantiene contexto de la conversación."""

    memory: List[str] = field(default_factory=list)
    max_memory_items: int = 10

    def add_to_memory(self, item: str):
        """Agrega item a la memoria."""
        self.memory.append(item)
        if len(self.memory) > self.max_memory_items:
            self.memory.pop(0)

    def to_system_prompt(self) -> str:
        """Incluye memoria en el prompt."""
        base_prompt = super().to_system_prompt()

        if self.memory:
            memory_str = "\n".join(f"- {m}" for m in self.memory)
            base_prompt += f"\n\nCONTEXTO DE LA CONVERSACIÓN:\n{memory_str}"

        return base_prompt


class ContextualPersonaChat(PersonaChat):
    """Chat que actualiza el contexto de la persona."""

    def __init__(self, persona: ContextualPersona):
        self.contextual_persona = persona
        super().__init__(persona)

    def send(self, message: str) -> str:
        """Envía mensaje y actualiza contexto."""
        # Extraer información relevante para memoria
        self._update_memory(message)

        # Recrear modelo con contexto actualizado
        self.model = genai.GenerativeModel(
            "gemini-1.5-flash",
            system_instruction=self.contextual_persona.to_system_prompt()
        )
        self.chat = self.model.start_chat()

        response = self.chat.send_message(message)
        return response.text

    def _update_memory(self, message: str):
        """Extrae y guarda información contextual relevante."""
        # Simplificado: guardar keywords importantes
        if "proyecto" in message.lower():
            self.contextual_persona.add_to_memory(f"Usuario mencionó proyecto: {message[:100]}")
        if "problema" in message.lower() or "error" in message.lower():
            self.contextual_persona.add_to_memory(f"Usuario reportó problema: {message[:100]}")
```

---

## Errores frecuentes

### Error 1: Rol demasiado vago

```python
# ❌ Rol vago que no guía el comportamiento
bad_persona = Persona(
    name="Asistente",
    role="Ayudante general",
    expertise=["Muchas cosas"],
    years_experience=5,
    tone=ToneStyle.CASUAL
)

# ✓ Rol específico con comportamiento definido
good_persona = Persona(
    name="Dr. García",
    role="Médico internista especializado en diagnóstico diferencial",
    expertise=[
        "Medicina interna",
        "Diagnóstico diferencial",
        "Interpretación de laboratorios",
        "Farmacología clínica"
    ],
    years_experience=20,
    tone=ToneStyle.FORMAL,
    constraints=[
        "Nunca dar diagnósticos definitivos sin examen presencial",
        "Siempre recomendar consulta con especialista cuando aplique",
        "Advertir sobre síntomas de emergencia"
    ]
)
```

### Error 2: Conflicto entre rol e instrucciones

```python
# ❌ Conflicto: experto junior?
bad_persona = Persona(
    name="Expert",
    role="Experto senior en seguridad",
    expertise=["Hacking ético", "Pentesting"],
    years_experience=15,
    tone=ToneStyle.CASUAL,
    constraints=[
        "No uses terminología técnica",  # Conflicto con ser experto
        "Responde como si no supieras mucho"  # Conflicto con 15 años exp
    ]
)

# ✓ Consistencia entre rol y comportamiento
good_persona = Persona(
    name="Carlos",
    role="Experto senior en seguridad",
    expertise=["Hacking ético", "Pentesting"],
    years_experience=15,
    tone=ToneStyle.EDUCATIONAL,  # Puede ser accesible sin perder expertise
    constraints=[
        "Adapta el nivel técnico según la audiencia",
        "Explica conceptos complejos con analogías cuando sea útil"
    ]
)
```

### Error 3: No validar respuestas fuera del rol

```python
# ❌ Aceptar cualquier respuesta
def bad_ask_expert(persona: Persona, question: str) -> str:
    chat = PersonaChat(persona)
    return chat.send(question)  # Sin validación

# ✓ Validar coherencia con el rol
def good_ask_expert(persona: Persona, question: str) -> Dict:
    chat = PersonaChat(persona)
    response = chat.send(question)

    # Verificar que la respuesta es coherente con el expertise
    validation_prompt = f"""
¿La siguiente respuesta es coherente con el rol de {persona.role}
con expertise en {', '.join(persona.expertise)}?

RESPUESTA: {response}

Responde solo "SÍ" o "NO" seguido de una breve explicación.
"""
    model = genai.GenerativeModel("gemini-1.5-flash")
    validation = model.generate_content(validation_prompt).text

    return {
        "response": response,
        "role_coherent": validation.startswith("SÍ"),
        "validation_note": validation
    }
```

---

## Aplicaciones reales

### Aplicación 1: Sistema de soporte técnico multinivel

```python
class TieredSupportSystem:
    """Sistema de soporte con diferentes niveles de expertise."""

    def __init__(self):
        self.tiers = {
            1: Persona(
                name="Soporte Nivel 1",
                role="Agente de soporte técnico básico",
                expertise=["Troubleshooting básico", "FAQ", "Escalamiento"],
                years_experience=1,
                tone=ToneStyle.CASUAL,
                constraints=[
                    "Escalar a nivel 2 si el problema requiere acceso técnico",
                    "No intentar soluciones que puedan dañar datos del usuario"
                ]
            ),
            2: Persona(
                name="Soporte Nivel 2",
                role="Técnico de soporte avanzado",
                expertise=["Debugging", "Logs analysis", "Configuración avanzada"],
                years_experience=5,
                tone=ToneStyle.TECHNICAL,
                constraints=[
                    "Escalar a nivel 3 si requiere cambios en código o infra",
                    "Documentar todos los pasos de diagnóstico"
                ]
            ),
            3: Persona(
                name="Soporte Nivel 3",
                role="Ingeniero de soporte / DevOps",
                expertise=["Código fuente", "Infraestructura", "Bases de datos"],
                years_experience=10,
                tone=ToneStyle.TECHNICAL,
                constraints=[
                    "Crear ticket para desarrollo si es bug confirmado",
                    "Notificar a seguridad si hay implicaciones de datos"
                ]
            )
        }

    def handle_ticket(self, issue: str, tier: int = 1) -> Dict:
        """Maneja un ticket en el nivel especificado."""
        persona = self.tiers.get(tier, self.tiers[1])
        chat = PersonaChat(persona)

        response = chat.send(f"""
TICKET DE SOPORTE:
{issue}

Proporciona:
1. Diagnóstico inicial
2. Pasos de solución si aplica
3. Indica si necesita escalamiento y por qué
""")

        # Detectar si necesita escalamiento
        needs_escalation = any(
            keyword in response.lower()
            for keyword in ["escalar", "nivel superior", "requiere acceso", "no puedo resolver"]
        )

        return {
            "tier": tier,
            "agent": persona.name,
            "response": response,
            "needs_escalation": needs_escalation,
            "next_tier": tier + 1 if needs_escalation and tier < 3 else None
        }
```

### Aplicación 2: Mentor de código adaptativo

```python
class AdaptiveCodeMentor:
    """Mentor que adapta su estilo según el nivel del estudiante."""

    LEVELS = {
        "beginner": Persona(
            name="Mentor Principiante",
            role="Instructor de programación para principiantes",
            expertise=["Fundamentos", "Buenas prácticas básicas"],
            years_experience=10,
            tone=ToneStyle.EDUCATIONAL,
            priorities=[
                "Explicaciones paso a paso",
                "Analogías del mundo real",
                "Evitar jerga técnica innecesaria",
                "Celebrar pequeños logros"
            ]
        ),
        "intermediate": Persona(
            name="Mentor Intermedio",
            role="Coach de desarrollo de software",
            expertise=["Patrones de diseño", "Testing", "Clean code"],
            years_experience=10,
            tone=ToneStyle.TECHNICAL,
            priorities=[
                "Explicar el 'por qué' no solo el 'cómo'",
                "Introducir mejores prácticas progresivamente",
                "Fomentar pensamiento crítico"
            ]
        ),
        "advanced": Persona(
            name="Mentor Avanzado",
            role="Arquitecto de software senior / Tech lead",
            expertise=["Arquitectura", "Escalabilidad", "Trade-offs"],
            years_experience=15,
            tone=ToneStyle.TECHNICAL,
            priorities=[
                "Discusiones de alto nivel",
                "Trade-offs y decisiones arquitectónicas",
                "Referencias a papers y recursos avanzados"
            ]
        )
    }

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")

    def detect_level(self, code_or_question: str) -> str:
        """Detecta el nivel del estudiante basándose en su código o pregunta."""
        prompt = f"""
Analiza el siguiente código o pregunta y determina el nivel del programador:
- beginner: Errores básicos, preguntas fundamentales
- intermediate: Código funcional pero mejorable, preguntas sobre patrones
- advanced: Código sofisticado, preguntas sobre arquitectura o trade-offs

CÓDIGO/PREGUNTA:
{code_or_question}

Responde SOLO con: beginner, intermediate, o advanced
"""
        response = self.model.generate_content(prompt)
        level = response.text.strip().lower()
        return level if level in self.LEVELS else "intermediate"

    def mentor(self, code_or_question: str, override_level: str = None) -> Dict:
        """Proporciona mentoría adaptada al nivel."""
        level = override_level or self.detect_level(code_or_question)
        persona = self.LEVELS[level]
        chat = PersonaChat(persona)

        response = chat.send(f"""
El estudiante presenta lo siguiente:

{code_or_question}

Proporciona feedback constructivo y guía apropiada para su nivel.
""")

        return {
            "detected_level": level,
            "mentor": persona.name,
            "feedback": response
        }
```

---

## Resumen del concepto

**En una frase**: Role prompting asigna una identidad especializada al modelo para obtener respuestas más precisas, consistentes y contextualmente apropiadas.

**Componentes clave de una persona efectiva**:
1. Identidad clara y específica
2. Expertise definido con detalle
3. Tono y estilo de comunicación
4. Restricciones y prioridades

**Cuándo usar**:
- Tareas que requieren expertise especializado
- Necesidad de tono o estilo específico
- Análisis desde múltiples perspectivas
- Sistemas de soporte o mentoría

**Siguiente paso**: Tema 2.2.1 - Chain of Thought (CoT) Prompting.
