# Prompts de Reflexión y Auto-Crítica

**Tiempo estimado**: 45 minutos
**Nivel**: Avanzado
**Prerrequisitos**: Prompts de Planificación y Descomposición (2.3.1)

## ¿Por qué importa este concepto?

Los agentes más efectivos no solo ejecutan tareas, sino que reflexionan sobre su propio razonamiento. La reflexión y auto-crítica permiten:

- Detectar errores antes de que se propaguen
- Mejorar iterativamente la calidad de las respuestas
- Identificar gaps en el conocimiento o razonamiento
- Aumentar la confiabilidad en tareas críticas
- Simular el proceso de revisión humana

Técnicas como Reflexion (Shinn et al., 2023) han demostrado mejoras significativas en tareas de código, razonamiento y toma de decisiones.

---

## Patrones de reflexión

```
┌─────────────────────────────────────────────────────────────────┐
│ REFLEXIÓN SIMPLE                                                │
│ ────────────────                                                │
│                                                                 │
│ Respuesta inicial ──► Reflexión ──► Respuesta mejorada         │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│ REFLEXIÓN ITERATIVA                                             │
│ ───────────────────                                             │
│                                                                 │
│ R₀ ──► Reflexión ──► R₁ ──► Reflexión ──► R₂ ──► ... ──► Rₙ   │
│                                                                 │
│ Continúa hasta convergencia o límite de iteraciones            │
├─────────────────────────────────────────────────────────────────┤
│ REFLEXIÓN CON CRITERIOS EXTERNOS                                │
│ ────────────────────────────────                                │
│                                                                 │
│                    ┌─────────────┐                              │
│ Respuesta ────────►│  Evaluador  │                              │
│     ▲              │  (Criterios)│                              │
│     │              └──────┬──────┘                              │
│     │                     │                                     │
│     │    Feedback         ▼                                     │
│     └────────────────[Mejoras]                                  │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│ AUTO-CRÍTICA MULTI-PERSPECTIVA                                  │
│ ──────────────────────────────                                  │
│                                                                 │
│              ┌─► Crítico 1 (Corrección) ──┐                     │
│              │                            │                     │
│ Respuesta ───┼─► Crítico 2 (Claridad) ────┼──► Síntesis         │
│              │                            │                     │
│              └─► Crítico 3 (Completitud) ─┘                     │
└─────────────────────────────────────────────────────────────────┘
```

---

## Implementación práctica

### Framework de reflexión

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable
from enum import Enum
import json


class ReflectionType(Enum):
    CORRECTNESS = "correctness"
    CLARITY = "clarity"
    COMPLETENESS = "completeness"
    EFFICIENCY = "efficiency"
    SAFETY = "safety"


@dataclass
class Critique:
    """Representa una crítica específica."""
    type: ReflectionType
    issue: str
    severity: str  # "low", "medium", "high"
    suggestion: str
    line_reference: Optional[str] = None


@dataclass
class ReflectionResult:
    """Resultado de un ciclo de reflexión."""
    original: str
    critiques: List[Critique]
    improved: str
    iteration: int
    confidence_before: float
    confidence_after: float

    @property
    def improvement_score(self) -> float:
        return self.confidence_after - self.confidence_before


class ReflectionEngine:
    """Motor de reflexión y auto-crítica para respuestas."""

    def __init__(
        self,
        model_name: str = "gemini-1.5-flash",
        max_iterations: int = 3,
        improvement_threshold: float = 0.05
    ):
        genai.configure(api_key="TU_API_KEY")
        self.model = genai.GenerativeModel(model_name)
        self.max_iterations = max_iterations
        self.improvement_threshold = improvement_threshold

    def _generate_critique(
        self,
        response: str,
        context: str,
        criteria: List[ReflectionType]
    ) -> List[Critique]:
        """Genera críticas para una respuesta."""

        criteria_prompts = {
            ReflectionType.CORRECTNESS: "¿Hay errores factuales, lógicos o de cálculo?",
            ReflectionType.CLARITY: "¿Es clara y fácil de entender? ¿Hay ambigüedades?",
            ReflectionType.COMPLETENESS: "¿Responde completamente la pregunta? ¿Falta información?",
            ReflectionType.EFFICIENCY: "¿Es conciso? ¿Hay redundancias o texto innecesario?",
            ReflectionType.SAFETY: "¿Hay contenido problemático, sesgos o información peligrosa?"
        }

        criteria_text = "\n".join(
            f"- {c.value.upper()}: {criteria_prompts[c]}"
            for c in criteria
        )

        prompt = f"""
Actúa como un revisor crítico experto. Analiza la siguiente respuesta.

CONTEXTO/PREGUNTA ORIGINAL:
{context}

RESPUESTA A EVALUAR:
{response}

CRITERIOS DE EVALUACIÓN:
{criteria_text}

Para cada problema encontrado, proporciona:
1. Tipo de problema (de los criterios anteriores)
2. Descripción del problema
3. Severidad (low/medium/high)
4. Sugerencia de mejora específica

Responde en JSON:
{{
  "critiques": [
    {{
      "type": "correctness|clarity|completeness|efficiency|safety",
      "issue": "descripción del problema",
      "severity": "low|medium|high",
      "suggestion": "cómo corregirlo"
    }}
  ],
  "overall_quality": 0-10,
  "key_strengths": ["fortaleza 1", "fortaleza 2"]
}}

Si no hay problemas en algún criterio, no lo incluyas.
"""
        response_text = self.model.generate_content(prompt).text

        try:
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            data = json.loads(response_text[json_start:json_end])

            return [
                Critique(
                    type=ReflectionType(c["type"]),
                    issue=c["issue"],
                    severity=c["severity"],
                    suggestion=c["suggestion"]
                )
                for c in data.get("critiques", [])
            ]
        except (json.JSONDecodeError, KeyError, ValueError):
            return []

    def _apply_improvements(
        self,
        response: str,
        critiques: List[Critique],
        context: str
    ) -> str:
        """Aplica mejoras basándose en las críticas."""

        if not critiques:
            return response

        critiques_text = "\n".join(
            f"- [{c.severity.upper()}] {c.type.value}: {c.issue}\n  Sugerencia: {c.suggestion}"
            for c in critiques
        )

        prompt = f"""
Mejora la siguiente respuesta basándote en las críticas recibidas.

CONTEXTO ORIGINAL:
{context}

RESPUESTA ACTUAL:
{response}

CRÍTICAS A ABORDAR:
{critiques_text}

INSTRUCCIONES:
1. Corrige todos los problemas identificados
2. Mantén lo que ya estaba bien
3. Asegúrate de que la respuesta mejorada sea coherente
4. No menciones las críticas en la respuesta

RESPUESTA MEJORADA:
"""
        return self.model.generate_content(prompt).text

    def _estimate_confidence(self, response: str, context: str) -> float:
        """Estima la confianza en una respuesta."""
        prompt = f"""
Evalúa la calidad de esta respuesta en escala 0-10.

PREGUNTA: {context[:200]}
RESPUESTA: {response[:500]}

Considera: corrección, claridad, completitud.
Responde SOLO con un número del 0 al 10.
"""
        try:
            score = float(self.model.generate_content(prompt).text.strip())
            return score / 10
        except ValueError:
            return 0.5

    def reflect(
        self,
        response: str,
        context: str,
        criteria: Optional[List[ReflectionType]] = None
    ) -> ReflectionResult:
        """
        Ejecuta un ciclo de reflexión sobre una respuesta.

        Args:
            response: Respuesta a evaluar
            context: Contexto/pregunta original
            criteria: Criterios de evaluación

        Returns:
            Resultado con críticas y respuesta mejorada
        """
        if criteria is None:
            criteria = [
                ReflectionType.CORRECTNESS,
                ReflectionType.CLARITY,
                ReflectionType.COMPLETENESS
            ]

        confidence_before = self._estimate_confidence(response, context)
        critiques = self._generate_critique(response, context, criteria)

        if critiques:
            improved = self._apply_improvements(response, critiques, context)
        else:
            improved = response

        confidence_after = self._estimate_confidence(improved, context)

        return ReflectionResult(
            original=response,
            critiques=critiques,
            improved=improved,
            iteration=1,
            confidence_before=confidence_before,
            confidence_after=confidence_after
        )

    def reflect_iterative(
        self,
        response: str,
        context: str,
        criteria: Optional[List[ReflectionType]] = None
    ) -> List[ReflectionResult]:
        """
        Ejecuta múltiples ciclos de reflexión hasta convergencia.

        Returns:
            Lista de resultados de cada iteración
        """
        results = []
        current_response = response

        for i in range(self.max_iterations):
            result = self.reflect(current_response, context, criteria)
            result.iteration = i + 1
            results.append(result)

            # Verificar convergencia
            if result.improvement_score < self.improvement_threshold:
                break

            if not result.critiques:
                break

            current_response = result.improved

        return results
```

### Uso básico

```python
# Crear motor de reflexión
reflector = ReflectionEngine(max_iterations=3)

# Respuesta inicial (potencialmente con errores)
initial_response = """
Python es un lenguaje de programación compilado creado en 1995.
Es conocido por su sintaxis verbosa y su uso principal en desarrollo web.
Para ejecutar un programa Python, debes compilarlo primero con el comando 'python compile script.py'.
"""

context = "Explica qué es Python y cómo ejecutar un programa básico."

# Reflexión simple
result = reflector.reflect(initial_response, context)

print("RESPUESTA ORIGINAL:")
print(initial_response)
print("\nCRÍTICAS ENCONTRADAS:")
for c in result.critiques:
    print(f"  [{c.severity}] {c.type.value}: {c.issue}")
    print(f"    Sugerencia: {c.suggestion}")
print("\nRESPUESTA MEJORADA:")
print(result.improved)
print(f"\nConfianza: {result.confidence_before:.0%} → {result.confidence_after:.0%}")
```

---

## Patrones avanzados de reflexión

### 1. Reflexión con múltiples perspectivas

```python
class MultiPerspectiveReflection:
    """
    Reflexión desde múltiples perspectivas/roles.
    """

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")

    def reflect_multi_perspective(
        self,
        response: str,
        context: str,
        perspectives: List[Dict[str, str]]
    ) -> Dict:
        """
        Evalúa desde múltiples perspectivas.

        Args:
            response: Respuesta a evaluar
            context: Contexto original
            perspectives: Lista de {"role": "...", "focus": "..."}
        """
        all_critiques = []

        for perspective in perspectives:
            prompt = f"""
Actúa como {perspective['role']}.
Tu enfoque principal es: {perspective['focus']}

CONTEXTO:
{context}

RESPUESTA A EVALUAR:
{response}

Desde tu perspectiva como {perspective['role']}, identifica:
1. Problemas o preocupaciones (si hay)
2. Sugerencias de mejora

Responde en JSON:
{{
  "perspective": "{perspective['role']}",
  "concerns": ["preocupación 1", "preocupación 2"],
  "suggestions": ["sugerencia 1", "sugerencia 2"],
  "approval": true|false
}}
"""
            result = self.model.generate_content(prompt)
            try:
                json_start = result.text.find('{')
                json_end = result.text.rfind('}') + 1
                critique = json.loads(result.text[json_start:json_end])
                all_critiques.append(critique)
            except (json.JSONDecodeError, ValueError):
                pass

        # Sintetizar críticas
        synthesis = self._synthesize_critiques(response, context, all_critiques)

        return {
            "perspectives": all_critiques,
            "synthesis": synthesis,
            "approval_rate": sum(1 for c in all_critiques if c.get("approval", False)) / len(all_critiques)
        }

    def _synthesize_critiques(
        self,
        response: str,
        context: str,
        critiques: List[Dict]
    ) -> str:
        """Sintetiza críticas de múltiples perspectivas."""
        critiques_summary = json.dumps(critiques, indent=2, ensure_ascii=False)

        prompt = f"""
Sintetiza las siguientes críticas de múltiples expertos y genera una respuesta mejorada.

RESPUESTA ORIGINAL:
{response}

CRÍTICAS DE EXPERTOS:
{critiques_summary}

Genera una respuesta mejorada que:
1. Aborde las preocupaciones de todos los expertos
2. Incorpore las mejores sugerencias
3. Mantenga coherencia y claridad

RESPUESTA SINTETIZADA:
"""
        return self.model.generate_content(prompt).text


# Uso
multi_reflector = MultiPerspectiveReflection()

response = """
Para optimizar el rendimiento de tu aplicación web, deberías:
1. Usar caché agresivo en todas las respuestas
2. Minimizar el JavaScript cargando todo asíncronamente
3. Usar imágenes en formato WebP
"""

perspectives = [
    {"role": "Ingeniero de Backend", "focus": "rendimiento del servidor y escalabilidad"},
    {"role": "Experto en Seguridad", "focus": "vulnerabilidades y mejores prácticas de seguridad"},
    {"role": "Especialista en UX", "focus": "experiencia del usuario y accesibilidad"},
]

result = multi_reflector.reflect_multi_perspective(
    response,
    "¿Cómo optimizo el rendimiento de mi aplicación web?",
    perspectives
)

print(f"Tasa de aprobación: {result['approval_rate']:.0%}")
for p in result["perspectives"]:
    print(f"\n{p['perspective']}:")
    print(f"  Preocupaciones: {p.get('concerns', [])}")
    print(f"  Aprueba: {p.get('approval', False)}")
print(f"\nRespuesta sintetizada:\n{result['synthesis']}")
```

### 2. Auto-crítica con verificación de hechos

```python
class FactCheckingReflection:
    """
    Reflexión que verifica hechos contra fuentes confiables.
    """

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")

    def extract_claims(self, response: str) -> List[str]:
        """Extrae afirmaciones verificables de una respuesta."""
        prompt = f"""
Extrae todas las afirmaciones factuales verificables del siguiente texto.
Ignora opiniones y declaraciones subjetivas.

TEXTO:
{response}

Lista cada afirmación en una línea separada.
Solo afirmaciones verificables, no opiniones.

AFIRMACIONES:
"""
        result = self.model.generate_content(prompt)
        claims = [
            line.strip()
            for line in result.text.split('\n')
            if line.strip() and not line.strip().startswith('#')
        ]
        return claims

    def verify_claim(self, claim: str) -> Dict:
        """Verifica una afirmación individual."""
        prompt = f"""
Verifica la siguiente afirmación:
"{claim}"

Evalúa:
1. ¿Es verdadera, falsa, o parcialmente correcta?
2. ¿Cuál es la información correcta?
3. ¿Qué nivel de confianza tienes? (alto/medio/bajo)

Responde en JSON:
{{
  "claim": "{claim}",
  "verdict": "true|false|partial",
  "correction": "información correcta si es necesario",
  "confidence": "high|medium|low",
  "reasoning": "breve explicación"
}}
"""
        result = self.model.generate_content(prompt)
        try:
            json_start = result.text.find('{')
            json_end = result.text.rfind('}') + 1
            return json.loads(result.text[json_start:json_end])
        except (json.JSONDecodeError, ValueError):
            return {"claim": claim, "verdict": "unknown", "confidence": "low"}

    def reflect_with_fact_check(
        self,
        response: str,
        context: str
    ) -> Dict:
        """Reflexiona verificando hechos."""
        # Extraer y verificar afirmaciones
        claims = self.extract_claims(response)
        verifications = [self.verify_claim(claim) for claim in claims]

        # Identificar errores
        errors = [v for v in verifications if v.get("verdict") in ["false", "partial"]]

        # Generar versión corregida si hay errores
        if errors:
            corrections_text = "\n".join(
                f"- Error: {e['claim']}\n  Corrección: {e.get('correction', 'N/A')}"
                for e in errors
            )

            prompt = f"""
Corrige la siguiente respuesta basándote en los errores identificados.

RESPUESTA ORIGINAL:
{response}

ERRORES ENCONTRADOS:
{corrections_text}

Genera una versión corregida que:
1. Corrija todos los errores factuales
2. Mantenga la estructura y tono original
3. Sea precisa y verificable

RESPUESTA CORREGIDA:
"""
            corrected = self.model.generate_content(prompt).text
        else:
            corrected = response

        accuracy = sum(
            1 for v in verifications if v.get("verdict") == "true"
        ) / len(verifications) if verifications else 1.0

        return {
            "claims_found": len(claims),
            "verifications": verifications,
            "errors_found": len(errors),
            "accuracy_score": accuracy,
            "corrected_response": corrected
        }


# Uso
fact_checker = FactCheckingReflection()

response_with_errors = """
Python fue creado por James Gosling en 1989.
Es el lenguaje más usado del mundo con más del 80% de participación.
Python 3.0 fue lanzado en 2010 con cambios incompatibles con Python 2.
"""

result = fact_checker.reflect_with_fact_check(
    response_with_errors,
    "Historia y estadísticas de Python"
)

print(f"Afirmaciones encontradas: {result['claims_found']}")
print(f"Errores encontrados: {result['errors_found']}")
print(f"Precisión: {result['accuracy_score']:.0%}")
print("\nVerificaciones:")
for v in result["verifications"]:
    status = "✓" if v["verdict"] == "true" else "✗" if v["verdict"] == "false" else "~"
    print(f"  {status} {v['claim']}")
    if v["verdict"] != "true":
        print(f"    → {v.get('correction', 'N/A')}")
print(f"\nRespuesta corregida:\n{result['corrected_response']}")
```

### 3. Reflexión con memoria de errores pasados

```python
class LearningReflection:
    """
    Reflexión que aprende de errores pasados para
    evitar repetirlos.
    """

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")
        self.error_memory: List[Dict] = []
        self.max_memory = 50

    def add_error_to_memory(self, error: Dict):
        """Agrega un error al historial."""
        self.error_memory.append(error)
        if len(self.error_memory) > self.max_memory:
            self.error_memory.pop(0)

    def get_relevant_past_errors(self, context: str, k: int = 5) -> List[Dict]:
        """Obtiene errores pasados relevantes al contexto actual."""
        if not self.error_memory:
            return []

        # Simplificado: búsqueda por palabras clave
        context_words = set(context.lower().split())
        scored_errors = []

        for error in self.error_memory:
            error_words = set(error.get("context", "").lower().split())
            overlap = len(context_words & error_words)
            scored_errors.append((overlap, error))

        scored_errors.sort(reverse=True, key=lambda x: x[0])
        return [e for _, e in scored_errors[:k]]

    def reflect_with_learning(
        self,
        response: str,
        context: str
    ) -> Dict:
        """Reflexiona usando memoria de errores pasados."""

        # Obtener errores pasados relevantes
        past_errors = self.get_relevant_past_errors(context)

        past_errors_text = ""
        if past_errors:
            past_errors_text = "ERRORES PASADOS SIMILARES A EVITAR:\n"
            for err in past_errors:
                past_errors_text += f"- {err['error']}: {err['lesson']}\n"

        prompt = f"""
Actúa como un revisor crítico con memoria de errores pasados.

CONTEXTO:
{context}

RESPUESTA A EVALUAR:
{response}

{past_errors_text}

Evalúa la respuesta considerando:
1. Errores generales (lógicos, factuales, de claridad)
2. Errores específicos que hemos cometido antes (ver lista arriba)

Para cada problema:
- Describe el error
- Explica por qué es problemático
- Sugiere la corrección
- Indica si es un error recurrente

Responde en JSON:
{{
  "errors": [
    {{
      "error": "descripción",
      "is_recurring": true|false,
      "correction": "cómo corregirlo"
    }}
  ],
  "improved_response": "respuesta mejorada completa"
}}
"""
        result = self.model.generate_content(prompt)

        try:
            json_start = result.text.find('{')
            json_end = result.text.rfind('}') + 1
            data = json.loads(result.text[json_start:json_end])

            # Agregar nuevos errores a la memoria
            for error in data.get("errors", []):
                self.add_error_to_memory({
                    "context": context,
                    "error": error["error"],
                    "lesson": error["correction"]
                })

            return {
                "errors": data.get("errors", []),
                "recurring_errors": sum(1 for e in data.get("errors", []) if e.get("is_recurring")),
                "improved": data.get("improved_response", response),
                "memory_size": len(self.error_memory)
            }

        except (json.JSONDecodeError, ValueError):
            return {
                "errors": [],
                "recurring_errors": 0,
                "improved": response,
                "memory_size": len(self.error_memory)
            }


# Uso
learning_reflector = LearningReflection()

# Simular errores pasados
learning_reflector.add_error_to_memory({
    "context": "explicar conceptos de programación",
    "error": "Usar jerga técnica sin explicar",
    "lesson": "Siempre definir términos técnicos antes de usarlos"
})

result = learning_reflector.reflect_with_learning(
    "Debes usar un singleton con lazy initialization para evitar race conditions.",
    "Explica cómo implementar un patrón de diseño para principiantes"
)

print(f"Errores encontrados: {len(result['errors'])}")
print(f"Errores recurrentes: {result['recurring_errors']}")
print(f"Tamaño de memoria: {result['memory_size']}")
```

---

## Errores frecuentes

### Error 1: Reflexión infinita

```python
# ❌ Sin límite de iteraciones o criterio de parada
def bad_reflection_loop(response, context):
    while True:  # ¡Bucle infinito potencial!
        result = reflect(response, context)
        if result.improved != response:
            response = result.improved
        # Sin condición de salida

# ✓ Con límites y criterios de convergencia
def good_reflection_loop(response, context, max_iter=3, threshold=0.05):
    for i in range(max_iter):
        result = reflect(response, context)

        # Criterio 1: Sin mejora significativa
        if result.improvement_score < threshold:
            break

        # Criterio 2: Sin críticas
        if not result.critiques:
            break

        # Criterio 3: Respuesta idéntica
        if result.improved == response:
            break

        response = result.improved

    return response
```

### Error 2: Auto-crítica destructiva

```python
# ❌ Crítico demasiado severo que destruye contenido válido
bad_prompt = """
Encuentra TODOS los problemas con esta respuesta.
Sé extremadamente crítico. Incluso pequeños issues son importantes.
"""
# Resultado: Reescritura completa innecesaria

# ✓ Crítica balanceada que preserva lo bueno
good_prompt = """
Evalúa esta respuesta balanceando fortalezas y áreas de mejora.

Identifica:
1. Lo que está bien (mantener)
2. Problemas significativos (corregir)
3. Mejoras opcionales (considerar si hay tiempo)

Prioriza problemas por severidad. Solo corrige lo que realmente necesita corrección.
"""
```

### Error 3: Ignorar el contexto en la reflexión

```python
# ❌ Reflexión sin considerar el contexto original
def bad_reflect(response):
    # Evalúa la respuesta en el vacío
    return evaluate(response)

# ✓ Reflexión contextualizada
def good_reflect(response, context, audience, goal):
    # Evalúa según el propósito específico
    prompt = f"""
    Contexto: {context}
    Audiencia: {audience}
    Objetivo: {goal}

    ¿La respuesta cumple con el objetivo para esta audiencia específica?
    """
    return evaluate_with_context(response, prompt)
```

---

## Aplicaciones reales

### Aplicación: Agente de código auto-corregible

```python
class SelfCorrectingCodeAgent:
    """
    Agente que genera código y lo corrige mediante reflexión.
    """

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")
        self.reflector = ReflectionEngine(max_iterations=3)

    def generate_code(self, task: str, language: str = "Python") -> str:
        """Genera código inicial."""
        prompt = f"""
Escribe código en {language} para:
{task}

Requisitos:
- Código limpio y bien documentado
- Manejo de errores apropiado
- Ejemplos de uso

```{language.lower()}
"""
        response = self.model.generate_content(prompt)
        return response.text

    def review_code(self, code: str, task: str) -> Dict:
        """Revisa código con criterios específicos."""
        prompt = f"""
Revisa el siguiente código.

TAREA ORIGINAL:
{task}

CÓDIGO:
{code}

Evalúa:
1. CORRECCIÓN: ¿El código hace lo que debería?
2. BUGS: ¿Hay errores lógicos o de sintaxis?
3. EDGE CASES: ¿Maneja casos límite?
4. SEGURIDAD: ¿Hay vulnerabilidades?
5. RENDIMIENTO: ¿Es eficiente?
6. ESTILO: ¿Sigue mejores prácticas?

Responde en JSON:
{{
  "issues": [
    {{
      "category": "CORRECCIÓN|BUGS|EDGE_CASES|SEGURIDAD|RENDIMIENTO|ESTILO",
      "severity": "critical|major|minor",
      "line": "número o rango",
      "description": "qué está mal",
      "fix": "cómo corregirlo"
    }}
  ],
  "overall_score": 0-10
}}
"""
        response = self.model.generate_content(prompt)
        try:
            json_start = response.text.find('{')
            json_end = response.text.rfind('}') + 1
            return json.loads(response.text[json_start:json_end])
        except (json.JSONDecodeError, ValueError):
            return {"issues": [], "overall_score": 5}

    def fix_code(self, code: str, issues: List[Dict]) -> str:
        """Corrige código basándose en issues."""
        issues_text = "\n".join(
            f"- [{i['severity']}] {i['category']}: {i['description']}\n  Fix: {i['fix']}"
            for i in issues
        )

        prompt = f"""
Corrige el siguiente código basándote en los issues identificados.

CÓDIGO ORIGINAL:
{code}

ISSUES A CORREGIR:
{issues_text}

Proporciona el código corregido completo.
No incluyas explicaciones, solo el código.

```python
"""
        response = self.model.generate_content(prompt)
        return response.text

    def generate_and_refine(self, task: str, language: str = "Python") -> Dict:
        """Genera código y lo refina iterativamente."""
        # Generación inicial
        code = self.generate_code(task, language)

        history = [{"iteration": 0, "code": code, "issues": []}]

        for i in range(3):
            # Revisar
            review = self.review_code(code, task)
            issues = review.get("issues", [])
            score = review.get("overall_score", 0)

            history[-1]["issues"] = issues
            history[-1]["score"] = score

            # Si no hay issues críticos o mayores, terminar
            critical_issues = [
                iss for iss in issues
                if iss.get("severity") in ["critical", "major"]
            ]

            if not critical_issues or score >= 8:
                break

            # Corregir
            code = self.fix_code(code, critical_issues)
            history.append({"iteration": i + 1, "code": code, "issues": []})

        return {
            "final_code": code,
            "iterations": len(history),
            "history": history,
            "final_score": history[-1].get("score", 0)
        }


# Uso
agent = SelfCorrectingCodeAgent()

result = agent.generate_and_refine(
    task="Función para validar emails con regex, incluyendo casos edge como subdominios",
    language="Python"
)

print(f"Iteraciones: {result['iterations']}")
print(f"Score final: {result['final_score']}/10")
print(f"\nCódigo final:\n{result['final_code']}")
```

---

## Resumen del concepto

**En una frase**: La reflexión y auto-crítica permiten que los agentes evalúen y mejoren sus propias respuestas iterativamente.

**Patrones clave**:
1. **Reflexión simple**: Evaluar → Criticar → Mejorar
2. **Reflexión iterativa**: Ciclos hasta convergencia
3. **Multi-perspectiva**: Críticas desde diferentes roles
4. **Con verificación**: Fact-checking integrado
5. **Con memoria**: Aprender de errores pasados

**Cuándo usar reflexión**:
- Respuestas que requieren alta precisión
- Tareas donde los errores son costosos
- Generación de código o contenido técnico
- Cuando hay tiempo/presupuesto para iteraciones

**Trade-off principal**: Calidad vs costo (más iteraciones = más tokens)

**Siguiente paso**: Tema 2.3.3 - Manejo de Errores y Recuperación.
