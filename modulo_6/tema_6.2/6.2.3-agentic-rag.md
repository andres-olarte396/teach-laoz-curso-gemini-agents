# 6.2.3 Agentic RAG: Retrieval Iterativo

## Objetivo de Aprendizaje

Al finalizar este subtema, ser√°s capaz de construir agentes RAG que realizan b√∫squedas iterativas y auto-refinan sus queries para encontrar informaci√≥n √≥ptima usando Google Gemini.

## Introducci√≥n

**Agentic RAG** transforma el RAG tradicional (una sola b√∫squeda) en un proceso iterativo donde el agente:

1. Eval√∫a si tiene suficiente informaci√≥n
2. Genera nuevas queries si es necesario
3. Combina informaci√≥n de m√∫ltiples b√∫squedas
4. Decide cu√°ndo tiene suficiente contexto para responder

### RAG Tradicional vs Agentic RAG

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RAG TRADICIONAL                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Query ‚îÄ‚îÄ‚ñ∂ [B√∫squeda] ‚îÄ‚îÄ‚ñ∂ Top-K docs ‚îÄ‚îÄ‚ñ∂ [Generaci√≥n] ‚îÄ‚îÄ‚ñ∂ Resp ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚Ä¢ Una sola b√∫squeda                                            ‚îÇ
‚îÇ  ‚Ä¢ No eval√∫a calidad del contexto                               ‚îÇ
‚îÇ  ‚Ä¢ Puede fallar si la query inicial es pobre                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AGENTIC RAG                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Query ‚îÄ‚îÄ‚ñ∂ [Analizar] ‚îÄ‚îÄ‚ñ∂ [B√∫squeda 1] ‚îÄ‚îÄ‚ñ∂ [Evaluar]           ‚îÇ
‚îÇ                                                ‚îÇ                 ‚îÇ
‚îÇ                                                ‚ñº                 ‚îÇ
‚îÇ                                     ¬øSuficiente?                ‚îÇ
‚îÇ                                    /           \                ‚îÇ
‚îÇ                                  NO            S√ç               ‚îÇ
‚îÇ                                  ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ                                  ‚ñº              ‚ñº                ‚îÇ
‚îÇ                          [Refinar Query]  [Generar]             ‚îÇ
‚îÇ                                  ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ                                  ‚ñº              ‚ñº                ‚îÇ
‚îÇ                          [B√∫squeda 2]     Respuesta             ‚îÇ
‚îÇ                                  ‚îÇ                               ‚îÇ
‚îÇ                                  ‚ñº                               ‚îÇ
‚îÇ                          [Evaluar] ‚îÄ‚îÄ‚ñ∂ ...                      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚Ä¢ M√∫ltiples b√∫squedas si es necesario                         ‚îÇ
‚îÇ  ‚Ä¢ Auto-eval√∫a y refina                                         ‚îÇ
‚îÇ  ‚Ä¢ Mayor probabilidad de √©xito                                  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Implementaci√≥n de Agentic RAG

### Agente RAG con Reflexi√≥n

```python
"""
Agente RAG con capacidad de reflexi√≥n y refinamiento iterativo
"""
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from enum import Enum
import json


class RetrievalDecision(Enum):
    """Decisiones del agente sobre el retrieval."""
    SUFFICIENT = "sufficient"      # Tiene suficiente informaci√≥n
    NEED_MORE = "need_more"        # Necesita m√°s b√∫squedas
    REFINE_QUERY = "refine_query"  # Necesita reformular la query
    CANNOT_ANSWER = "cannot_answer"  # No puede responder


@dataclass
class RetrievalStep:
    """Registro de un paso de retrieval."""
    query: str
    results: List[Dict]
    decision: RetrievalDecision
    reasoning: str
    iteration: int


@dataclass
class AgenticRAGState:
    """Estado del agente RAG."""
    original_query: str
    current_query: str
    retrieved_docs: List[Dict] = field(default_factory=list)
    retrieval_history: List[RetrievalStep] = field(default_factory=list)
    iteration: int = 0
    max_iterations: int = 3
    sufficient_context: bool = False


class AgenticRAGAgent:
    """
    Agente RAG que realiza retrieval iterativo con reflexi√≥n.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,  # Tu implementaci√≥n de vector store
        max_iterations: int = 3,
        min_relevance_score: float = 0.7
    ):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")
        self.vector_store = vector_store
        self.max_iterations = max_iterations
        self.min_relevance_score = min_relevance_score

    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analiza la query para identificar sub-preguntas y t√©rminos clave.
        """
        prompt = f"""Analiza la siguiente pregunta y extrae:
1. La intenci√≥n principal del usuario
2. Sub-preguntas impl√≠citas que necesitan respuesta
3. T√©rminos clave para b√∫squeda
4. Posibles queries alternativas

PREGUNTA: {query}

Responde en JSON:
{{
    "main_intent": "...",
    "sub_questions": ["...", "..."],
    "key_terms": ["...", "..."],
    "alternative_queries": ["...", "..."]
}}"""

        response = self.model.generate_content(prompt)

        try:
            json_match = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return json_match
        except:
            return {
                "main_intent": query,
                "sub_questions": [],
                "key_terms": query.split(),
                "alternative_queries": []
            }

    def _evaluate_context(
        self,
        query: str,
        retrieved_docs: List[Dict]
    ) -> tuple[RetrievalDecision, str, Optional[str]]:
        """
        Eval√∫a si el contexto recuperado es suficiente.

        Returns:
            (decision, reasoning, refined_query)
        """
        if not retrieved_docs:
            return (
                RetrievalDecision.REFINE_QUERY,
                "No se encontraron documentos relevantes",
                None
            )

        context = "\n\n".join([
            f"[Doc {i+1}] {doc['content'][:500]}"
            for i, doc in enumerate(retrieved_docs[:5])
        ])

        prompt = f"""Eval√∫a si el contexto recuperado es suficiente para responder la pregunta.

PREGUNTA: {query}

CONTEXTO RECUPERADO:
{context}

Eval√∫a:
1. ¬øEl contexto contiene informaci√≥n directamente relevante?
2. ¬øHay informaci√≥n faltante cr√≠tica?
3. ¬øSe podr√≠a mejorar la b√∫squeda?

Responde en JSON:
{{
    "decision": "sufficient" | "need_more" | "refine_query" | "cannot_answer",
    "reasoning": "explicaci√≥n breve",
    "refined_query": "nueva query si decision es refine_query, null si no",
    "missing_info": ["info faltante 1", "info faltante 2"]
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return (
                RetrievalDecision(result["decision"]),
                result["reasoning"],
                result.get("refined_query")
            )
        except:
            # Default: asumir suficiente si hay docs
            return (
                RetrievalDecision.SUFFICIENT,
                "Evaluaci√≥n por defecto",
                None
            )

    def _retrieve(
        self,
        query: str,
        top_k: int = 5
    ) -> List[Dict]:
        """Realiza b√∫squeda en el vector store."""
        return self.vector_store.search(query, top_k=top_k)

    def _merge_results(
        self,
        existing: List[Dict],
        new_results: List[Dict]
    ) -> List[Dict]:
        """Combina resultados evitando duplicados."""
        seen_ids = {doc.get('id') for doc in existing}
        merged = existing.copy()

        for doc in new_results:
            if doc.get('id') not in seen_ids:
                merged.append(doc)
                seen_ids.add(doc.get('id'))

        return merged

    def _generate_response(
        self,
        query: str,
        context_docs: List[Dict],
        state: AgenticRAGState
    ) -> str:
        """Genera respuesta final con el contexto acumulado."""
        context = "\n\n".join([
            f"[Fuente {i+1}]\n{doc['content']}"
            for i, doc in enumerate(context_docs[:7])
        ])

        # Incluir historial de b√∫squeda para transparencia
        search_history = "\n".join([
            f"- Iteraci√≥n {step.iteration}: '{step.query}' ({step.decision.value})"
            for step in state.retrieval_history
        ])

        prompt = f"""Responde la pregunta bas√°ndote en el contexto proporcionado.

PREGUNTA ORIGINAL: {state.original_query}

HISTORIAL DE B√öSQUEDA:
{search_history}

CONTEXTO ACUMULADO:
{context}

Instrucciones:
- Responde de forma completa y precisa
- Si hay informaci√≥n contradictoria, menci√≥nalo
- Si algo no est√° en el contexto, ind√≠calo

RESPUESTA:"""

        response = self.model.generate_content(prompt)
        return response.text

    def query(
        self,
        query: str,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Ejecuta el pipeline Agentic RAG completo.

        Args:
            query: Pregunta del usuario
            verbose: Si True, imprime pasos intermedios

        Returns:
            Dict con respuesta, fuentes e historial
        """
        # Inicializar estado
        state = AgenticRAGState(
            original_query=query,
            current_query=query,
            max_iterations=self.max_iterations
        )

        # Analizar query inicial
        analysis = self._analyze_query(query)
        if verbose:
            print(f"üìù An√°lisis de query: {analysis['main_intent']}")
            print(f"   Sub-preguntas: {analysis['sub_questions']}")

        # Loop de retrieval iterativo
        while state.iteration < state.max_iterations:
            state.iteration += 1

            if verbose:
                print(f"\nüîç Iteraci√≥n {state.iteration}: Buscando '{state.current_query}'")

            # Realizar b√∫squeda
            results = self._retrieve(state.current_query)

            if verbose:
                print(f"   Encontrados: {len(results)} documentos")

            # Agregar resultados al estado
            state.retrieved_docs = self._merge_results(
                state.retrieved_docs,
                results
            )

            # Evaluar contexto
            decision, reasoning, refined_query = self._evaluate_context(
                state.original_query,
                state.retrieved_docs
            )

            if verbose:
                print(f"   Decisi√≥n: {decision.value}")
                print(f"   Raz√≥n: {reasoning}")

            # Registrar paso
            state.retrieval_history.append(RetrievalStep(
                query=state.current_query,
                results=results,
                decision=decision,
                reasoning=reasoning,
                iteration=state.iteration
            ))

            # Actuar seg√∫n decisi√≥n
            if decision == RetrievalDecision.SUFFICIENT:
                state.sufficient_context = True
                break
            elif decision == RetrievalDecision.CANNOT_ANSWER:
                break
            elif decision == RetrievalDecision.REFINE_QUERY and refined_query:
                state.current_query = refined_query
            elif decision == RetrievalDecision.NEED_MORE:
                # Probar con query alternativa del an√°lisis
                alternatives = analysis.get("alternative_queries", [])
                if alternatives and state.iteration <= len(alternatives):
                    state.current_query = alternatives[state.iteration - 1]
                else:
                    # Combinar t√©rminos clave
                    state.current_query = " ".join(analysis["key_terms"][:3])

        # Generar respuesta
        if state.retrieved_docs:
            answer = self._generate_response(
                state.original_query,
                state.retrieved_docs,
                state
            )
        else:
            answer = "No pude encontrar informaci√≥n relevante para tu pregunta."

        return {
            "answer": answer,
            "sources": [
                {"id": d.get("id"), "content": d["content"][:200]}
                for d in state.retrieved_docs[:5]
            ],
            "iterations": state.iteration,
            "history": [
                {
                    "iteration": s.iteration,
                    "query": s.query,
                    "decision": s.decision.value,
                    "docs_found": len(s.results)
                }
                for s in state.retrieval_history
            ],
            "sufficient_context": state.sufficient_context
        }
```

### Query Decomposition Agent

```python
"""
Agente que descompone queries complejas en sub-queries
"""


class QueryDecompositionAgent:
    """
    Descompone queries complejas en sub-queries m√°s espec√≠ficas.
    """

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")

    def decompose(self, query: str) -> List[str]:
        """
        Descompone una query compleja en sub-queries.
        """
        prompt = f"""Analiza la siguiente pregunta y descomponla en sub-preguntas
m√°s espec√≠ficas que, al responderse, permitir√≠an responder la pregunta original.

PREGUNTA: {query}

Reglas:
- Cada sub-pregunta debe ser independiente y espec√≠fica
- Las sub-preguntas deben cubrir todos los aspectos de la pregunta original
- M√°ximo 4 sub-preguntas
- Si la pregunta es simple, retorna solo la pregunta original

Responde en JSON:
{{
    "is_complex": true/false,
    "sub_queries": ["query1", "query2", ...],
    "reasoning": "por qu√© se descompuso as√≠"
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            if result.get("is_complex", False):
                return result.get("sub_queries", [query])
            return [query]
        except:
            return [query]

    def synthesize_answers(
        self,
        original_query: str,
        sub_answers: List[Dict[str, str]]
    ) -> str:
        """
        Sintetiza respuestas de sub-queries en una respuesta coherente.
        """
        answers_text = "\n\n".join([
            f"Sub-pregunta: {a['query']}\nRespuesta: {a['answer']}"
            for a in sub_answers
        ])

        prompt = f"""Sintetiza las siguientes respuestas parciales en una respuesta
coherente y completa para la pregunta original.

PREGUNTA ORIGINAL: {original_query}

RESPUESTAS PARCIALES:
{answers_text}

Genera una respuesta unificada que:
- Integre toda la informaci√≥n relevante
- Sea coherente y fluida
- No repita informaci√≥n
- Responda completamente la pregunta original

RESPUESTA SINTETIZADA:"""

        response = self.model.generate_content(prompt)
        return response.text


class DecompositionRAGAgent:
    """
    Agente RAG con descomposici√≥n de queries.
    """

    def __init__(
        self,
        api_key: str,
        vector_store
    ):
        self.decomposer = QueryDecompositionAgent(api_key)
        self.rag_agent = AgenticRAGAgent(api_key, vector_store)

    def query(self, query: str, verbose: bool = False) -> Dict:
        """Ejecuta RAG con descomposici√≥n de query."""

        # Descomponer query
        sub_queries = self.decomposer.decompose(query)

        if verbose:
            print(f"üìù Query descompuesta en {len(sub_queries)} partes:")
            for sq in sub_queries:
                print(f"   - {sq}")

        # Ejecutar RAG para cada sub-query
        sub_answers = []
        all_sources = []

        for i, sq in enumerate(sub_queries):
            if verbose:
                print(f"\nüîç Procesando sub-query {i+1}/{len(sub_queries)}")

            result = self.rag_agent.query(sq, verbose=verbose)

            sub_answers.append({
                "query": sq,
                "answer": result["answer"]
            })
            all_sources.extend(result["sources"])

        # Sintetizar respuesta final
        if len(sub_queries) > 1:
            final_answer = self.decomposer.synthesize_answers(query, sub_answers)
        else:
            final_answer = sub_answers[0]["answer"]

        # Deduplicar fuentes
        seen_ids = set()
        unique_sources = []
        for source in all_sources:
            if source["id"] not in seen_ids:
                unique_sources.append(source)
                seen_ids.add(source["id"])

        return {
            "answer": final_answer,
            "sources": unique_sources,
            "sub_queries": sub_queries,
            "sub_answers": sub_answers
        }
```

## Self-RAG: Retrieval Selectivo

```python
"""
Self-RAG: El agente decide cu√°ndo necesita retrieval
"""


class SelfRAGAgent:
    """
    Agente que decide aut√≥nomamente si necesita hacer retrieval.
    """

    def __init__(
        self,
        api_key: str,
        vector_store
    ):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")
        self.vector_store = vector_store

    def _needs_retrieval(self, query: str) -> tuple[bool, str]:
        """
        Determina si la query necesita informaci√≥n externa.
        """
        prompt = f"""Analiza si la siguiente pregunta requiere b√∫squeda de informaci√≥n externa
o si puede responderse con conocimiento general.

PREGUNTA: {query}

Necesita b√∫squeda externa si:
- Pregunta sobre datos espec√≠ficos (fechas, n√∫meros, nombres)
- Requiere informaci√≥n t√©cnica detallada
- Pregunta sobre pol√≠ticas, procedimientos o documentaci√≥n espec√≠fica
- La respuesta correcta depende de contexto espec√≠fico

NO necesita b√∫squeda si:
- Es una pregunta conceptual general
- Pide una explicaci√≥n de un concepto com√∫n
- Es una pregunta de opini√≥n o preferencia

Responde en JSON:
{{
    "needs_retrieval": true/false,
    "reasoning": "explicaci√≥n breve",
    "confidence": 0.0 a 1.0
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return result.get("needs_retrieval", True), result.get("reasoning", "")
        except:
            return True, "Por defecto, usar retrieval"

    def _critique_response(
        self,
        query: str,
        response: str,
        used_context: bool
    ) -> Dict[str, Any]:
        """
        Auto-eval√∫a la respuesta generada.
        """
        prompt = f"""Eval√∫a cr√≠ticamente la siguiente respuesta.

PREGUNTA: {query}

RESPUESTA:
{response}

Eval√∫a:
1. ¬øLa respuesta es relevante para la pregunta?
2. ¬øLa respuesta est√° bien fundamentada?
3. ¬øHay informaci√≥n que parece incorrecta o inventada?
4. ¬øLa respuesta es completa?

Responde en JSON:
{{
    "is_relevant": true/false,
    "is_supported": true/false,
    "has_hallucinations": true/false,
    "is_complete": true/false,
    "overall_quality": "good" | "acceptable" | "poor",
    "issues": ["issue1", "issue2"]
}}"""

        response_eval = self.model.generate_content(prompt)

        try:
            return json.loads(
                response_eval.text[response_eval.text.find('{'):response_eval.text.rfind('}')+1]
            )
        except:
            return {"overall_quality": "acceptable", "issues": []}

    def query(
        self,
        query: str,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Ejecuta Self-RAG con decisi√≥n de retrieval autom√°tica.
        """
        # Paso 1: Decidir si necesita retrieval
        needs_retrieval, reasoning = self._needs_retrieval(query)

        if verbose:
            print(f"üìä ¬øNecesita retrieval? {needs_retrieval}")
            print(f"   Raz√≥n: {reasoning}")

        context = ""
        sources = []

        # Paso 2: Hacer retrieval si es necesario
        if needs_retrieval:
            results = self.vector_store.search(query, top_k=5)
            if results:
                context = "\n\n".join([
                    f"[Doc {i+1}] {doc['content']}"
                    for i, doc in enumerate(results)
                ])
                sources = results

                if verbose:
                    print(f"üîç Recuperados {len(results)} documentos")

        # Paso 3: Generar respuesta
        if context:
            prompt = f"""Bas√°ndote en el contexto, responde la pregunta.

CONTEXTO:
{context}

PREGUNTA: {query}

RESPUESTA:"""
        else:
            prompt = f"""Responde la siguiente pregunta usando tu conocimiento general.

PREGUNTA: {query}

RESPUESTA:"""

        response = self.model.generate_content(prompt)
        answer = response.text

        # Paso 4: Auto-cr√≠tica
        critique = self._critique_response(query, answer, bool(context))

        if verbose:
            print(f"üìù Auto-evaluaci√≥n: {critique['overall_quality']}")
            if critique.get('issues'):
                print(f"   Problemas: {critique['issues']}")

        # Paso 5: Reintentar si la calidad es pobre
        if critique["overall_quality"] == "poor" and not needs_retrieval:
            if verbose:
                print("üîÑ Calidad pobre, reintentando con retrieval...")

            # Forzar retrieval
            results = self.vector_store.search(query, top_k=5)
            if results:
                context = "\n\n".join([
                    f"[Doc {i+1}] {doc['content']}"
                    for i, doc in enumerate(results)
                ])
                sources = results

                prompt = f"""Bas√°ndote en el contexto, responde la pregunta.

CONTEXTO:
{context}

PREGUNTA: {query}

RESPUESTA:"""

                response = self.model.generate_content(prompt)
                answer = response.text
                critique = self._critique_response(query, answer, True)

        return {
            "answer": answer,
            "sources": [{"id": s.get("id"), "content": s["content"][:200]} for s in sources],
            "used_retrieval": bool(sources),
            "quality_assessment": critique
        }
```

## Corrective RAG (CRAG)

```python
"""
Corrective RAG: Corrige y mejora resultados de retrieval
"""


class CorrectiveRAGAgent:
    """
    Implementaci√≥n de CRAG que eval√∫a y corrige el retrieval.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,
        web_search_tool=None  # Opcional: b√∫squeda web como fallback
    ):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")
        self.vector_store = vector_store
        self.web_search = web_search_tool

    def _grade_documents(
        self,
        query: str,
        documents: List[Dict]
    ) -> List[Dict]:
        """
        Califica documentos como relevantes o irrelevantes.
        """
        graded_docs = []

        for doc in documents:
            prompt = f"""Determina si el documento es relevante para responder la pregunta.

PREGUNTA: {query}

DOCUMENTO:
{doc['content'][:1000]}

El documento es relevante si contiene informaci√≥n que ayuda a responder la pregunta.

Responde SOLO: "relevant" o "irrelevant"
"""
            response = self.model.generate_content(prompt)
            is_relevant = "relevant" in response.text.lower()

            graded_docs.append({
                **doc,
                "is_relevant": is_relevant,
                "grade": "relevant" if is_relevant else "irrelevant"
            })

        return graded_docs

    def _transform_query(self, query: str) -> str:
        """
        Transforma la query para mejor retrieval.
        """
        prompt = f"""La siguiente query no encontr√≥ buenos resultados.
Reescr√≠bela de forma m√°s espec√≠fica y con diferentes t√©rminos.

QUERY ORIGINAL: {query}

Proporciona una query alternativa que busque la misma informaci√≥n
pero con t√©rminos diferentes o m√°s espec√≠ficos.

QUERY MEJORADA:"""

        response = self.model.generate_content(prompt)
        return response.text.strip()

    def _web_search_fallback(self, query: str) -> List[Dict]:
        """
        B√∫squeda web como fallback si el vector store falla.
        """
        if self.web_search is None:
            return []

        # Implementar seg√∫n tu herramienta de b√∫squeda web
        return self.web_search.search(query)

    def query(
        self,
        query: str,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Ejecuta Corrective RAG.
        """
        # Paso 1: Retrieval inicial
        documents = self.vector_store.search(query, top_k=5)

        if verbose:
            print(f"üîç Retrieval inicial: {len(documents)} documentos")

        # Paso 2: Calificar documentos
        graded_docs = self._grade_documents(query, documents)
        relevant_docs = [d for d in graded_docs if d["is_relevant"]]
        irrelevant_count = len(graded_docs) - len(relevant_docs)

        if verbose:
            print(f"üìä Relevantes: {len(relevant_docs)}, Irrelevantes: {irrelevant_count}")

        # Paso 3: Decidir acci√≥n correctiva
        action = "generate"  # Por defecto, generar con lo que hay

        if len(relevant_docs) == 0:
            action = "web_search"
        elif len(relevant_docs) < len(graded_docs) / 2:
            action = "transform_query"

        if verbose:
            print(f"üîß Acci√≥n: {action}")

        # Paso 4: Ejecutar acci√≥n correctiva
        final_docs = relevant_docs

        if action == "transform_query":
            # Intentar con query transformada
            new_query = self._transform_query(query)
            if verbose:
                print(f"üîÑ Query transformada: {new_query}")

            new_docs = self.vector_store.search(new_query, top_k=5)
            new_graded = self._grade_documents(query, new_docs)
            new_relevant = [d for d in new_graded if d["is_relevant"]]

            # Combinar resultados
            seen_ids = {d.get('id') for d in final_docs}
            for doc in new_relevant:
                if doc.get('id') not in seen_ids:
                    final_docs.append(doc)

        elif action == "web_search":
            # Fallback a b√∫squeda web
            if verbose:
                print("üåê Usando b√∫squeda web como fallback")

            web_docs = self._web_search_fallback(query)
            web_graded = self._grade_documents(query, web_docs)
            final_docs.extend([d for d in web_graded if d["is_relevant"]])

        # Paso 5: Generar respuesta
        if final_docs:
            context = "\n\n".join([
                f"[Fuente {i+1}]\n{doc['content']}"
                for i, doc in enumerate(final_docs[:5])
            ])

            prompt = f"""Responde la pregunta bas√°ndote en el contexto.

CONTEXTO:
{context}

PREGUNTA: {query}

RESPUESTA:"""

            response = self.model.generate_content(prompt)
            answer = response.text
        else:
            answer = "No pude encontrar informaci√≥n suficiente para responder esta pregunta."

        return {
            "answer": answer,
            "sources": [
                {"id": d.get("id"), "content": d["content"][:200], "grade": d.get("grade")}
                for d in final_docs
            ],
            "action_taken": action,
            "stats": {
                "initial_docs": len(documents),
                "relevant_docs": len(relevant_docs),
                "final_docs": len(final_docs)
            }
        }
```

## Ejemplo Completo: Sistema Multi-Estrategia

```python
"""
Sistema RAG que combina m√∫ltiples estrategias
"""


class MultiStrategyRAG:
    """
    Sistema RAG que selecciona la mejor estrategia seg√∫n la query.
    """

    def __init__(self, api_key: str, vector_store):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")

        # Inicializar estrategias
        self.simple_rag = AgenticRAGAgent(api_key, vector_store, max_iterations=1)
        self.iterative_rag = AgenticRAGAgent(api_key, vector_store, max_iterations=3)
        self.decomposition_rag = DecompositionRAGAgent(api_key, vector_store)
        self.corrective_rag = CorrectiveRAGAgent(api_key, vector_store)

    def _select_strategy(self, query: str) -> str:
        """
        Selecciona la mejor estrategia para la query.
        """
        prompt = f"""Analiza la query y selecciona la mejor estrategia de b√∫squeda.

QUERY: {query}

Estrategias disponibles:
1. "simple": Query directa, una sola b√∫squeda (para preguntas simples)
2. "iterative": B√∫squeda iterativa con refinamiento (para info dif√≠cil de encontrar)
3. "decomposition": Descomposici√≥n en sub-preguntas (para preguntas complejas)
4. "corrective": Con verificaci√≥n y correcci√≥n (cuando se necesita precisi√≥n)

Selecciona bas√°ndote en:
- Complejidad de la query
- Si tiene m√∫ltiples aspectos
- Si requiere alta precisi√≥n
- Si es probable que necesite refinamiento

Responde SOLO con el nombre de la estrategia: simple, iterative, decomposition, o corrective"""

        response = self.model.generate_content(prompt)
        strategy = response.text.strip().lower()

        if strategy not in ["simple", "iterative", "decomposition", "corrective"]:
            strategy = "iterative"  # Default

        return strategy

    def query(
        self,
        query: str,
        verbose: bool = False,
        force_strategy: str = None
    ) -> Dict[str, Any]:
        """
        Ejecuta RAG con la estrategia √≥ptima.
        """
        # Seleccionar estrategia
        strategy = force_strategy or self._select_strategy(query)

        if verbose:
            print(f"üéØ Estrategia seleccionada: {strategy}")

        # Ejecutar estrategia
        if strategy == "simple":
            result = self.simple_rag.query(query, verbose)
        elif strategy == "iterative":
            result = self.iterative_rag.query(query, verbose)
        elif strategy == "decomposition":
            result = self.decomposition_rag.query(query, verbose)
        elif strategy == "corrective":
            result = self.corrective_rag.query(query, verbose)
        else:
            result = self.iterative_rag.query(query, verbose)

        result["strategy_used"] = strategy
        return result
```

## Ejercicio Pr√°ctico

```python
"""
EJERCICIO: Agente RAG con Memoria de Conversaci√≥n

Implementa un agente RAG que mantiene contexto de conversaci√≥n
y usa el historial para mejorar las b√∫squedas.
"""


class ConversationalAgenticRAG:
    """
    Agente RAG con memoria de conversaci√≥n.

    TODO: Implementar las funcionalidades.
    """

    def __init__(self, api_key: str, vector_store):
        """
        TODO:
        1. Inicializar componentes RAG
        2. Inicializar memoria de conversaci√≥n
        """
        pass

    def _expand_query_with_context(
        self,
        query: str,
        conversation_history: List[Dict]
    ) -> str:
        """
        Expande la query usando el contexto de conversaci√≥n.

        Ejemplo:
        - Historial: [{"user": "H√°blame de Python"}, {"assistant": "Python es..."}]
        - Query actual: "¬øY sus ventajas?"
        - Query expandida: "¬øCu√°les son las ventajas de Python?"

        TODO:
        1. Analizar la query actual
        2. Identificar referencias impl√≠citas
        3. Expandir con contexto del historial
        """
        pass

    def _should_search_new(
        self,
        query: str,
        conversation_history: List[Dict]
    ) -> bool:
        """
        Decide si hacer nueva b√∫squeda o usar contexto existente.

        TODO:
        1. Analizar si la query pide informaci√≥n nueva
        2. O si es una pregunta de seguimiento sobre lo ya discutido
        """
        pass

    def chat(
        self,
        user_message: str
    ) -> str:
        """
        Procesa un mensaje en el contexto de la conversaci√≥n.

        TODO:
        1. Agregar mensaje al historial
        2. Decidir si necesita nueva b√∫squeda
        3. Expandir query si es necesario
        4. Ejecutar RAG con contexto
        5. Generar respuesta
        6. Actualizar historial con respuesta
        """
        pass

    def reset_conversation(self):
        """Reinicia la conversaci√≥n."""
        pass


def test_conversational_rag():
    """Prueba el RAG conversacional."""

    # TODO: Implementar prueba con conversaci√≥n
    # rag = ConversationalAgenticRAG(api_key="TU_KEY", vector_store=...)
    #
    # # Conversaci√≥n de prueba
    # print(rag.chat("¬øQu√© es machine learning?"))
    # print(rag.chat("¬øCu√°les son sus aplicaciones principales?"))  # Referencia impl√≠cita
    # print(rag.chat("¬øY deep learning en qu√© se diferencia?"))  # Nueva b√∫squeda
    # print(rag.chat("Dame un ejemplo de la segunda aplicaci√≥n que mencionaste"))  # Usa historial


if __name__ == "__main__":
    test_conversational_rag()
```

## Resumen

| Estrategia | Cu√°ndo Usar | Complejidad |
|------------|-------------|-------------|
| RAG Simple | Queries directas | Baja |
| Iterativo | Info dif√≠cil de encontrar | Media |
| Decomposition | Queries multi-aspecto | Alta |
| Self-RAG | Decisi√≥n autom√°tica | Media |
| Corrective | Alta precisi√≥n requerida | Alta |

### Checklist de Implementaci√≥n

- [ ] Implementar evaluaci√≥n de contexto
- [ ] Configurar l√≠mite de iteraciones
- [ ] Agregar logging detallado
- [ ] Implementar fallbacks
- [ ] Monitorear tokens consumidos
- [ ] Evaluar calidad de respuestas

## Siguiente Paso

En el pr√≥ximo tema exploraremos **RAG en Producci√≥n**, con estrategias de chunking, evaluaci√≥n de calidad y actualizaci√≥n de √≠ndices.
