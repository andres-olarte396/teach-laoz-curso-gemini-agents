# 6.1.2 Embeddings con Gemini

## Objetivo de Aprendizaje

Al finalizar este subtema, ser√°s capaz de generar, optimizar y utilizar embeddings de texto con la API de Gemini para construir sistemas de b√∫squeda sem√°ntica y memoria de agentes.

## Introducci√≥n

Los **embeddings** son representaciones vectoriales densas de texto que capturan el significado sem√°ntico. Dos textos con significados similares tendr√°n embeddings cercanos en el espacio vectorial, independientemente de las palabras exactas utilizadas.

### Visualizaci√≥n del Concepto

```
Espacio de Embeddings (simplificado a 2D)

        ‚ñ≤ dimensi√≥n 2
        ‚îÇ
        ‚îÇ    ‚Ä¢ "perro"
        ‚îÇ         ‚Ä¢ "cachorro"
        ‚îÇ              ‚Ä¢ "canino"
        ‚îÇ
        ‚îÇ                           ‚Ä¢ "autom√≥vil"
        ‚îÇ                                ‚Ä¢ "carro"
        ‚îÇ                                     ‚Ä¢ "veh√≠culo"
        ‚îÇ
        ‚îÇ  ‚Ä¢ "feliz"
        ‚îÇ       ‚Ä¢ "contento"
        ‚îÇ            ‚Ä¢ "alegre"
        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ dimensi√≥n 1

Los conceptos relacionados se agrupan en el espacio vectorial
```

## Modelos de Embedding de Gemini

### Modelos Disponibles

| Modelo | Dimensiones | Caso de Uso |
|--------|-------------|-------------|
| `text-embedding-004` | 768 | General, alta calidad |
| `embedding-001` | 768 | Legacy, compatible |

### Task Types

Gemini soporta diferentes **task types** que optimizan los embeddings para casos espec√≠ficos:

```python
"""
Tipos de tareas para embeddings en Gemini
"""
import google.generativeai as genai

# Configurar API
genai.configure(api_key="TU_API_KEY")

# Task types disponibles
TASK_TYPES = {
    "retrieval_document": "Para indexar documentos en una base de conocimiento",
    "retrieval_query": "Para queries de b√∫squeda (buscar en documentos)",
    "semantic_similarity": "Para comparar similitud entre textos",
    "classification": "Para clasificaci√≥n de textos",
    "clustering": "Para agrupar textos similares"
}


def demo_task_types():
    """Demuestra el impacto de diferentes task types."""

    texto = "¬øC√≥mo puedo resetear mi contrase√±a?"

    for task_type, descripcion in TASK_TYPES.items():
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=texto,
                task_type=task_type
            )

            embedding = result['embedding']
            print(f"\n{task_type}:")
            print(f"  Descripci√≥n: {descripcion}")
            print(f"  Dimensiones: {len(embedding)}")
            print(f"  Primeros 5 valores: {embedding[:5]}")

        except Exception as e:
            print(f"\n{task_type}: Error - {e}")


if __name__ == "__main__":
    demo_task_types()
```

## Generaci√≥n de Embeddings

### Embedding B√°sico

```python
"""
Generaci√≥n b√°sica de embeddings con Gemini
"""
import google.generativeai as genai
from typing import List, Union


def get_embedding(
    text: str,
    model: str = "models/text-embedding-004",
    task_type: str = "retrieval_document"
) -> List[float]:
    """
    Genera embedding para un texto.

    Args:
        text: Texto a vectorizar
        model: Modelo de embedding
        task_type: Tipo de tarea (afecta la optimizaci√≥n)

    Returns:
        Lista de floats representando el embedding
    """
    result = genai.embed_content(
        model=model,
        content=text,
        task_type=task_type
    )
    return result['embedding']


def get_embeddings_batch(
    texts: List[str],
    model: str = "models/text-embedding-004",
    task_type: str = "retrieval_document"
) -> List[List[float]]:
    """
    Genera embeddings para m√∫ltiples textos (m√°s eficiente).

    Args:
        texts: Lista de textos
        model: Modelo de embedding
        task_type: Tipo de tarea

    Returns:
        Lista de embeddings
    """
    result = genai.embed_content(
        model=model,
        content=texts,  # Pasar lista directamente
        task_type=task_type
    )
    return result['embedding']


# Ejemplo de uso
def demo_embeddings():
    genai.configure(api_key="TU_API_KEY")

    # Embedding individual
    texto = "El aprendizaje autom√°tico transforma datos en conocimiento."
    embedding = get_embedding(texto)
    print(f"Texto: {texto}")
    print(f"Embedding shape: {len(embedding)} dimensiones")
    print(f"Muestra: {embedding[:5]}...")

    # Embeddings en batch
    textos = [
        "Python es un lenguaje de programaci√≥n",
        "JavaScript se usa para desarrollo web",
        "La inteligencia artificial avanza r√°pidamente"
    ]

    embeddings = get_embeddings_batch(textos)
    print(f"\nBatch de {len(embeddings)} embeddings generados")
    for i, (texto, emb) in enumerate(zip(textos, embeddings)):
        print(f"  {i+1}. {texto[:30]}... -> {len(emb)} dims")


if __name__ == "__main__":
    demo_embeddings()
```

### Embedding con T√≠tulo (para documentos)

```python
"""
Embeddings con t√≠tulo para mejorar relevancia de documentos
"""
import google.generativeai as genai


def get_document_embedding(
    content: str,
    title: str = None,
    model: str = "models/text-embedding-004"
) -> List[float]:
    """
    Genera embedding de documento con t√≠tulo opcional.

    El t√≠tulo ayuda a contextualizar el contenido y
    mejora la relevancia en b√∫squedas.
    """
    result = genai.embed_content(
        model=model,
        content=content,
        task_type="retrieval_document",
        title=title  # T√≠tulo opcional para contexto
    )
    return result['embedding']


# Ejemplo
def demo_document_embedding():
    genai.configure(api_key="TU_API_KEY")

    documento = """
    Para crear una cuenta nueva, haga clic en el bot√≥n "Registrarse"
    en la esquina superior derecha. Complete el formulario con su
    email y contrase√±a. Recibir√° un email de confirmaci√≥n.
    """

    # Sin t√≠tulo
    emb_sin_titulo = get_document_embedding(documento)

    # Con t√≠tulo
    emb_con_titulo = get_document_embedding(
        documento,
        title="Gu√≠a de Registro de Usuario"
    )

    print(f"Embedding sin t√≠tulo: {len(emb_sin_titulo)} dims")
    print(f"Embedding con t√≠tulo: {len(emb_con_titulo)} dims")

    # Los embeddings ser√°n diferentes debido al contexto del t√≠tulo


if __name__ == "__main__":
    demo_document_embedding()
```

## C√°lculo de Similitud

### M√©tricas de Similitud

```python
"""
M√©tricas de similitud para embeddings
"""
import numpy as np
from typing import List


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Similitud coseno: mide el √°ngulo entre vectores.
    Rango: -1 (opuestos) a 1 (id√©nticos)
    """
    v1 = np.array(vec1)
    v2 = np.array(vec2)

    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)

    if norm1 == 0 or norm2 == 0:
        return 0.0

    return dot_product / (norm1 * norm2)


def dot_product_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Producto punto: similitud no normalizada.
    √ötil cuando los embeddings ya est√°n normalizados.
    """
    return np.dot(vec1, vec2)


def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:
    """
    Distancia euclidiana: distancia geom√©trica.
    Menor = m√°s similar.
    """
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    return np.linalg.norm(v1 - v2)


def manhattan_distance(vec1: List[float], vec2: List[float]) -> float:
    """
    Distancia Manhattan (L1): suma de diferencias absolutas.
    """
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    return np.sum(np.abs(v1 - v2))


# Comparaci√≥n de m√©tricas
def compare_metrics():
    genai.configure(api_key="TU_API_KEY")

    textos = [
        "El gato duerme en el sof√°",
        "El felino descansa en el sill√≥n",
        "Python es un lenguaje de programaci√≥n",
        "La econom√≠a global enfrenta desaf√≠os"
    ]

    # Generar embeddings
    embeddings = [get_embedding(t) for t in textos]

    print("Comparaci√≥n de m√©tricas de similitud:")
    print("="*60)

    base_text = textos[0]
    base_emb = embeddings[0]

    for i, (texto, emb) in enumerate(zip(textos, embeddings)):
        if i == 0:
            continue

        cos_sim = cosine_similarity(base_emb, emb)
        dot_sim = dot_product_similarity(base_emb, emb)
        euc_dist = euclidean_distance(base_emb, emb)

        print(f"\n'{base_text}' vs '{texto}':")
        print(f"  Coseno: {cos_sim:.4f}")
        print(f"  Dot Product: {dot_sim:.4f}")
        print(f"  Euclidiana: {euc_dist:.4f}")
```

### B√∫squeda Sem√°ntica

```python
"""
Sistema de b√∫squeda sem√°ntica con embeddings
"""
import google.generativeai as genai
from dataclasses import dataclass
from typing import List, Tuple


@dataclass
class SemanticSearchResult:
    """Resultado de b√∫squeda sem√°ntica."""
    text: str
    score: float
    index: int


class SemanticSearchEngine:
    """Motor de b√∫squeda sem√°ntica simple."""

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.documents: List[str] = []
        self.embeddings: List[List[float]] = []

    def add_documents(self, documents: List[str]):
        """Agrega documentos al √≠ndice."""
        # Generar embeddings para documentos
        new_embeddings = get_embeddings_batch(
            documents,
            task_type="retrieval_document"
        )

        self.documents.extend(documents)
        self.embeddings.extend(new_embeddings)

        print(f"Agregados {len(documents)} documentos. Total: {len(self.documents)}")

    def search(
        self,
        query: str,
        top_k: int = 5,
        threshold: float = 0.0
    ) -> List[SemanticSearchResult]:
        """
        Busca documentos sem√°nticamente similares a la query.
        """
        # Embedding de la query (con task_type diferente)
        query_embedding = get_embedding(
            query,
            task_type="retrieval_query"  # Optimizado para queries
        )

        # Calcular similitudes
        similarities = []
        for i, doc_emb in enumerate(self.embeddings):
            score = cosine_similarity(query_embedding, doc_emb)
            if score >= threshold:
                similarities.append((i, score))

        # Ordenar por score descendente
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Retornar top-k resultados
        results = []
        for idx, score in similarities[:top_k]:
            results.append(SemanticSearchResult(
                text=self.documents[idx],
                score=score,
                index=idx
            ))

        return results


# Demostraci√≥n
def demo_semantic_search():
    engine = SemanticSearchEngine(api_key="TU_API_KEY")

    # Base de conocimiento
    documentos = [
        "Python es un lenguaje de programaci√≥n interpretado de alto nivel",
        "JavaScript es el lenguaje principal para desarrollo web frontend",
        "Los gatos son animales dom√©sticos muy independientes",
        "El machine learning es una rama de la inteligencia artificial",
        "Los perros son conocidos por su lealtad hacia sus due√±os",
        "TensorFlow es una biblioteca de c√≥digo abierto para ML",
        "React es una biblioteca de JavaScript para construir interfaces",
        "Los embeddings convierten texto en vectores num√©ricos",
        "Docker permite empaquetar aplicaciones en contenedores",
        "Las redes neuronales imitan el funcionamiento del cerebro"
    ]

    # Indexar documentos
    engine.add_documents(documentos)

    # Realizar b√∫squedas
    queries = [
        "lenguajes de programaci√≥n web",
        "mascotas y animales de compa√±√≠a",
        "aprendizaje profundo y redes neuronales",
        "representaci√≥n vectorial de texto"
    ]

    print("\n" + "="*60)
    print("B√öSQUEDA SEM√ÅNTICA")
    print("="*60)

    for query in queries:
        print(f"\nüîç Query: '{query}'")
        print("-" * 40)

        results = engine.search(query, top_k=3)

        for i, result in enumerate(results, 1):
            print(f"  {i}. [{result.score:.3f}] {result.text}")


if __name__ == "__main__":
    demo_semantic_search()
```

## Optimizaci√≥n de Embeddings

### Preprocesamiento de Texto

```python
"""
Preprocesamiento de texto para mejorar embeddings
"""
import re
from typing import List


class TextPreprocessor:
    """Preprocesador de texto para embeddings."""

    def __init__(
        self,
        lowercase: bool = False,
        remove_urls: bool = True,
        remove_emails: bool = True,
        normalize_whitespace: bool = True,
        max_length: int = None
    ):
        self.lowercase = lowercase
        self.remove_urls = remove_urls
        self.remove_emails = remove_emails
        self.normalize_whitespace = normalize_whitespace
        self.max_length = max_length

    def process(self, text: str) -> str:
        """Preprocesa un texto."""
        if self.remove_urls:
            text = re.sub(
                r'https?://\S+|www\.\S+',
                '[URL]',
                text
            )

        if self.remove_emails:
            text = re.sub(
                r'\S+@\S+\.\S+',
                '[EMAIL]',
                text
            )

        if self.normalize_whitespace:
            text = re.sub(r'\s+', ' ', text).strip()

        if self.lowercase:
            text = text.lower()

        if self.max_length and len(text) > self.max_length:
            text = text[:self.max_length]

        return text

    def process_batch(self, texts: List[str]) -> List[str]:
        """Preprocesa m√∫ltiples textos."""
        return [self.process(t) for t in texts]


# Uso
def demo_preprocessing():
    preprocessor = TextPreprocessor(
        remove_urls=True,
        remove_emails=True,
        normalize_whitespace=True,
        max_length=500
    )

    texto_sucio = """
    Visita nuestra web en https://ejemplo.com para m√°s info.
    Contacto: soporte@empresa.com

    Tenemos   muchos    espacios    extras   aqu√≠.
    """

    texto_limpio = preprocessor.process(texto_sucio)
    print(f"Original:\n{texto_sucio}")
    print(f"\nPreprocesado:\n{texto_limpio}")
```

### Chunking Inteligente para Embeddings

```python
"""
Estrategias de chunking para embeddings √≥ptimos
"""
from typing import List, Dict, Any
from dataclasses import dataclass


@dataclass
class TextChunk:
    """Chunk de texto con metadata."""
    content: str
    metadata: Dict[str, Any]
    start_char: int
    end_char: int


class SmartChunker:
    """
    Chunker inteligente que respeta l√≠mites sem√°nticos.
    """

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        respect_sentences: bool = True,
        respect_paragraphs: bool = True
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_sentences = respect_sentences
        self.respect_paragraphs = respect_paragraphs

    def chunk_text(self, text: str) -> List[TextChunk]:
        """Divide texto en chunks inteligentes."""
        chunks = []

        if self.respect_paragraphs:
            # Dividir por p√°rrafos primero
            paragraphs = text.split('\n\n')
            current_chunk = ""
            current_start = 0

            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue

                if len(current_chunk) + len(para) <= self.chunk_size:
                    current_chunk += ("\n\n" if current_chunk else "") + para
                else:
                    # Guardar chunk actual
                    if current_chunk:
                        chunks.append(TextChunk(
                            content=current_chunk,
                            metadata={"type": "paragraph_based"},
                            start_char=current_start,
                            end_char=current_start + len(current_chunk)
                        ))

                    # Iniciar nuevo chunk
                    current_start = text.find(para)
                    current_chunk = para

            # √öltimo chunk
            if current_chunk:
                chunks.append(TextChunk(
                    content=current_chunk,
                    metadata={"type": "paragraph_based"},
                    start_char=current_start,
                    end_char=current_start + len(current_chunk)
                ))

        else:
            # Chunking simple por tama√±o
            for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
                end = min(i + self.chunk_size, len(text))
                chunk_text = text[i:end]

                # Ajustar al final de oraci√≥n si es posible
                if self.respect_sentences and end < len(text):
                    last_period = chunk_text.rfind('.')
                    if last_period > self.chunk_size * 0.5:
                        chunk_text = chunk_text[:last_period + 1]
                        end = i + last_period + 1

                chunks.append(TextChunk(
                    content=chunk_text.strip(),
                    metadata={"type": "size_based"},
                    start_char=i,
                    end_char=end
                ))

        return chunks


# Demostraci√≥n
def demo_smart_chunking():
    texto_largo = """
    # Introducci√≥n a Python

    Python es un lenguaje de programaci√≥n de alto nivel, interpretado
    y de prop√≥sito general. Fue creado por Guido van Rossum y lanzado
    en 1991. Python enfatiza la legibilidad del c√≥digo.

    ## Caracter√≠sticas Principales

    Python tiene varias caracter√≠sticas que lo hacen popular. Primero,
    su sintaxis clara y concisa facilita el aprendizaje. Segundo,
    cuenta con una amplia biblioteca est√°ndar. Tercero, tiene una
    comunidad activa que contribuye con miles de paquetes.

    ## Casos de Uso

    Python se usa en muchas √°reas. En ciencia de datos, bibliotecas
    como NumPy y Pandas son fundamentales. En desarrollo web,
    frameworks como Django y Flask son muy populares. En inteligencia
    artificial, TensorFlow y PyTorch dominan el campo.

    ## Conclusi√≥n

    Python sigue creciendo en popularidad. Es una excelente opci√≥n
    tanto para principiantes como para desarrolladores experimentados.
    """

    chunker = SmartChunker(
        chunk_size=300,
        chunk_overlap=50,
        respect_paragraphs=True
    )

    chunks = chunker.chunk_text(texto_largo)

    print(f"Texto dividido en {len(chunks)} chunks:\n")

    for i, chunk in enumerate(chunks, 1):
        print(f"--- Chunk {i} ({len(chunk.content)} chars) ---")
        print(chunk.content[:100] + "...")
        print()
```

## Cach√© de Embeddings

```python
"""
Sistema de cach√© para embeddings
"""
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Optional
import time


class EmbeddingCache:
    """
    Cach√© de embeddings para evitar regeneraci√≥n.
    """

    def __init__(self, cache_dir: str = ".embedding_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache: Dict[str, List[float]] = {}
        self.stats = {"hits": 0, "misses": 0}

    def _get_key(self, text: str, model: str, task_type: str) -> str:
        """Genera clave √∫nica para el cach√©."""
        content = f"{model}:{task_type}:{text}"
        return hashlib.sha256(content.encode()).hexdigest()

    def get(
        self,
        text: str,
        model: str = "text-embedding-004",
        task_type: str = "retrieval_document"
    ) -> Optional[List[float]]:
        """Obtiene embedding del cach√© si existe."""
        key = self._get_key(text, model, task_type)

        # Buscar en memoria primero
        if key in self.memory_cache:
            self.stats["hits"] += 1
            return self.memory_cache[key]

        # Buscar en disco
        cache_file = self.cache_dir / f"{key}.json"
        if cache_file.exists():
            try:
                data = json.loads(cache_file.read_text())
                embedding = data["embedding"]
                self.memory_cache[key] = embedding  # Cargar a memoria
                self.stats["hits"] += 1
                return embedding
            except Exception:
                pass

        self.stats["misses"] += 1
        return None

    def set(
        self,
        text: str,
        embedding: List[float],
        model: str = "text-embedding-004",
        task_type: str = "retrieval_document"
    ):
        """Guarda embedding en cach√©."""
        key = self._get_key(text, model, task_type)

        # Guardar en memoria
        self.memory_cache[key] = embedding

        # Guardar en disco
        cache_file = self.cache_dir / f"{key}.json"
        data = {
            "text_preview": text[:100],
            "model": model,
            "task_type": task_type,
            "embedding": embedding,
            "cached_at": time.time()
        }
        cache_file.write_text(json.dumps(data))

    def get_or_compute(
        self,
        text: str,
        model: str = "text-embedding-004",
        task_type: str = "retrieval_document"
    ) -> List[float]:
        """Obtiene del cach√© o genera nuevo embedding."""
        # Intentar obtener del cach√©
        cached = self.get(text, model, task_type)
        if cached is not None:
            return cached

        # Generar nuevo embedding
        result = genai.embed_content(
            model=f"models/{model}",
            content=text,
            task_type=task_type
        )
        embedding = result['embedding']

        # Guardar en cach√©
        self.set(text, embedding, model, task_type)

        return embedding

    def clear(self):
        """Limpia todo el cach√©."""
        self.memory_cache.clear()
        for f in self.cache_dir.glob("*.json"):
            f.unlink()

    def get_stats(self) -> Dict:
        """Retorna estad√≠sticas del cach√©."""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total if total > 0 else 0

        return {
            **self.stats,
            "total_requests": total,
            "hit_rate": f"{hit_rate:.2%}",
            "memory_entries": len(self.memory_cache),
            "disk_entries": len(list(self.cache_dir.glob("*.json")))
        }


# Uso con cach√©
def demo_cache():
    genai.configure(api_key="TU_API_KEY")

    cache = EmbeddingCache()

    textos = [
        "Python es genial",
        "JavaScript tambi√©n es √∫til",
        "Python es genial",  # Repetido - deber√≠a usar cach√©
        "Me gusta programar",
        "Python es genial"   # Repetido de nuevo
    ]

    print("Generando embeddings con cach√©:\n")

    for texto in textos:
        start = time.time()
        embedding = cache.get_or_compute(texto)
        elapsed = (time.time() - start) * 1000

        print(f"'{texto}' -> {elapsed:.1f}ms")

    print(f"\nEstad√≠sticas del cach√©: {cache.get_stats()}")


if __name__ == "__main__":
    demo_cache()
```

## Comparaci√≥n y An√°lisis de Embeddings

```python
"""
Herramientas de an√°lisis de embeddings
"""
import numpy as np
from typing import List, Dict
import google.generativeai as genai


class EmbeddingAnalyzer:
    """Analizador de embeddings para debugging y optimizaci√≥n."""

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)

    def analyze_similarity_matrix(
        self,
        texts: List[str],
        task_type: str = "semantic_similarity"
    ) -> np.ndarray:
        """
        Genera matriz de similitud entre todos los textos.
        """
        # Generar embeddings
        embeddings = []
        for text in texts:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type=task_type
            )
            embeddings.append(result['embedding'])

        embeddings = np.array(embeddings)

        # Calcular matriz de similitud
        n = len(texts)
        similarity_matrix = np.zeros((n, n))

        for i in range(n):
            for j in range(n):
                similarity_matrix[i, j] = cosine_similarity(
                    embeddings[i],
                    embeddings[j]
                )

        return similarity_matrix

    def find_outliers(
        self,
        texts: List[str],
        threshold: float = 0.5
    ) -> List[Dict]:
        """
        Encuentra textos que son outliers (poco similares al resto).
        """
        matrix = self.analyze_similarity_matrix(texts)

        outliers = []
        for i, text in enumerate(texts):
            # Promedio de similitud con otros (excluyendo a s√≠ mismo)
            similarities = [matrix[i, j] for j in range(len(texts)) if j != i]
            avg_similarity = np.mean(similarities)

            if avg_similarity < threshold:
                outliers.append({
                    "index": i,
                    "text": text,
                    "avg_similarity": avg_similarity
                })

        return sorted(outliers, key=lambda x: x["avg_similarity"])

    def cluster_by_similarity(
        self,
        texts: List[str],
        threshold: float = 0.8
    ) -> List[List[int]]:
        """
        Agrupa textos por similitud (clustering simple).
        """
        matrix = self.analyze_similarity_matrix(texts)

        n = len(texts)
        assigned = [False] * n
        clusters = []

        for i in range(n):
            if assigned[i]:
                continue

            cluster = [i]
            assigned[i] = True

            for j in range(i + 1, n):
                if not assigned[j] and matrix[i, j] >= threshold:
                    cluster.append(j)
                    assigned[j] = True

            clusters.append(cluster)

        return clusters


# Demostraci√≥n
def demo_analyzer():
    analyzer = EmbeddingAnalyzer(api_key="TU_API_KEY")

    textos = [
        "Python es un lenguaje de programaci√≥n",
        "Python se usa para desarrollo de software",
        "El gato duerme en el sof√°",
        "JavaScript es popular para web",
        "El felino descansa tranquilo",
        "Programar en Python es divertido"
    ]

    print("Matriz de Similitud:")
    print("="*60)

    matrix = analyzer.analyze_similarity_matrix(textos)

    # Imprimir matriz
    for i, text in enumerate(textos):
        print(f"\n{i}: {text[:30]}...")
        scores = [f"{matrix[i,j]:.2f}" for j in range(len(textos))]
        print(f"   Similitudes: {scores}")

    # Encontrar clusters
    print("\n\nClusters (threshold=0.7):")
    print("="*60)

    clusters = analyzer.cluster_by_similarity(textos, threshold=0.7)
    for i, cluster in enumerate(clusters):
        print(f"\nCluster {i+1}:")
        for idx in cluster:
            print(f"  - {textos[idx]}")


if __name__ == "__main__":
    demo_analyzer()
```

## Ejercicio Pr√°ctico

### Sistema de Detecci√≥n de Duplicados Sem√°nticos

```python
"""
EJERCICIO: Detector de Duplicados Sem√°nticos

Construye un sistema que detecte contenido duplicado o muy similar
en una colecci√≥n de documentos usando embeddings.
"""


class SemanticDuplicateDetector:
    """
    Detecta duplicados sem√°nticos en colecciones de texto.
    """

    def __init__(self, api_key: str, similarity_threshold: float = 0.85):
        """
        TODO:
        1. Configurar genai
        2. Inicializar threshold
        3. Crear estructuras para almacenar textos y embeddings
        """
        pass

    def add_text(self, text: str, text_id: str = None) -> Dict:
        """
        Agrega un texto y verifica si es duplicado.

        TODO:
        1. Generar embedding del nuevo texto
        2. Comparar con todos los textos existentes
        3. Si hay coincidencia >= threshold, marcar como duplicado
        4. Almacenar el texto y su embedding
        5. Retornar resultado con info de duplicados

        Returns:
            {
                "text_id": "...",
                "is_duplicate": True/False,
                "similar_to": ["id1", "id2"],  # Si es duplicado
                "max_similarity": 0.92
            }
        """
        pass

    def find_all_duplicates(self) -> List[tuple]:
        """
        Encuentra todos los pares de duplicados en la colecci√≥n.

        TODO:
        1. Calcular matriz de similitud
        2. Identificar pares con similitud >= threshold
        3. Retornar lista de pares

        Returns:
            [(id1, id2, similarity), ...]
        """
        pass

    def deduplicate(self) -> List[str]:
        """
        Retorna lista de IDs √∫nicos (eliminando duplicados).

        TODO:
        1. Identificar grupos de duplicados
        2. Mantener solo uno de cada grupo
        3. Retornar IDs de textos √∫nicos
        """
        pass


def test_duplicate_detector():
    """Prueba el detector de duplicados."""

    textos = [
        ("doc1", "Python es un lenguaje de programaci√≥n vers√°til"),
        ("doc2", "Python es un lenguaje vers√°til para programar"),  # Similar a doc1
        ("doc3", "JavaScript se usa para desarrollo web"),
        ("doc4", "El desarrollo web utiliza JavaScript"),  # Similar a doc3
        ("doc5", "Los gatos son mascotas independientes"),
        ("doc6", "Machine learning transforma la industria"),
        ("doc7", "La industria se transforma con machine learning"),  # Similar a doc6
    ]

    # TODO: Crear detector y probar
    # detector = SemanticDuplicateDetector(
    #     api_key="TU_API_KEY",
    #     similarity_threshold=0.85
    # )
    #
    # for text_id, text in textos:
    #     result = detector.add_text(text, text_id)
    #     status = "DUPLICADO" if result["is_duplicate"] else "√öNICO"
    #     print(f"{text_id}: {status}")
    #     if result["is_duplicate"]:
    #         print(f"   Similar a: {result['similar_to']}")
    #         print(f"   Similitud: {result['max_similarity']:.2%}")
    #
    # print("\nTextos √∫nicos despu√©s de deduplicaci√≥n:")
    # unicos = detector.deduplicate()
    # for uid in unicos:
    #     print(f"  - {uid}")


if __name__ == "__main__":
    test_duplicate_detector()
```

## Mejores Pr√°cticas

### Checklist de Embeddings

- [ ] Usar `task_type` correcto seg√∫n el caso de uso
- [ ] Preprocesar texto antes de generar embeddings
- [ ] Implementar cach√© para evitar regeneraci√≥n
- [ ] Usar batch processing para m√∫ltiples textos
- [ ] Monitorear dimensionalidad y uso de memoria
- [ ] Validar similitudes con casos de prueba conocidos

### Errores Comunes

| Error | Causa | Soluci√≥n |
|-------|-------|----------|
| Baja relevancia en b√∫squeda | Task type incorrecto | Usar `retrieval_query` para queries |
| Embeddings lentos | Sin batch processing | Usar `embed_content` con lista |
| Alta latencia | Sin cach√© | Implementar cach√© de embeddings |
| Textos largos fallan | L√≠mite de tokens | Implementar chunking |

## Resumen

En este subtema aprendiste:

1. **Modelos de Embedding**: text-embedding-004 y sus task types
2. **Generaci√≥n**: Individual y batch, con y sin t√≠tulo
3. **Similitud**: Coseno, dot product, distancias
4. **Optimizaci√≥n**: Preprocesamiento, chunking, cach√©
5. **An√°lisis**: Matrices de similitud, clustering, detecci√≥n de duplicados

## Siguiente Paso

En el pr√≥ximo subtema exploraremos **Vector Databases (Pinecone, Chroma, Weaviate)**, aprendiendo a usar bases de datos vectoriales profesionales para RAG en producci√≥n.
