# 6.1.3 Vector Databases: Pinecone, Chroma y Weaviate

## Objetivo de Aprendizaje

Al finalizar este subtema, ser√°s capaz de integrar bases de datos vectoriales profesionales con agentes de Gemini para construir sistemas RAG escalables y de producci√≥n.

## Introducci√≥n

Las **bases de datos vectoriales** est√°n optimizadas para almacenar, indexar y buscar embeddings de alta dimensionalidad de manera eficiente. A diferencia de un vector store en memoria, estas bases de datos ofrecen persistencia, escalabilidad y b√∫squedas optimizadas mediante algoritmos especializados como HNSW o IVF.

### Comparaci√≥n de Opciones

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              COMPARACI√ìN DE VECTOR DATABASES                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  CHROMA                                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Tipo: Open-source, embebido o servidor                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Ideal para: Desarrollo, prototipos, aplicaciones locales  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Escalabilidad: Media (millones de vectores)               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Setup: pip install chromadb                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  PINECONE                                                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Tipo: Servicio cloud (SaaS)                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Ideal para: Producci√≥n, alta escalabilidad                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Escalabilidad: Alta (billones de vectores)                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Setup: Crear cuenta, obtener API key                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  WEAVIATE                                                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Tipo: Open-source, self-hosted o cloud                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Ideal para: Flexibilidad, GraphQL, m√≥dulos ML             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Escalabilidad: Alta (horizontal scaling)                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Setup: Docker o Weaviate Cloud                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  OTROS                                                          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Milvus: Open-source, alta performance                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Qdrant: Open-source, Rust-based, r√°pido                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ pgvector: Extensi√≥n PostgreSQL                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ FAISS: Biblioteca de Meta (no es DB completa)             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## ChromaDB

ChromaDB es la opci√≥n m√°s simple para comenzar. Funciona como base de datos embebida (como SQLite) o como servidor independiente.

### Instalaci√≥n y Configuraci√≥n

```bash
# Instalaci√≥n b√°sica
pip install chromadb

# Con soporte para servidor
pip install chromadb[server]
```

### Uso B√°sico con Gemini

```python
"""
Integraci√≥n de ChromaDB con Google Gemini
"""
import chromadb
from chromadb.config import Settings
import google.generativeai as genai
from typing import List, Dict, Any, Optional
import uuid


class GeminiChromaDB:
    """
    Integraci√≥n de ChromaDB con embeddings de Gemini.
    """

    def __init__(
        self,
        api_key: str,
        collection_name: str = "default",
        persist_directory: str = "./chroma_db"
    ):
        # Configurar Gemini
        genai.configure(api_key=api_key)

        # Configurar ChromaDB con persistencia
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False
            )
        )

        # Obtener o crear colecci√≥n
        # Usamos funci√≥n de embedding personalizada
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # M√©trica de similitud
        )

        self.embedding_model = "models/text-embedding-004"

    def _get_embedding(self, text: str, task_type: str = "retrieval_document") -> List[float]:
        """Genera embedding usando Gemini."""
        result = genai.embed_content(
            model=self.embedding_model,
            content=text,
            task_type=task_type
        )
        return result['embedding']

    def _get_embeddings_batch(self, texts: List[str], task_type: str = "retrieval_document") -> List[List[float]]:
        """Genera embeddings en batch."""
        result = genai.embed_content(
            model=self.embedding_model,
            content=texts,
            task_type=task_type
        )
        return result['embedding']

    def add_documents(
        self,
        documents: List[str],
        metadatas: List[Dict] = None,
        ids: List[str] = None
    ) -> List[str]:
        """
        Agrega documentos a la colecci√≥n.

        Args:
            documents: Lista de textos
            metadatas: Lista de diccionarios con metadata
            ids: IDs personalizados (opcional)

        Returns:
            Lista de IDs de los documentos agregados
        """
        # Generar IDs si no se proporcionan
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in documents]

        # Generar metadatas vac√≠as si no se proporcionan
        if metadatas is None:
            metadatas = [{} for _ in documents]

        # Generar embeddings
        embeddings = self._get_embeddings_batch(documents)

        # Agregar a ChromaDB
        self.collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

        return ids

    def query(
        self,
        query_text: str,
        n_results: int = 5,
        where: Dict = None,
        where_document: Dict = None
    ) -> Dict[str, Any]:
        """
        Busca documentos similares.

        Args:
            query_text: Texto de b√∫squeda
            n_results: N√∫mero de resultados
            where: Filtro por metadata
            where_document: Filtro por contenido

        Returns:
            Diccionario con resultados
        """
        # Generar embedding de la query
        query_embedding = self._get_embedding(
            query_text,
            task_type="retrieval_query"
        )

        # Realizar b√∫squeda
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where,
            where_document=where_document,
            include=["documents", "metadatas", "distances"]
        )

        return {
            "ids": results["ids"][0],
            "documents": results["documents"][0],
            "metadatas": results["metadatas"][0],
            "distances": results["distances"][0]
        }

    def update_document(
        self,
        doc_id: str,
        document: str = None,
        metadata: Dict = None
    ):
        """Actualiza un documento existente."""
        update_kwargs = {"ids": [doc_id]}

        if document:
            update_kwargs["documents"] = [document]
            update_kwargs["embeddings"] = [self._get_embedding(document)]

        if metadata:
            update_kwargs["metadatas"] = [metadata]

        self.collection.update(**update_kwargs)

    def delete_documents(self, ids: List[str]):
        """Elimina documentos por ID."""
        self.collection.delete(ids=ids)

    def count(self) -> int:
        """Retorna el n√∫mero de documentos."""
        return self.collection.count()

    def get_by_id(self, doc_id: str) -> Optional[Dict]:
        """Obtiene un documento por ID."""
        result = self.collection.get(ids=[doc_id])
        if result["ids"]:
            return {
                "id": result["ids"][0],
                "document": result["documents"][0] if result["documents"] else None,
                "metadata": result["metadatas"][0] if result["metadatas"] else None
            }
        return None


# Ejemplo de uso
def demo_chroma():
    # Crear instancia
    db = GeminiChromaDB(
        api_key="TU_API_KEY",
        collection_name="mi_knowledge_base",
        persist_directory="./data/chroma"
    )

    # Agregar documentos
    docs = [
        "Python es un lenguaje de programaci√≥n interpretado",
        "JavaScript es esencial para desarrollo web frontend",
        "Machine Learning usa algoritmos para aprender de datos",
        "Docker permite containerizar aplicaciones",
        "Kubernetes orquesta contenedores en producci√≥n"
    ]

    metadatas = [
        {"category": "programming", "language": "python"},
        {"category": "programming", "language": "javascript"},
        {"category": "ai", "topic": "ml"},
        {"category": "devops", "tool": "docker"},
        {"category": "devops", "tool": "kubernetes"}
    ]

    ids = db.add_documents(docs, metadatas)
    print(f"Documentos agregados: {len(ids)}")
    print(f"Total en colecci√≥n: {db.count()}")

    # B√∫squeda simple
    print("\n--- B√∫squeda: 'lenguajes de programaci√≥n' ---")
    results = db.query("lenguajes de programaci√≥n", n_results=3)

    for i, (doc, dist) in enumerate(zip(results["documents"], results["distances"])):
        print(f"{i+1}. [{1-dist:.3f}] {doc}")

    # B√∫squeda con filtro
    print("\n--- B√∫squeda con filtro (category=devops) ---")
    results = db.query(
        "herramientas de infraestructura",
        n_results=3,
        where={"category": "devops"}
    )

    for doc, meta in zip(results["documents"], results["metadatas"]):
        print(f"- {doc} [{meta}]")


if __name__ == "__main__":
    demo_chroma()
```

### Filtros Avanzados en ChromaDB

```python
"""
Filtros avanzados en ChromaDB
"""

# Filtros por metadata (where)
where_examples = {
    # Igualdad exacta
    "simple": {"category": "programming"},

    # Operadores de comparaci√≥n
    "greater_than": {"price": {"$gt": 100}},
    "less_than": {"price": {"$lt": 50}},
    "greater_equal": {"rating": {"$gte": 4.5}},
    "less_equal": {"count": {"$lte": 10}},
    "not_equal": {"status": {"$ne": "deleted"}},

    # Operadores l√≥gicos
    "and": {
        "$and": [
            {"category": "tech"},
            {"price": {"$lt": 100}}
        ]
    },
    "or": {
        "$or": [
            {"category": "python"},
            {"category": "javascript"}
        ]
    },

    # In (est√° en lista)
    "in": {"category": {"$in": ["python", "javascript", "go"]}}
}

# Filtros por contenido del documento (where_document)
where_document_examples = {
    # Contiene texto
    "contains": {"$contains": "machine learning"},

    # No contiene
    "not_contains": {"$not_contains": "deprecated"}
}


def demo_filters():
    db = GeminiChromaDB(api_key="TU_API_KEY", collection_name="products")

    # Agregar productos de ejemplo
    products = [
        ("Laptop Pro", {"category": "electronics", "price": 999, "rating": 4.5}),
        ("Mouse Wireless", {"category": "electronics", "price": 29, "rating": 4.2}),
        ("Python Book", {"category": "books", "price": 45, "rating": 4.8}),
        ("JavaScript Guide", {"category": "books", "price": 39, "rating": 4.3}),
        ("Mechanical Keyboard", {"category": "electronics", "price": 150, "rating": 4.7})
    ]

    db.add_documents(
        documents=[p[0] for p in products],
        metadatas=[p[1] for p in products]
    )

    # B√∫squeda: electr√≥nicos baratos (< $100)
    print("Electr√≥nicos baratos:")
    results = db.query(
        "dispositivos de computadora",
        where={
            "$and": [
                {"category": "electronics"},
                {"price": {"$lt": 100}}
            ]
        }
    )
    for doc, meta in zip(results["documents"], results["metadatas"]):
        print(f"  - {doc}: ${meta['price']}")

    # B√∫squeda: productos bien calificados
    print("\nProductos rating >= 4.5:")
    results = db.query(
        "productos tecnol√≥gicos",
        where={"rating": {"$gte": 4.5}}
    )
    for doc, meta in zip(results["documents"], results["metadatas"]):
        print(f"  - {doc}: {meta['rating']}‚≠ê")
```

## Pinecone

Pinecone es un servicio cloud totalmente administrado, optimizado para escalabilidad y rendimiento en producci√≥n.

### Configuraci√≥n

```bash
pip install pinecone-client
```

```python
"""
Integraci√≥n de Pinecone con Google Gemini
"""
from pinecone import Pinecone, ServerlessSpec
import google.generativeai as genai
from typing import List, Dict, Any, Optional
import time


class GeminiPinecone:
    """
    Integraci√≥n de Pinecone con embeddings de Gemini.
    """

    def __init__(
        self,
        gemini_api_key: str,
        pinecone_api_key: str,
        index_name: str = "gemini-rag",
        dimension: int = 768,  # Dimensi√≥n de text-embedding-004
        metric: str = "cosine"
    ):
        # Configurar Gemini
        genai.configure(api_key=gemini_api_key)
        self.embedding_model = "models/text-embedding-004"

        # Configurar Pinecone
        self.pc = Pinecone(api_key=pinecone_api_key)

        # Crear √≠ndice si no existe
        existing_indexes = [idx.name for idx in self.pc.list_indexes()]

        if index_name not in existing_indexes:
            self.pc.create_index(
                name=index_name,
                dimension=dimension,
                metric=metric,
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )
            # Esperar a que el √≠ndice est√© listo
            while not self.pc.describe_index(index_name).status['ready']:
                time.sleep(1)

        self.index = self.pc.Index(index_name)
        self.index_name = index_name

    def _get_embedding(self, text: str, task_type: str = "retrieval_document") -> List[float]:
        """Genera embedding usando Gemini."""
        result = genai.embed_content(
            model=self.embedding_model,
            content=text,
            task_type=task_type
        )
        return result['embedding']

    def upsert_documents(
        self,
        documents: List[Dict[str, Any]],
        namespace: str = ""
    ) -> int:
        """
        Inserta o actualiza documentos.

        Args:
            documents: Lista de dicts con 'id', 'text', y opcionalmente 'metadata'
            namespace: Namespace para organizar datos

        Returns:
            N√∫mero de vectores upserted
        """
        vectors = []

        for doc in documents:
            embedding = self._get_embedding(doc["text"])

            metadata = doc.get("metadata", {})
            # Pinecone requiere que el texto est√© en metadata para recuperarlo
            metadata["text"] = doc["text"]

            vectors.append({
                "id": doc["id"],
                "values": embedding,
                "metadata": metadata
            })

        # Upsert en batches de 100 (l√≠mite de Pinecone)
        batch_size = 100
        upserted = 0

        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i+batch_size]
            self.index.upsert(vectors=batch, namespace=namespace)
            upserted += len(batch)

        return upserted

    def query(
        self,
        query_text: str,
        top_k: int = 5,
        namespace: str = "",
        filter: Dict = None,
        include_metadata: bool = True
    ) -> List[Dict]:
        """
        Busca documentos similares.

        Args:
            query_text: Texto de b√∫squeda
            top_k: N√∫mero de resultados
            namespace: Namespace a buscar
            filter: Filtro de metadata
            include_metadata: Incluir metadata en resultados

        Returns:
            Lista de resultados con score y metadata
        """
        query_embedding = self._get_embedding(
            query_text,
            task_type="retrieval_query"
        )

        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            namespace=namespace,
            filter=filter,
            include_metadata=include_metadata
        )

        return [
            {
                "id": match.id,
                "score": match.score,
                "text": match.metadata.get("text", "") if match.metadata else "",
                "metadata": {k: v for k, v in match.metadata.items() if k != "text"} if match.metadata else {}
            }
            for match in results.matches
        ]

    def delete_by_ids(self, ids: List[str], namespace: str = ""):
        """Elimina vectores por ID."""
        self.index.delete(ids=ids, namespace=namespace)

    def delete_by_filter(self, filter: Dict, namespace: str = ""):
        """Elimina vectores que coincidan con el filtro."""
        self.index.delete(filter=filter, namespace=namespace)

    def stats(self) -> Dict:
        """Retorna estad√≠sticas del √≠ndice."""
        return self.index.describe_index_stats()


# Ejemplo de uso
def demo_pinecone():
    db = GeminiPinecone(
        gemini_api_key="TU_GEMINI_KEY",
        pinecone_api_key="TU_PINECONE_KEY",
        index_name="knowledge-base"
    )

    # Documentos de ejemplo
    documents = [
        {
            "id": "doc1",
            "text": "Python es ideal para ciencia de datos y machine learning",
            "metadata": {"category": "programming", "topic": "python"}
        },
        {
            "id": "doc2",
            "text": "TensorFlow es una biblioteca para deep learning",
            "metadata": {"category": "ai", "topic": "frameworks"}
        },
        {
            "id": "doc3",
            "text": "Docker facilita el despliegue de aplicaciones",
            "metadata": {"category": "devops", "topic": "containers"}
        }
    ]

    # Insertar documentos
    count = db.upsert_documents(documents)
    print(f"Documentos insertados: {count}")

    # Estad√≠sticas
    print(f"Stats: {db.stats()}")

    # B√∫squeda
    results = db.query(
        "herramientas para inteligencia artificial",
        top_k=2
    )

    print("\nResultados de b√∫squeda:")
    for r in results:
        print(f"  [{r['score']:.3f}] {r['text']}")

    # B√∫squeda con filtro
    results = db.query(
        "tecnolog√≠a",
        top_k=5,
        filter={"category": {"$eq": "devops"}}
    )

    print("\nResultados filtrados (devops):")
    for r in results:
        print(f"  [{r['score']:.3f}] {r['text']}")
```

### Namespaces en Pinecone

```python
"""
Uso de namespaces para organizar datos en Pinecone
"""

def demo_namespaces():
    db = GeminiPinecone(
        gemini_api_key="TU_KEY",
        pinecone_api_key="TU_KEY"
    )

    # Documentos por tenant/cliente
    tenant_a_docs = [
        {"id": "a1", "text": "Pol√≠tica de vacaciones: 15 d√≠as al a√±o", "metadata": {}},
        {"id": "a2", "text": "Horario laboral: 9am a 6pm", "metadata": {}}
    ]

    tenant_b_docs = [
        {"id": "b1", "text": "Pol√≠tica de vacaciones: 20 d√≠as al a√±o", "metadata": {}},
        {"id": "b2", "text": "Horario laboral: flexible", "metadata": {}}
    ]

    # Insertar en namespaces separados
    db.upsert_documents(tenant_a_docs, namespace="tenant_a")
    db.upsert_documents(tenant_b_docs, namespace="tenant_b")

    # Buscar solo en namespace espec√≠fico
    query = "vacaciones"

    print("Resultados Tenant A:")
    results_a = db.query(query, namespace="tenant_a")
    for r in results_a:
        print(f"  {r['text']}")

    print("\nResultados Tenant B:")
    results_b = db.query(query, namespace="tenant_b")
    for r in results_b:
        print(f"  {r['text']}")
```

## Weaviate

Weaviate es una base de datos vectorial con soporte para GraphQL y m√≥dulos de ML integrados.

### Configuraci√≥n

```bash
pip install weaviate-client
```

```python
"""
Integraci√≥n de Weaviate con Google Gemini
"""
import weaviate
from weaviate.classes.config import Configure, Property, DataType
from weaviate.classes.query import MetadataQuery
import google.generativeai as genai
from typing import List, Dict, Any, Optional


class GeminiWeaviate:
    """
    Integraci√≥n de Weaviate con embeddings de Gemini.
    """

    def __init__(
        self,
        gemini_api_key: str,
        weaviate_url: str = "http://localhost:8080",
        class_name: str = "Document"
    ):
        # Configurar Gemini
        genai.configure(api_key=gemini_api_key)
        self.embedding_model = "models/text-embedding-004"

        # Conectar a Weaviate
        self.client = weaviate.connect_to_local(
            host=weaviate_url.replace("http://", "").split(":")[0],
            port=int(weaviate_url.split(":")[-1])
        )

        self.class_name = class_name
        self._ensure_class_exists()

    def _ensure_class_exists(self):
        """Crea la clase si no existe."""
        if not self.client.collections.exists(self.class_name):
            self.client.collections.create(
                name=self.class_name,
                properties=[
                    Property(name="text", data_type=DataType.TEXT),
                    Property(name="category", data_type=DataType.TEXT),
                    Property(name="source", data_type=DataType.TEXT)
                ],
                # Sin vectorizer - usamos embeddings externos
                vectorizer_config=Configure.Vectorizer.none()
            )

    def _get_embedding(self, text: str, task_type: str = "retrieval_document") -> List[float]:
        """Genera embedding usando Gemini."""
        result = genai.embed_content(
            model=self.embedding_model,
            content=text,
            task_type=task_type
        )
        return result['embedding']

    def add_documents(
        self,
        documents: List[Dict[str, Any]]
    ) -> List[str]:
        """
        Agrega documentos a Weaviate.

        Args:
            documents: Lista de dicts con 'text' y propiedades adicionales

        Returns:
            Lista de UUIDs creados
        """
        collection = self.client.collections.get(self.class_name)
        uuids = []

        with collection.batch.dynamic() as batch:
            for doc in documents:
                # Generar embedding
                embedding = self._get_embedding(doc["text"])

                # Preparar propiedades
                properties = {
                    "text": doc["text"],
                    "category": doc.get("category", ""),
                    "source": doc.get("source", "")
                }

                # Agregar al batch
                uuid = batch.add_object(
                    properties=properties,
                    vector=embedding
                )
                uuids.append(str(uuid))

        return uuids

    def query(
        self,
        query_text: str,
        limit: int = 5,
        filters: Dict = None
    ) -> List[Dict]:
        """
        Busca documentos similares.

        Args:
            query_text: Texto de b√∫squeda
            limit: N√∫mero de resultados
            filters: Filtros de propiedades

        Returns:
            Lista de resultados
        """
        collection = self.client.collections.get(self.class_name)

        # Generar embedding de la query
        query_embedding = self._get_embedding(
            query_text,
            task_type="retrieval_query"
        )

        # Construir query
        query_builder = collection.query.near_vector(
            near_vector=query_embedding,
            limit=limit,
            return_metadata=MetadataQuery(distance=True)
        )

        # Aplicar filtros si existen
        if filters:
            # Weaviate usa su propio sistema de filtros
            pass  # Simplificado para el ejemplo

        results = query_builder

        return [
            {
                "uuid": str(obj.uuid),
                "text": obj.properties.get("text", ""),
                "category": obj.properties.get("category", ""),
                "source": obj.properties.get("source", ""),
                "distance": obj.metadata.distance if obj.metadata else None
            }
            for obj in results.objects
        ]

    def close(self):
        """Cierra la conexi√≥n."""
        self.client.close()


# Ejemplo de uso
def demo_weaviate():
    db = GeminiWeaviate(
        gemini_api_key="TU_KEY",
        weaviate_url="http://localhost:8080",
        class_name="KnowledgeBase"
    )

    try:
        # Agregar documentos
        documents = [
            {
                "text": "FastAPI es un framework moderno para APIs en Python",
                "category": "web",
                "source": "docs"
            },
            {
                "text": "Django es un framework web full-stack para Python",
                "category": "web",
                "source": "docs"
            },
            {
                "text": "NumPy es fundamental para computaci√≥n num√©rica",
                "category": "data-science",
                "source": "tutorial"
            }
        ]

        uuids = db.add_documents(documents)
        print(f"Documentos agregados: {len(uuids)}")

        # Buscar
        results = db.query("frameworks web python", limit=2)

        print("\nResultados:")
        for r in results:
            print(f"  [{r['distance']:.3f}] {r['text']}")
            print(f"    Categor√≠a: {r['category']}, Fuente: {r['source']}")

    finally:
        db.close()
```

## Comparaci√≥n de Filtros

### Sintaxis de Filtros por Base de Datos

```python
"""
Comparaci√≥n de sintaxis de filtros entre bases de datos vectoriales
"""

# CHROMADB - Sintaxis MongoDB-like
chroma_filters = {
    "igualdad": {"category": "tech"},
    "mayor_que": {"price": {"$gt": 100}},
    "and": {"$and": [{"a": 1}, {"b": 2}]},
    "or": {"$or": [{"a": 1}, {"a": 2}]},
    "in": {"category": {"$in": ["a", "b", "c"]}}
}

# PINECONE - Sintaxis similar
pinecone_filters = {
    "igualdad": {"category": {"$eq": "tech"}},  # Nota: requiere $eq expl√≠cito
    "mayor_que": {"price": {"$gt": 100}},
    "and": {"$and": [{"a": {"$eq": 1}}, {"b": {"$eq": 2}}]},
    "or": {"$or": [{"a": {"$eq": 1}}, {"a": {"$eq": 2}}]},
    "in": {"category": {"$in": ["a", "b", "c"]}}
}

# WEAVIATE - Sintaxis GraphQL-based
weaviate_filters = {
    "igualdad": {
        "path": ["category"],
        "operator": "Equal",
        "valueText": "tech"
    },
    "mayor_que": {
        "path": ["price"],
        "operator": "GreaterThan",
        "valueNumber": 100
    },
    "and": {
        "operator": "And",
        "operands": [
            {"path": ["a"], "operator": "Equal", "valueInt": 1},
            {"path": ["b"], "operator": "Equal", "valueInt": 2}
        ]
    }
}
```

## Pipeline RAG con Vector Database

```python
"""
Pipeline RAG completo usando ChromaDB como ejemplo
"""
import google.generativeai as genai
from typing import List, Dict, Optional


class RAGWithVectorDB:
    """
    Pipeline RAG completo con base de datos vectorial.
    """

    def __init__(
        self,
        api_key: str,
        vector_db: GeminiChromaDB,  # O GeminiPinecone, GeminiWeaviate
        generation_model: str = "gemini-2.0-flash"
    ):
        genai.configure(api_key=api_key)
        self.vector_db = vector_db
        self.generation_model = genai.GenerativeModel(generation_model)

    def ingest_documents(
        self,
        documents: List[str],
        metadatas: List[Dict] = None,
        chunk_size: int = 500
    ):
        """
        Ingesta documentos con chunking autom√°tico.
        """
        all_chunks = []
        all_metadatas = []

        for i, doc in enumerate(documents):
            # Dividir en chunks
            chunks = self._chunk_text(doc, chunk_size)

            for j, chunk in enumerate(chunks):
                all_chunks.append(chunk)

                # Metadata del chunk
                meta = metadatas[i].copy() if metadatas else {}
                meta["chunk_index"] = j
                meta["total_chunks"] = len(chunks)
                all_metadatas.append(meta)

        # Agregar a la base de datos vectorial
        self.vector_db.add_documents(all_chunks, all_metadatas)

        return len(all_chunks)

    def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
        """Divide texto en chunks."""
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0

        for word in words:
            if current_size + len(word) > chunk_size and current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_size = 0

            current_chunk.append(word)
            current_size += len(word) + 1

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

    def query(
        self,
        question: str,
        top_k: int = 5,
        filter_metadata: Dict = None
    ) -> Dict:
        """
        Ejecuta el pipeline RAG completo.
        """
        # 1. Recuperar contexto
        results = self.vector_db.query(
            question,
            n_results=top_k,
            where=filter_metadata
        )

        if not results["documents"]:
            return {
                "answer": "No encontr√© informaci√≥n relevante.",
                "sources": []
            }

        # 2. Construir contexto
        context = "\n\n".join([
            f"[Fragmento {i+1}]: {doc}"
            for i, doc in enumerate(results["documents"])
        ])

        # 3. Generar respuesta
        prompt = f"""Bas√°ndote en el siguiente contexto, responde la pregunta.
Si la informaci√≥n no est√° en el contexto, indica que no tienes esa informaci√≥n.

CONTEXTO:
{context}

PREGUNTA: {question}

RESPUESTA:"""

        response = self.generation_model.generate_content(prompt)

        return {
            "answer": response.text,
            "sources": [
                {"text": doc[:100] + "...", "metadata": meta}
                for doc, meta in zip(results["documents"], results["metadatas"])
            ],
            "distances": results.get("distances", [])
        }


# Ejemplo completo
def demo_rag_pipeline():
    # Crear vector DB
    vector_db = GeminiChromaDB(
        api_key="TU_KEY",
        collection_name="rag_demo"
    )

    # Crear pipeline RAG
    rag = RAGWithVectorDB(
        api_key="TU_KEY",
        vector_db=vector_db
    )

    # Documentos de ejemplo
    docs = [
        """La empresa ofrece 15 d√≠as de vacaciones al a√±o.
        Los empleados pueden acumular hasta 30 d√≠as.
        Las vacaciones deben solicitarse con 2 semanas de anticipaci√≥n.""",

        """El horario de trabajo es de 9am a 6pm.
        Se permite trabajo remoto 2 d√≠as a la semana.
        Los viernes el horario es de 9am a 3pm.""",

        """Los beneficios incluyen seguro m√©dico completo.
        Tambi√©n se ofrece seguro dental y de visi√≥n.
        El seguro cubre al empleado y dependientes."""
    ]

    metadatas = [
        {"topic": "vacaciones", "department": "rrhh"},
        {"topic": "horarios", "department": "rrhh"},
        {"topic": "beneficios", "department": "rrhh"}
    ]

    # Ingestar documentos
    chunks = rag.ingest_documents(docs, metadatas)
    print(f"Chunks indexados: {chunks}")

    # Consultas
    questions = [
        "¬øCu√°ntos d√≠as de vacaciones tengo?",
        "¬øPuedo trabajar desde casa?",
        "¬øEl seguro cubre a mi familia?"
    ]

    for q in questions:
        print(f"\n‚ùì {q}")
        result = rag.query(q, top_k=2)
        print(f"üí¨ {result['answer']}")
        print(f"üìö Fuentes: {len(result['sources'])}")


if __name__ == "__main__":
    demo_rag_pipeline()
```

## Ejercicio Pr√°ctico

```python
"""
EJERCICIO: Sistema RAG Multi-Tenant

Construye un sistema RAG que soporte m√∫ltiples organizaciones
(tenants), cada una con su propia base de conocimiento aislada.
"""


class MultiTenantRAG:
    """
    Sistema RAG multi-tenant usando namespaces/colecciones.

    TODO: Implementar las siguientes funcionalidades:
    """

    def __init__(self, api_key: str, db_type: str = "chroma"):
        """
        TODO:
        1. Configurar API de Gemini
        2. Inicializar la base de datos vectorial elegida
        3. Mantener registro de tenants activos
        """
        pass

    def create_tenant(self, tenant_id: str) -> bool:
        """
        Crea un nuevo tenant con su espacio aislado.

        TODO:
        1. Verificar que el tenant no exista
        2. Crear namespace/colecci√≥n para el tenant
        3. Registrar el tenant
        """
        pass

    def add_knowledge(
        self,
        tenant_id: str,
        documents: List[str],
        metadatas: List[Dict] = None
    ) -> int:
        """
        Agrega conocimiento al tenant espec√≠fico.

        TODO:
        1. Validar que el tenant exista
        2. Agregar documentos al namespace del tenant
        3. Retornar cantidad agregada
        """
        pass

    def query_tenant(
        self,
        tenant_id: str,
        question: str,
        top_k: int = 5
    ) -> Dict:
        """
        Consulta solo dentro del conocimiento del tenant.

        TODO:
        1. Validar tenant
        2. Buscar en el namespace del tenant
        3. Generar respuesta con Gemini
        """
        pass

    def delete_tenant(self, tenant_id: str) -> bool:
        """
        Elimina un tenant y todo su conocimiento.

        TODO:
        1. Eliminar namespace/colecci√≥n
        2. Limpiar registro del tenant
        """
        pass

    def list_tenants(self) -> List[Dict]:
        """
        Lista todos los tenants con estad√≠sticas.

        TODO:
        1. Obtener lista de tenants
        2. Incluir conteo de documentos por tenant
        """
        pass


def test_multi_tenant():
    """Prueba el sistema multi-tenant."""

    # TODO: Implementar pruebas
    # rag = MultiTenantRAG(api_key="TU_KEY")
    #
    # # Crear tenants
    # rag.create_tenant("empresa_a")
    # rag.create_tenant("empresa_b")
    #
    # # Agregar conocimiento diferente
    # rag.add_knowledge("empresa_a", [
    #     "Pol√≠tica: 15 d√≠as de vacaciones",
    #     "Horario: 9am - 5pm"
    # ])
    #
    # rag.add_knowledge("empresa_b", [
    #     "Pol√≠tica: 20 d√≠as de vacaciones",
    #     "Horario: Flexible"
    # ])
    #
    # # Verificar aislamiento
    # result_a = rag.query_tenant("empresa_a", "¬øCu√°ntas vacaciones tengo?")
    # result_b = rag.query_tenant("empresa_b", "¬øCu√°ntas vacaciones tengo?")
    #
    # print(f"Empresa A: {result_a['answer']}")  # Deber√≠a decir 15 d√≠as
    # print(f"Empresa B: {result_b['answer']}")  # Deber√≠a decir 20 d√≠as


if __name__ == "__main__":
    test_multi_tenant()
```

## Resumen

| Base de Datos | Mejor Para | Escalabilidad | Costo |
|---------------|------------|---------------|-------|
| ChromaDB | Desarrollo, prototipos | Media | Gratis (open-source) |
| Pinecone | Producci√≥n, SaaS | Muy alta | Pay-as-you-go |
| Weaviate | Flexibilidad, GraphQL | Alta | Gratis (self-hosted) |

### Checklist de Implementaci√≥n

- [ ] Elegir base de datos seg√∫n caso de uso
- [ ] Implementar generaci√≥n de embeddings con Gemini
- [ ] Configurar √≠ndices con la m√©trica correcta (coseno)
- [ ] Implementar manejo de errores y reintentos
- [ ] Agregar logging y monitoreo
- [ ] Considerar estrategia de namespaces/multi-tenant

## Siguiente Paso

En el pr√≥ximo subtema exploraremos **Hybrid Search (Dense + Sparse)**, combinando b√∫squeda sem√°ntica con b√∫squeda por keywords para mejorar la relevancia.
