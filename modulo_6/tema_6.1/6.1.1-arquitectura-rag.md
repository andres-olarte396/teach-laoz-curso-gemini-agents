# 6.1.1 Arquitectura RAG: Indexaci√≥n y Retrieval

## Objetivo de Aprendizaje

Al finalizar este subtema, ser√°s capaz de construir pipelines RAG completos que indexan documentos y recuperan informaci√≥n relevante para enriquecer las respuestas de agentes con Google Gemini.

## Introducci√≥n

**Retrieval-Augmented Generation (RAG)** es una arquitectura que combina la capacidad de recuperaci√≥n de informaci√≥n con la generaci√≥n de texto de LLMs. En lugar de depender √∫nicamente del conocimiento interno del modelo, RAG permite que los agentes accedan a fuentes externas de informaci√≥n actualizada y espec√≠fica del dominio.

### Por qu√© RAG es Esencial para Agentes

```
Sin RAG:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Usuario: ¬øCu√°l es la pol√≠tica de           ‚îÇ
‚îÇ          devoluciones de mi empresa?        ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Agente: No tengo informaci√≥n espec√≠fica     ‚îÇ
‚îÇ         sobre tu empresa...                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Con RAG:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Usuario: ¬øCu√°l es la pol√≠tica de           ‚îÇ
‚îÇ          devoluciones de mi empresa?        ‚îÇ
‚îÇ                    ‚îÇ                        ‚îÇ
‚îÇ                    ‚ñº                        ‚îÇ
‚îÇ         [Buscar en documentos]              ‚îÇ
‚îÇ                    ‚îÇ                        ‚îÇ
‚îÇ                    ‚ñº                        ‚îÇ
‚îÇ Agente: Seg√∫n la pol√≠tica interna v2.3,    ‚îÇ
‚îÇ         las devoluciones se aceptan        ‚îÇ
‚îÇ         dentro de 30 d√≠as con recibo...    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Arquitectura de un Sistema RAG

### Componentes Principales

```
                    PIPELINE RAG COMPLETO

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE DE INDEXACI√ìN                     ‚îÇ
‚îÇ                    (Offline / Batch)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇDocumentos‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Chunking ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Embeddings    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (PDFs,  ‚îÇ    ‚îÇ(Dividir) ‚îÇ    ‚îÇ   (Vectorizar)   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  TXT...)‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ              ‚îÇ
‚îÇ                                           ‚ñº              ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ                              ‚îÇ   Vector Store     ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  (Base de Datos)   ‚îÇ     ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE DE RETRIEVAL                      ‚îÇ
‚îÇ                    (Online / Query)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Query   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Embedding‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  B√∫squeda de     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Usuario  ‚îÇ    ‚îÇ de Query ‚îÇ    ‚îÇ  Similitud       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                           ‚îÇ              ‚îÇ
‚îÇ                                           ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇRespuesta ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Gemini  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Top-K Chunks   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Final    ‚îÇ    ‚îÇ Generate ‚îÇ    ‚îÇ   Relevantes     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Implementaci√≥n Paso a Paso

### 1. Sistema de Indexaci√≥n de Documentos

```python
"""
Sistema de Indexaci√≥n RAG con Google Gemini
"""
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
import hashlib
import json
from pathlib import Path
from datetime import datetime
import re


@dataclass
class Document:
    """Representa un documento fuente."""
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    doc_id: str = ""

    def __post_init__(self):
        if not self.doc_id:
            # Generar ID √∫nico basado en contenido
            self.doc_id = hashlib.md5(
                self.content.encode()
            ).hexdigest()[:12]


@dataclass
class Chunk:
    """Representa un fragmento de documento."""
    content: str
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    chunk_id: str = ""
    source_doc_id: str = ""

    def __post_init__(self):
        if not self.chunk_id:
            self.chunk_id = hashlib.md5(
                f"{self.source_doc_id}:{self.content[:100]}".encode()
            ).hexdigest()[:12]


class DocumentProcessor:
    """Procesa documentos para el pipeline RAG."""

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", ". ", " "]

    def load_text_file(self, file_path: str) -> Document:
        """Carga un archivo de texto."""
        path = Path(file_path)
        content = path.read_text(encoding='utf-8')

        return Document(
            content=content,
            metadata={
                "source": str(path),
                "filename": path.name,
                "type": "text",
                "loaded_at": datetime.now().isoformat()
            }
        )

    def split_document(self, document: Document) -> List[Chunk]:
        """Divide un documento en chunks."""
        text = document.content
        chunks = []

        # Dividir usando separadores jer√°rquicos
        segments = self._recursive_split(
            text,
            self.separators,
            self.chunk_size
        )

        # Crear chunks con overlap
        for i, segment in enumerate(segments):
            # Agregar contexto del chunk anterior (overlap)
            if i > 0 and self.chunk_overlap > 0:
                prev_text = segments[i-1][-self.chunk_overlap:]
                segment = prev_text + segment

            chunk = Chunk(
                content=segment.strip(),
                source_doc_id=document.doc_id,
                metadata={
                    **document.metadata,
                    "chunk_index": i,
                    "total_chunks": len(segments)
                }
            )
            chunks.append(chunk)

        return chunks

    def _recursive_split(
        self,
        text: str,
        separators: List[str],
        max_size: int
    ) -> List[str]:
        """Divisi√≥n recursiva con separadores jer√°rquicos."""
        if len(text) <= max_size:
            return [text] if text.strip() else []

        if not separators:
            # Sin separadores, dividir por tama√±o fijo
            return [
                text[i:i+max_size]
                for i in range(0, len(text), max_size)
            ]

        separator = separators[0]
        parts = text.split(separator)

        result = []
        current = ""

        for part in parts:
            candidate = current + separator + part if current else part

            if len(candidate) <= max_size:
                current = candidate
            else:
                if current:
                    result.append(current)

                # Si la parte es muy grande, dividir recursivamente
                if len(part) > max_size:
                    sub_parts = self._recursive_split(
                        part,
                        separators[1:],
                        max_size
                    )
                    result.extend(sub_parts[:-1])
                    current = sub_parts[-1] if sub_parts else ""
                else:
                    current = part

        if current:
            result.append(current)

        return result


class EmbeddingGenerator:
    """Genera embeddings usando Gemini."""

    def __init__(self, model_name: str = "models/text-embedding-004"):
        self.model_name = model_name

    def embed_text(self, text: str) -> List[float]:
        """Genera embedding para un texto."""
        result = genai.embed_content(
            model=self.model_name,
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']

    def embed_query(self, query: str) -> List[float]:
        """Genera embedding para una query (optimizado para b√∫squeda)."""
        result = genai.embed_content(
            model=self.model_name,
            content=query,
            task_type="retrieval_query"
        )
        return result['embedding']

    def embed_chunks(self, chunks: List[Chunk]) -> List[Chunk]:
        """Genera embeddings para una lista de chunks."""
        for chunk in chunks:
            chunk.embedding = self.embed_text(chunk.content)
        return chunks


# Ejemplo de uso
def demo_indexacion():
    """Demuestra el proceso de indexaci√≥n."""
    genai.configure(api_key="TU_API_KEY")

    # Crear procesador
    processor = DocumentProcessor(
        chunk_size=500,
        chunk_overlap=50
    )

    # Crear documento de ejemplo
    documento_ejemplo = Document(
        content="""
        # Manual de Usuario - Sistema CRM

        ## Introducci√≥n
        Este manual describe el uso del sistema CRM corporativo.
        El sistema permite gestionar clientes, oportunidades y ventas.

        ## Gesti√≥n de Clientes
        Para crear un nuevo cliente, navegue a Clientes > Nuevo.
        Complete los campos obligatorios: nombre, email y tel√©fono.
        Los campos opcionales incluyen direcci√≥n y notas.

        ## Oportunidades de Venta
        Las oportunidades representan posibles ventas.
        Cada oportunidad tiene un estado: Nueva, En Progreso, Ganada, Perdida.
        El valor estimado se usa para proyecciones de ingresos.

        ## Reportes
        El m√≥dulo de reportes genera an√°lisis autom√°ticos.
        Puede exportar reportes en PDF o Excel.
        Los dashboards muestran m√©tricas en tiempo real.
        """,
        metadata={"tipo": "manual", "version": "1.0"}
    )

    # Dividir en chunks
    chunks = processor.split_document(documento_ejemplo)
    print(f"Documento dividido en {len(chunks)} chunks")

    for i, chunk in enumerate(chunks):
        print(f"\n--- Chunk {i+1} ---")
        print(f"ID: {chunk.chunk_id}")
        print(f"Contenido ({len(chunk.content)} chars):")
        print(chunk.content[:200] + "...")

    # Generar embeddings
    embedder = EmbeddingGenerator()
    chunks_con_embeddings = embedder.embed_chunks(chunks)

    print(f"\nEmbeddings generados: {len(chunks_con_embeddings)}")
    print(f"Dimensi√≥n: {len(chunks_con_embeddings[0].embedding)}")


if __name__ == "__main__":
    demo_indexacion()
```

### 2. Vector Store Simple (En Memoria)

```python
"""
Vector Store en memoria para RAG
"""
import numpy as np
from typing import List, Tuple, Dict, Any
from dataclasses import dataclass
import json
from pathlib import Path


@dataclass
class SearchResult:
    """Resultado de b√∫squeda."""
    chunk: Chunk
    score: float
    rank: int


class InMemoryVectorStore:
    """
    Vector Store simple en memoria.
    Para producci√≥n, usar Chroma, Pinecone, etc.
    """

    def __init__(self):
        self.chunks: Dict[str, Chunk] = {}
        self.embeddings: Dict[str, np.ndarray] = {}

    def add_chunks(self, chunks: List[Chunk]) -> int:
        """Agrega chunks al store."""
        added = 0
        for chunk in chunks:
            if chunk.embedding is None:
                raise ValueError(f"Chunk {chunk.chunk_id} sin embedding")

            self.chunks[chunk.chunk_id] = chunk
            self.embeddings[chunk.chunk_id] = np.array(chunk.embedding)
            added += 1

        return added

    def search(
        self,
        query_embedding: List[float],
        top_k: int = 5,
        score_threshold: float = 0.0
    ) -> List[SearchResult]:
        """Busca chunks similares a la query."""
        if not self.embeddings:
            return []

        query_vec = np.array(query_embedding)

        # Calcular similaridad coseno con todos los chunks
        similarities = []
        for chunk_id, embedding in self.embeddings.items():
            score = self._cosine_similarity(query_vec, embedding)
            if score >= score_threshold:
                similarities.append((chunk_id, score))

        # Ordenar por score descendente
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Retornar top-k resultados
        results = []
        for rank, (chunk_id, score) in enumerate(similarities[:top_k], 1):
            results.append(SearchResult(
                chunk=self.chunks[chunk_id],
                score=score,
                rank=rank
            ))

        return results

    def _cosine_similarity(
        self,
        vec1: np.ndarray,
        vec2: np.ndarray
    ) -> float:
        """Calcula similaridad coseno entre dos vectores."""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def save(self, path: str):
        """Guarda el store en disco."""
        data = {
            "chunks": {
                cid: {
                    "content": c.content,
                    "metadata": c.metadata,
                    "source_doc_id": c.source_doc_id,
                    "embedding": c.embedding
                }
                for cid, c in self.chunks.items()
            }
        }

        Path(path).write_text(json.dumps(data, indent=2))

    def load(self, path: str):
        """Carga el store desde disco."""
        data = json.loads(Path(path).read_text())

        for chunk_id, chunk_data in data["chunks"].items():
            chunk = Chunk(
                content=chunk_data["content"],
                metadata=chunk_data["metadata"],
                source_doc_id=chunk_data["source_doc_id"],
                embedding=chunk_data["embedding"],
                chunk_id=chunk_id
            )
            self.chunks[chunk_id] = chunk
            self.embeddings[chunk_id] = np.array(chunk.embedding)

    def __len__(self) -> int:
        return len(self.chunks)

    def stats(self) -> Dict[str, Any]:
        """Retorna estad√≠sticas del store."""
        if not self.chunks:
            return {"count": 0}

        embedding_dim = len(next(iter(self.embeddings.values())))
        content_lengths = [len(c.content) for c in self.chunks.values()]

        return {
            "count": len(self.chunks),
            "embedding_dimension": embedding_dim,
            "avg_content_length": sum(content_lengths) / len(content_lengths),
            "min_content_length": min(content_lengths),
            "max_content_length": max(content_lengths)
        }
```

### 3. Pipeline RAG Completo

```python
"""
Pipeline RAG completo con Gemini
"""
import google.generativeai as genai
from typing import List, Optional, Dict, Any


class RAGPipeline:
    """
    Pipeline RAG completo que integra:
    - Procesamiento de documentos
    - Generaci√≥n de embeddings
    - B√∫squeda sem√°ntica
    - Generaci√≥n aumentada
    """

    def __init__(
        self,
        api_key: str,
        generation_model: str = "gemini-2.0-flash",
        embedding_model: str = "models/text-embedding-004",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        top_k: int = 5
    ):
        genai.configure(api_key=api_key)

        self.generation_model = genai.GenerativeModel(generation_model)
        self.embedding_model = embedding_model
        self.top_k = top_k

        # Componentes
        self.processor = DocumentProcessor(chunk_size, chunk_overlap)
        self.embedder = EmbeddingGenerator(embedding_model)
        self.vector_store = InMemoryVectorStore()

        # Estad√≠sticas
        self.stats = {
            "documents_indexed": 0,
            "chunks_indexed": 0,
            "queries_processed": 0
        }

    def index_document(self, document: Document) -> int:
        """Indexa un documento completo."""
        # 1. Dividir en chunks
        chunks = self.processor.split_document(document)

        # 2. Generar embeddings
        chunks_with_embeddings = self.embedder.embed_chunks(chunks)

        # 3. Almacenar en vector store
        added = self.vector_store.add_chunks(chunks_with_embeddings)

        # Actualizar estad√≠sticas
        self.stats["documents_indexed"] += 1
        self.stats["chunks_indexed"] += added

        return added

    def index_text(
        self,
        text: str,
        metadata: Dict[str, Any] = None
    ) -> int:
        """Indexa texto directamente."""
        document = Document(
            content=text,
            metadata=metadata or {}
        )
        return self.index_document(document)

    def retrieve(
        self,
        query: str,
        top_k: int = None
    ) -> List[SearchResult]:
        """Recupera chunks relevantes para una query."""
        # 1. Generar embedding de la query
        query_embedding = self.embedder.embed_query(query)

        # 2. Buscar en vector store
        results = self.vector_store.search(
            query_embedding=query_embedding,
            top_k=top_k or self.top_k
        )

        return results

    def generate(
        self,
        query: str,
        context_chunks: List[SearchResult],
        system_instruction: str = None
    ) -> str:
        """Genera respuesta usando contexto recuperado."""
        # Construir contexto
        context_parts = []
        for result in context_chunks:
            source = result.chunk.metadata.get("source", "Desconocido")
            context_parts.append(
                f"[Fuente: {source}, Relevancia: {result.score:.2f}]\n"
                f"{result.chunk.content}"
            )

        context = "\n\n---\n\n".join(context_parts)

        # Construir prompt
        prompt = f"""Contexto recuperado de la base de conocimiento:

{context}

---

Pregunta del usuario: {query}

Instrucciones:
- Responde bas√°ndote √öNICAMENTE en el contexto proporcionado
- Si la informaci√≥n no est√° en el contexto, indica que no tienes esa informaci√≥n
- Cita las fuentes cuando sea relevante
- S√© conciso y directo

Respuesta:"""

        # Generar respuesta
        if system_instruction:
            model = genai.GenerativeModel(
                model_name=self.generation_model.model_name,
                system_instruction=system_instruction
            )
            response = model.generate_content(prompt)
        else:
            response = self.generation_model.generate_content(prompt)

        return response.text

    def query(
        self,
        query: str,
        top_k: int = None,
        system_instruction: str = None,
        return_sources: bool = False
    ) -> Dict[str, Any]:
        """
        Pipeline completo: retrieve + generate.
        """
        self.stats["queries_processed"] += 1

        # 1. Recuperar contexto relevante
        results = self.retrieve(query, top_k)

        if not results:
            return {
                "answer": "No encontr√© informaci√≥n relevante para tu pregunta.",
                "sources": [],
                "chunks_used": 0
            }

        # 2. Generar respuesta
        answer = self.generate(query, results, system_instruction)

        # Preparar respuesta
        response = {
            "answer": answer,
            "chunks_used": len(results)
        }

        if return_sources:
            response["sources"] = [
                {
                    "content": r.chunk.content[:200] + "...",
                    "score": r.score,
                    "metadata": r.chunk.metadata
                }
                for r in results
            ]

        return response


# Ejemplo completo
def demo_rag_pipeline():
    """Demostraci√≥n del pipeline RAG completo."""

    # Crear pipeline
    rag = RAGPipeline(
        api_key="TU_API_KEY",
        chunk_size=300,
        top_k=3
    )

    # Documentos de ejemplo (base de conocimiento)
    documentos = [
        """
        # Pol√≠tica de Devoluciones - TechStore

        Aceptamos devoluciones dentro de 30 d√≠as desde la compra.
        El producto debe estar en su empaque original y sin uso.
        Para iniciar una devoluci√≥n, contacte a soporte@techstore.com.
        Los reembolsos se procesan en 5-7 d√≠as h√°biles.
        Los productos en oferta no son reembolsables.
        """,
        """
        # Horarios de Atenci√≥n - TechStore

        Tiendas f√≠sicas: Lunes a S√°bado, 9:00 AM - 9:00 PM
        Domingos: 10:00 AM - 6:00 PM

        Soporte telef√≥nico: 24/7 al 1-800-TECH
        Chat en l√≠nea: Lunes a Viernes, 8:00 AM - 10:00 PM

        D√≠as festivos: Horario reducido, consulte el calendario.
        """,
        """
        # Garant√≠a de Productos - TechStore

        Todos los productos incluyen garant√≠a del fabricante.
        Laptops y PCs: 2 a√±os de garant√≠a
        Smartphones: 1 a√±o de garant√≠a
        Accesorios: 6 meses de garant√≠a

        La garant√≠a cubre defectos de fabricaci√≥n.
        No cubre da√±os por mal uso o accidentes.
        Para reclamaciones, presente factura original.
        """
    ]

    # Indexar documentos
    print("Indexando documentos...")
    for doc in documentos:
        chunks = rag.index_text(doc, {"source": "TechStore Knowledge Base"})
        print(f"  - Indexado: {chunks} chunks")

    print(f"\nEstad√≠sticas: {rag.stats}")
    print(f"Vector Store: {rag.vector_store.stats()}")

    # Realizar consultas
    preguntas = [
        "¬øCu√°l es la pol√≠tica de devoluciones?",
        "¬øCu√°nto dura la garant√≠a de un smartphone?",
        "¬øA qu√© hora abren los domingos?",
        "¬øC√≥mo puedo contactar al soporte t√©cnico?"
    ]

    print("\n" + "="*60)
    print("CONSULTAS RAG")
    print("="*60)

    for pregunta in preguntas:
        print(f"\nüìù Pregunta: {pregunta}")
        print("-" * 40)

        resultado = rag.query(
            pregunta,
            return_sources=True
        )

        print(f"üí¨ Respuesta: {resultado['answer']}")
        print(f"\nüìö Fuentes usadas: {resultado['chunks_used']}")

        for i, source in enumerate(resultado.get("sources", []), 1):
            print(f"   {i}. Score: {source['score']:.3f}")


if __name__ == "__main__":
    demo_rag_pipeline()
```

## M√©tricas y Monitoreo

### Evaluaci√≥n del Sistema RAG

```python
"""
M√©tricas para evaluar calidad del RAG
"""
from dataclasses import dataclass
from typing import List, Dict
import time


@dataclass
class RAGMetrics:
    """M√©tricas de una consulta RAG."""
    query: str
    retrieval_time_ms: float
    generation_time_ms: float
    total_time_ms: float
    chunks_retrieved: int
    avg_relevance_score: float
    top_score: float
    answer_length: int


class RAGMonitor:
    """Monitor de rendimiento para RAG."""

    def __init__(self, rag_pipeline: RAGPipeline):
        self.rag = rag_pipeline
        self.metrics_history: List[RAGMetrics] = []

    def query_with_metrics(
        self,
        query: str,
        **kwargs
    ) -> tuple[Dict, RAGMetrics]:
        """Ejecuta query y captura m√©tricas."""

        # Medir retrieval
        start_retrieval = time.time()
        results = self.rag.retrieve(query)
        retrieval_time = (time.time() - start_retrieval) * 1000

        # Medir generaci√≥n
        start_generation = time.time()
        answer = self.rag.generate(query, results)
        generation_time = (time.time() - start_generation) * 1000

        # Calcular m√©tricas
        scores = [r.score for r in results]
        metrics = RAGMetrics(
            query=query,
            retrieval_time_ms=retrieval_time,
            generation_time_ms=generation_time,
            total_time_ms=retrieval_time + generation_time,
            chunks_retrieved=len(results),
            avg_relevance_score=sum(scores) / len(scores) if scores else 0,
            top_score=max(scores) if scores else 0,
            answer_length=len(answer)
        )

        self.metrics_history.append(metrics)

        return {"answer": answer, "sources": results}, metrics

    def summary(self) -> Dict:
        """Resumen de m√©tricas acumuladas."""
        if not self.metrics_history:
            return {}

        return {
            "total_queries": len(self.metrics_history),
            "avg_retrieval_ms": sum(m.retrieval_time_ms for m in self.metrics_history) / len(self.metrics_history),
            "avg_generation_ms": sum(m.generation_time_ms for m in self.metrics_history) / len(self.metrics_history),
            "avg_total_ms": sum(m.total_time_ms for m in self.metrics_history) / len(self.metrics_history),
            "avg_relevance": sum(m.avg_relevance_score for m in self.metrics_history) / len(self.metrics_history),
            "avg_top_score": sum(m.top_score for m in self.metrics_history) / len(self.metrics_history)
        }
```

## Diagrama de Flujo de Datos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FLUJO DE DATOS RAG                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  INDEXACI√ìN (una vez por documento)                             ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Documento.txt ‚îÄ‚îÄ‚ñ∂ [Chunking] ‚îÄ‚îÄ‚ñ∂ ["chunk1", "chunk2", ...]    ‚îÇ
‚îÇ                          ‚îÇ                                      ‚îÇ
‚îÇ                          ‚ñº                                      ‚îÇ
‚îÇ              [Embedding Model]                                  ‚îÇ
‚îÇ                          ‚îÇ                                      ‚îÇ
‚îÇ                          ‚ñº                                      ‚îÇ
‚îÇ              [[0.1, 0.3, ...], [0.2, 0.5, ...], ...]           ‚îÇ
‚îÇ                          ‚îÇ                                      ‚îÇ
‚îÇ                          ‚ñº                                      ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ              ‚îÇ    Vector Store     ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îÇchunk_id‚îÇembedding‚îÇ ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ  c1   ‚îÇ[0.1,..]‚îÇ ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ  c2   ‚îÇ[0.2,..]‚îÇ ‚îÇ                           ‚îÇ
‚îÇ              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                           ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  CONSULTA (por cada pregunta)                                   ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  "¬øPol√≠tica devoluciones?" ‚îÄ‚îÄ‚ñ∂ [Embedding] ‚îÄ‚îÄ‚ñ∂ [0.15, 0.35,...]‚îÇ
‚îÇ                                                    ‚îÇ            ‚îÇ
‚îÇ                                                    ‚ñº            ‚îÇ
‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ                                    ‚îÇ B√∫squeda Similaridad ‚îÇ    ‚îÇ
‚îÇ                                    ‚îÇ   (Coseno/Dot)       ‚îÇ    ‚îÇ
‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                               ‚îÇ                 ‚îÇ
‚îÇ                                               ‚ñº                 ‚îÇ
‚îÇ                              Top-K Chunks: [c1: 0.92, c3: 0.87]‚îÇ
‚îÇ                                               ‚îÇ                 ‚îÇ
‚îÇ                                               ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PROMPT = Contexto + Query                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Contexto: [contenido de c1 y c3]                   ‚îÇ ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Pregunta: ¬øPol√≠tica devoluciones?                  ‚îÇ ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                ‚îÇ                                ‚îÇ
‚îÇ                                ‚ñº                                ‚îÇ
‚îÇ                        [Gemini Model]                          ‚îÇ
‚îÇ                                ‚îÇ                                ‚îÇ
‚îÇ                                ‚ñº                                ‚îÇ
‚îÇ  "Seg√∫n la pol√≠tica, se aceptan devoluciones en 30 d√≠as..."    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Ejercicio Pr√°ctico

### Construir un RAG para FAQ de Producto

```python
"""
EJERCICIO: Sistema RAG para FAQ de Producto
Completa las funciones marcadas con TODO
"""
import google.generativeai as genai


class ProductFAQRAG:
    """
    Sistema RAG especializado para FAQ de productos.
    """

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        # TODO: Inicializar componentes (processor, embedder, vector_store)
        pass

    def add_faq(self, question: str, answer: str, category: str = "general"):
        """
        Agrega un par pregunta-respuesta al sistema.

        TODO:
        1. Crear documento combinando question y answer
        2. Agregar metadata con la categor√≠a
        3. Indexar en el vector store
        """
        pass

    def bulk_add_faqs(self, faqs: List[Dict]):
        """
        Agrega m√∫ltiples FAQs de una vez.

        faqs = [
            {"question": "...", "answer": "...", "category": "..."},
            ...
        ]

        TODO: Iterar y agregar cada FAQ
        """
        pass

    def search_faq(self, user_question: str, top_k: int = 3) -> List[Dict]:
        """
        Busca FAQs similares a la pregunta del usuario.

        TODO:
        1. Generar embedding de la pregunta
        2. Buscar en vector store
        3. Retornar resultados con scores
        """
        pass

    def answer_question(self, user_question: str) -> str:
        """
        Responde una pregunta usando el contexto del FAQ.

        TODO:
        1. Buscar FAQs relevantes
        2. Construir contexto
        3. Generar respuesta con Gemini
        """
        pass


def test_faq_rag():
    """Prueba el sistema FAQ RAG."""

    faqs = [
        {
            "question": "¬øC√≥mo puedo resetear mi contrase√±a?",
            "answer": "Para resetear tu contrase√±a, ve a Configuraci√≥n > Seguridad > Cambiar Contrase√±a. Recibir√°s un c√≥digo por email.",
            "category": "cuenta"
        },
        {
            "question": "¬øCu√°nto cuesta la suscripci√≥n premium?",
            "answer": "La suscripci√≥n premium cuesta $9.99/mes o $99/a√±o. Incluye almacenamiento ilimitado y soporte prioritario.",
            "category": "facturaci√≥n"
        },
        {
            "question": "¬øPuedo cancelar mi suscripci√≥n en cualquier momento?",
            "answer": "S√≠, puedes cancelar cuando quieras desde Configuraci√≥n > Suscripci√≥n > Cancelar. No hay penalizaciones.",
            "category": "facturaci√≥n"
        },
        {
            "question": "¬øLa app funciona sin conexi√≥n?",
            "answer": "S√≠, la app tiene modo offline. Los datos se sincronizar√°n autom√°ticamente cuando recuperes conexi√≥n.",
            "category": "funcionalidades"
        },
        {
            "question": "¬øC√≥mo exporto mis datos?",
            "answer": "Ve a Configuraci√≥n > Datos > Exportar. Puedes elegir formato CSV o JSON. El archivo se enviar√° a tu email.",
            "category": "funcionalidades"
        }
    ]

    # TODO: Crear instancia y probar
    # faq_rag = ProductFAQRAG(api_key="TU_API_KEY")
    # faq_rag.bulk_add_faqs(faqs)
    #
    # preguntas_test = [
    #     "Olvid√© mi password",
    #     "¬øCu√°nto sale el plan anual?",
    #     "¬øFunciona offline la aplicaci√≥n?"
    # ]
    #
    # for pregunta in preguntas_test:
    #     respuesta = faq_rag.answer_question(pregunta)
    #     print(f"P: {pregunta}")
    #     print(f"R: {respuesta}\n")


if __name__ == "__main__":
    test_faq_rag()
```

## Mejores Pr√°cticas

### Configuraci√≥n √ìptima

| Par√°metro | Valor Recomendado | Notas |
|-----------|-------------------|-------|
| `chunk_size` | 300-500 chars | M√°s peque√±o = m√°s preciso, m√°s grande = m√°s contexto |
| `chunk_overlap` | 10-20% del chunk_size | Evita perder informaci√≥n en los bordes |
| `top_k` | 3-5 | Balance entre relevancia y ruido |
| `score_threshold` | 0.7-0.8 | Filtrar resultados poco relevantes |

### Checklist de Implementaci√≥n

- [ ] Documentos preprocesados y limpios
- [ ] Estrategia de chunking definida
- [ ] Embeddings generados con task_type correcto
- [ ] Vector store con persistencia
- [ ] Prompts de generaci√≥n optimizados
- [ ] M√©tricas de monitoreo implementadas
- [ ] Manejo de casos sin resultados

## Resumen

En este subtema aprendiste:

1. **Arquitectura RAG**: Los dos flujos principales (indexaci√≥n y retrieval)
2. **Componentes**: Document processor, embeddings, vector store
3. **Pipeline completo**: Integraci√≥n de todos los componentes
4. **M√©tricas**: C√≥mo evaluar la calidad del sistema

## Siguiente Paso

En el pr√≥ximo subtema exploraremos **Embeddings con Gemini**, profundizando en c√≥mo generar y optimizar embeddings para diferentes casos de uso.
