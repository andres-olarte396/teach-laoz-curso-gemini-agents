# 6.3.2 Evaluación de Calidad de RAG

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar sistemas de evaluación para medir y mejorar la calidad de sistemas RAG usando métricas de retrieval y generación.

## Introducción

Evaluar un sistema RAG requiere medir dos componentes:
1. **Retrieval**: ¿Se recuperan los documentos correctos?
2. **Generation**: ¿La respuesta generada es correcta y útil?

### Dimensiones de Evaluación

```
┌─────────────────────────────────────────────────────────────────┐
│                    DIMENSIONES DE CALIDAD RAG                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  RETRIEVAL                           GENERATION                  │
│  ─────────                           ──────────                  │
│  ┌───────────────────┐               ┌───────────────────┐      │
│  │ Precision         │               │ Faithfulness      │      │
│  │ ¿Docs relevantes? │               │ ¿Fiel al contexto?│      │
│  └───────────────────┘               └───────────────────┘      │
│  ┌───────────────────┐               ┌───────────────────┐      │
│  │ Recall            │               │ Relevance         │      │
│  │ ¿Todos los docs?  │               │ ¿Responde query?  │      │
│  └───────────────────┘               └───────────────────┘      │
│  ┌───────────────────┐               ┌───────────────────┐      │
│  │ MRR               │               │ Completeness      │      │
│  │ ¿Ranking correcto?│               │ ¿Respuesta completa?│    │
│  └───────────────────┘               └───────────────────┘      │
│                                      ┌───────────────────┐      │
│                                      │ Hallucination     │      │
│                                      │ ¿Inventa datos?   │      │
│                                      └───────────────────┘      │
│                                                                  │
│  END-TO-END                                                      │
│  ───────────                                                     │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ Answer Correctness: ¿La respuesta final es correcta?    │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Métricas de Retrieval

### Implementación de Métricas Básicas

```python
"""
Métricas de evaluación para el componente de retrieval
"""
from typing import List, Set, Dict
import numpy as np


class RetrievalMetrics:
    """
    Métricas para evaluar la calidad del retrieval.
    """

    @staticmethod
    def precision_at_k(
        retrieved: List[str],
        relevant: Set[str],
        k: int
    ) -> float:
        """
        Precision@K: Proporción de documentos relevantes en los top-K.

        Args:
            retrieved: Lista ordenada de IDs recuperados
            relevant: Set de IDs relevantes (ground truth)
            k: Número de documentos a considerar

        Returns:
            Precision en el rango [0, 1]
        """
        retrieved_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc in retrieved_k if doc in relevant)
        return relevant_retrieved / k if k > 0 else 0.0

    @staticmethod
    def recall_at_k(
        retrieved: List[str],
        relevant: Set[str],
        k: int
    ) -> float:
        """
        Recall@K: Proporción de documentos relevantes recuperados.

        Args:
            retrieved: Lista ordenada de IDs recuperados
            relevant: Set de IDs relevantes

        Returns:
            Recall en el rango [0, 1]
        """
        if not relevant:
            return 0.0

        retrieved_k = set(retrieved[:k])
        relevant_retrieved = len(retrieved_k.intersection(relevant))
        return relevant_retrieved / len(relevant)

    @staticmethod
    def f1_at_k(
        retrieved: List[str],
        relevant: Set[str],
        k: int
    ) -> float:
        """
        F1@K: Media armónica de Precision y Recall.
        """
        precision = RetrievalMetrics.precision_at_k(retrieved, relevant, k)
        recall = RetrievalMetrics.recall_at_k(retrieved, relevant, k)

        if precision + recall == 0:
            return 0.0

        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def mrr(
        retrieved: List[str],
        relevant: Set[str]
    ) -> float:
        """
        Mean Reciprocal Rank: 1 / posición del primer documento relevante.

        Mide qué tan arriba aparece el primer resultado relevante.
        """
        for i, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                return 1.0 / i
        return 0.0

    @staticmethod
    def average_precision(
        retrieved: List[str],
        relevant: Set[str]
    ) -> float:
        """
        Average Precision: Promedio de precisión en cada posición relevante.
        """
        if not relevant:
            return 0.0

        precisions = []
        relevant_found = 0

        for i, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                relevant_found += 1
                precisions.append(relevant_found / i)

        return sum(precisions) / len(relevant) if precisions else 0.0

    @staticmethod
    def ndcg_at_k(
        retrieved: List[str],
        relevance_scores: Dict[str, float],
        k: int
    ) -> float:
        """
        Normalized Discounted Cumulative Gain.

        Args:
            retrieved: Lista ordenada de IDs recuperados
            relevance_scores: Dict con scores de relevancia por documento
            k: Número de documentos a considerar

        Returns:
            NDCG en el rango [0, 1]
        """
        def dcg(scores: List[float]) -> float:
            return sum(
                score / np.log2(i + 2)
                for i, score in enumerate(scores)
            )

        # DCG de los resultados actuales
        retrieved_scores = [
            relevance_scores.get(doc_id, 0)
            for doc_id in retrieved[:k]
        ]
        actual_dcg = dcg(retrieved_scores)

        # DCG ideal (documentos ordenados por relevancia)
        ideal_scores = sorted(relevance_scores.values(), reverse=True)[:k]
        ideal_dcg = dcg(ideal_scores)

        return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0


# Ejemplo de uso
def demo_retrieval_metrics():
    # Resultados de búsqueda (ordenados por relevancia estimada)
    retrieved = ["doc_3", "doc_1", "doc_5", "doc_7", "doc_2", "doc_4"]

    # Ground truth: documentos realmente relevantes
    relevant = {"doc_1", "doc_2", "doc_6"}

    # Scores de relevancia (para NDCG)
    relevance_scores = {
        "doc_1": 3.0,  # Muy relevante
        "doc_2": 2.0,  # Relevante
        "doc_6": 1.0,  # Algo relevante
    }

    print("Métricas de Retrieval:")
    print(f"  Precision@3: {RetrievalMetrics.precision_at_k(retrieved, relevant, 3):.3f}")
    print(f"  Recall@3: {RetrievalMetrics.recall_at_k(retrieved, relevant, 3):.3f}")
    print(f"  F1@3: {RetrievalMetrics.f1_at_k(retrieved, relevant, 3):.3f}")
    print(f"  MRR: {RetrievalMetrics.mrr(retrieved, relevant):.3f}")
    print(f"  MAP: {RetrievalMetrics.average_precision(retrieved, relevant):.3f}")
    print(f"  NDCG@5: {RetrievalMetrics.ndcg_at_k(retrieved, relevance_scores, 5):.3f}")


if __name__ == "__main__":
    demo_retrieval_metrics()
```

## Métricas de Generación con LLM-as-Judge

### Evaluación con Gemini

```python
"""
Evaluación de generación usando Gemini como juez
"""
import google.generativeai as genai
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import re


@dataclass
class GenerationEvaluation:
    """Resultados de evaluación de generación."""
    faithfulness: float  # ¿Fiel al contexto?
    relevance: float     # ¿Responde la pregunta?
    completeness: float  # ¿Respuesta completa?
    coherence: float     # ¿Texto coherente?
    hallucination_score: float  # 0 = no alucinaciones
    overall_score: float
    explanation: str


class GenerationEvaluator:
    """
    Evalúa la calidad de respuestas generadas usando Gemini.
    """

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")

    def evaluate_faithfulness(
        self,
        context: str,
        answer: str
    ) -> tuple[float, str]:
        """
        Evalúa si la respuesta es fiel al contexto (no inventa información).
        """
        prompt = f"""Evalúa si la respuesta está completamente soportada por el contexto.

CONTEXTO:
{context}

RESPUESTA:
{answer}

Evalúa:
1. ¿Cada afirmación en la respuesta está soportada por el contexto?
2. ¿Hay información que no aparece en el contexto?
3. ¿Hay contradicciones con el contexto?

Responde en JSON:
{{
    "score": 0.0 a 1.0 (1.0 = completamente fiel),
    "unsupported_claims": ["lista de afirmaciones no soportadas"],
    "explanation": "breve explicación"
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return result.get("score", 0.5), result.get("explanation", "")
        except:
            return 0.5, "Error en evaluación"

    def evaluate_relevance(
        self,
        question: str,
        answer: str
    ) -> tuple[float, str]:
        """
        Evalúa si la respuesta es relevante para la pregunta.
        """
        prompt = f"""Evalúa si la respuesta responde directamente la pregunta.

PREGUNTA:
{question}

RESPUESTA:
{answer}

Evalúa:
1. ¿La respuesta aborda directamente la pregunta?
2. ¿La respuesta contiene información relevante para la pregunta?
3. ¿Hay divagaciones o información innecesaria?

Responde en JSON:
{{
    "score": 0.0 a 1.0 (1.0 = perfectamente relevante),
    "addresses_question": true/false,
    "explanation": "breve explicación"
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return result.get("score", 0.5), result.get("explanation", "")
        except:
            return 0.5, "Error en evaluación"

    def evaluate_completeness(
        self,
        question: str,
        context: str,
        answer: str
    ) -> tuple[float, str]:
        """
        Evalúa si la respuesta es completa.
        """
        prompt = f"""Evalúa si la respuesta es completa considerando la información disponible.

PREGUNTA:
{question}

CONTEXTO DISPONIBLE:
{context}

RESPUESTA:
{answer}

Evalúa:
1. ¿La respuesta incluye toda la información relevante del contexto?
2. ¿Faltan aspectos importantes de la pregunta?
3. ¿El nivel de detalle es apropiado?

Responde en JSON:
{{
    "score": 0.0 a 1.0 (1.0 = completamente completa),
    "missing_aspects": ["aspectos faltantes si los hay"],
    "explanation": "breve explicación"
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return result.get("score", 0.5), result.get("explanation", "")
        except:
            return 0.5, "Error en evaluación"

    def detect_hallucinations(
        self,
        context: str,
        answer: str
    ) -> tuple[float, List[str]]:
        """
        Detecta alucinaciones (información inventada).
        """
        prompt = f"""Identifica cualquier información en la respuesta que NO esté en el contexto.

CONTEXTO:
{context}

RESPUESTA:
{answer}

Busca:
1. Hechos específicos (fechas, números, nombres) no mencionados en el contexto
2. Afirmaciones que contradicen el contexto
3. Información que parece inventada o asumida

Responde en JSON:
{{
    "hallucination_score": 0.0 a 1.0 (0.0 = sin alucinaciones),
    "hallucinated_content": ["lista de contenido posiblemente alucinado"],
    "confidence": 0.0 a 1.0
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return (
                result.get("hallucination_score", 0.5),
                result.get("hallucinated_content", [])
            )
        except:
            return 0.5, []

    def evaluate_full(
        self,
        question: str,
        context: str,
        answer: str
    ) -> GenerationEvaluation:
        """
        Evaluación completa de una respuesta generada.
        """
        # Evaluar cada dimensión
        faithfulness, faith_exp = self.evaluate_faithfulness(context, answer)
        relevance, rel_exp = self.evaluate_relevance(question, answer)
        completeness, comp_exp = self.evaluate_completeness(question, context, answer)
        halluc_score, halluc_content = self.detect_hallucinations(context, answer)

        # Coherencia simple (basada en estructura)
        coherence = self._evaluate_coherence(answer)

        # Score general (ponderado)
        overall = (
            faithfulness * 0.3 +
            relevance * 0.25 +
            completeness * 0.2 +
            coherence * 0.1 +
            (1 - halluc_score) * 0.15
        )

        explanation = f"""
Faithfulness ({faithfulness:.2f}): {faith_exp}
Relevance ({relevance:.2f}): {rel_exp}
Completeness ({completeness:.2f}): {comp_exp}
Hallucinations: {halluc_content if halluc_content else 'None detected'}
"""

        return GenerationEvaluation(
            faithfulness=faithfulness,
            relevance=relevance,
            completeness=completeness,
            coherence=coherence,
            hallucination_score=halluc_score,
            overall_score=overall,
            explanation=explanation.strip()
        )

    def _evaluate_coherence(self, text: str) -> float:
        """Evaluación simple de coherencia."""
        # Indicadores básicos
        has_structure = len(text.split('\n')) > 1 or len(text.split('. ')) > 2
        reasonable_length = 20 < len(text.split()) < 500
        starts_properly = text[0].isupper() if text else False
        ends_properly = text.rstrip()[-1] in '.!?' if text.rstrip() else False

        score = sum([has_structure, reasonable_length, starts_properly, ends_properly]) / 4
        return score


# Ejemplo de uso
def demo_generation_evaluation():
    evaluator = GenerationEvaluator(api_key="TU_API_KEY")

    question = "¿Cuál es la política de devoluciones?"

    context = """
    Nuestra política de devoluciones permite devolver productos dentro de 30 días
    desde la fecha de compra. El producto debe estar en su empaque original y
    sin usar. Los reembolsos se procesan en 5-7 días hábiles.
    """

    # Respuesta buena
    good_answer = """
    La política de devoluciones permite devolver productos dentro de 30 días
    desde la compra. El producto debe estar en su empaque original y sin uso.
    Los reembolsos tardan entre 5 y 7 días hábiles en procesarse.
    """

    # Respuesta con alucinaciones
    bad_answer = """
    La política de devoluciones permite devolver productos dentro de 60 días.
    Puede obtener un reembolso instantáneo en la tienda física más cercana.
    Además, ofrecemos un bono del 10% en compras futuras.
    """

    print("=== Evaluación de respuesta BUENA ===")
    eval_good = evaluator.evaluate_full(question, context, good_answer)
    print(f"Overall Score: {eval_good.overall_score:.2f}")
    print(f"Faithfulness: {eval_good.faithfulness:.2f}")
    print(f"Hallucinations: {eval_good.hallucination_score:.2f}")
    print(eval_good.explanation)

    print("\n=== Evaluación de respuesta MALA ===")
    eval_bad = evaluator.evaluate_full(question, context, bad_answer)
    print(f"Overall Score: {eval_bad.overall_score:.2f}")
    print(f"Faithfulness: {eval_bad.faithfulness:.2f}")
    print(f"Hallucinations: {eval_bad.hallucination_score:.2f}")
    print(eval_bad.explanation)


if __name__ == "__main__":
    demo_generation_evaluation()
```

## Evaluación End-to-End

```python
"""
Evaluación completa del sistema RAG (retrieval + generation)
"""
from dataclasses import dataclass
from typing import List, Dict, Optional


@dataclass
class RAGEvaluationResult:
    """Resultado completo de evaluación RAG."""
    # Métricas de Retrieval
    retrieval_precision: float
    retrieval_recall: float
    retrieval_mrr: float

    # Métricas de Generación
    generation_faithfulness: float
    generation_relevance: float
    generation_completeness: float
    hallucination_score: float

    # Métricas End-to-End
    answer_correctness: float
    overall_score: float

    # Detalles
    query: str
    retrieved_docs: List[str]
    generated_answer: str
    ground_truth_answer: Optional[str]


class RAGEvaluator:
    """
    Evaluador end-to-end para sistemas RAG.
    """

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")
        self.retrieval_metrics = RetrievalMetrics()
        self.generation_evaluator = GenerationEvaluator(api_key)

    def evaluate_answer_correctness(
        self,
        question: str,
        generated_answer: str,
        ground_truth: str
    ) -> float:
        """
        Evalúa qué tan correcta es la respuesta comparada con ground truth.
        """
        prompt = f"""Compara la respuesta generada con la respuesta correcta.

PREGUNTA:
{question}

RESPUESTA GENERADA:
{generated_answer}

RESPUESTA CORRECTA (Ground Truth):
{ground_truth}

Evalúa:
1. ¿La respuesta generada transmite la misma información que la correcta?
2. ¿Hay errores factuales en la respuesta generada?
3. ¿La respuesta generada omite información importante?

Responde en JSON:
{{
    "correctness_score": 0.0 a 1.0,
    "factual_errors": ["lista de errores si hay"],
    "missing_info": ["información omitida"],
    "extra_info": ["información adicional (puede ser positivo)"]
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )
            return result.get("correctness_score", 0.5)
        except:
            return 0.5

    def evaluate(
        self,
        query: str,
        retrieved_doc_ids: List[str],
        retrieved_contexts: List[str],
        generated_answer: str,
        relevant_doc_ids: Set[str] = None,
        ground_truth_answer: str = None
    ) -> RAGEvaluationResult:
        """
        Evaluación completa del sistema RAG.
        """
        # Métricas de Retrieval (si hay ground truth de docs)
        if relevant_doc_ids:
            ret_precision = self.retrieval_metrics.precision_at_k(
                retrieved_doc_ids, relevant_doc_ids, 5
            )
            ret_recall = self.retrieval_metrics.recall_at_k(
                retrieved_doc_ids, relevant_doc_ids, 5
            )
            ret_mrr = self.retrieval_metrics.mrr(
                retrieved_doc_ids, relevant_doc_ids
            )
        else:
            ret_precision = ret_recall = ret_mrr = None

        # Métricas de Generación
        combined_context = "\n\n".join(retrieved_contexts)
        gen_eval = self.generation_evaluator.evaluate_full(
            query, combined_context, generated_answer
        )

        # Correctness (si hay ground truth de respuesta)
        if ground_truth_answer:
            correctness = self.evaluate_answer_correctness(
                query, generated_answer, ground_truth_answer
            )
        else:
            correctness = None

        # Score overall
        scores = [
            gen_eval.faithfulness,
            gen_eval.relevance,
            gen_eval.completeness,
            1 - gen_eval.hallucination_score
        ]

        if ret_precision is not None:
            scores.extend([ret_precision, ret_recall])

        if correctness is not None:
            scores.append(correctness)

        overall = sum(scores) / len(scores)

        return RAGEvaluationResult(
            retrieval_precision=ret_precision,
            retrieval_recall=ret_recall,
            retrieval_mrr=ret_mrr,
            generation_faithfulness=gen_eval.faithfulness,
            generation_relevance=gen_eval.relevance,
            generation_completeness=gen_eval.completeness,
            hallucination_score=gen_eval.hallucination_score,
            answer_correctness=correctness,
            overall_score=overall,
            query=query,
            retrieved_docs=retrieved_doc_ids,
            generated_answer=generated_answer,
            ground_truth_answer=ground_truth_answer
        )


class RAGBenchmark:
    """
    Benchmark para evaluar sistema RAG con múltiples casos de prueba.
    """

    def __init__(self, api_key: str):
        self.evaluator = RAGEvaluator(api_key)
        self.results: List[RAGEvaluationResult] = []

    def add_test_case(
        self,
        query: str,
        relevant_doc_ids: Set[str] = None,
        ground_truth_answer: str = None
    ):
        """Agrega un caso de prueba al benchmark."""
        self.test_cases.append({
            "query": query,
            "relevant_doc_ids": relevant_doc_ids,
            "ground_truth_answer": ground_truth_answer
        })

    def run(
        self,
        rag_system,  # Tu sistema RAG
        test_cases: List[Dict]
    ) -> Dict[str, float]:
        """
        Ejecuta el benchmark completo.

        Returns:
            Dict con métricas agregadas
        """
        self.results = []

        for case in test_cases:
            # Ejecutar RAG
            rag_response = rag_system.query(case["query"])

            # Evaluar
            result = self.evaluator.evaluate(
                query=case["query"],
                retrieved_doc_ids=rag_response.get("doc_ids", []),
                retrieved_contexts=rag_response.get("contexts", []),
                generated_answer=rag_response.get("answer", ""),
                relevant_doc_ids=case.get("relevant_doc_ids"),
                ground_truth_answer=case.get("ground_truth_answer")
            )

            self.results.append(result)

        # Calcular métricas agregadas
        return self._aggregate_results()

    def _aggregate_results(self) -> Dict[str, float]:
        """Agrega resultados de todos los casos de prueba."""
        if not self.results:
            return {}

        metrics = {
            "avg_overall_score": np.mean([r.overall_score for r in self.results]),
            "avg_faithfulness": np.mean([r.generation_faithfulness for r in self.results]),
            "avg_relevance": np.mean([r.generation_relevance for r in self.results]),
            "avg_completeness": np.mean([r.generation_completeness for r in self.results]),
            "avg_hallucination": np.mean([r.hallucination_score for r in self.results]),
        }

        # Métricas de retrieval (si están disponibles)
        ret_precisions = [r.retrieval_precision for r in self.results if r.retrieval_precision is not None]
        if ret_precisions:
            metrics["avg_retrieval_precision"] = np.mean(ret_precisions)
            metrics["avg_retrieval_recall"] = np.mean([
                r.retrieval_recall for r in self.results if r.retrieval_recall is not None
            ])

        # Correctness (si está disponible)
        correctness = [r.answer_correctness for r in self.results if r.answer_correctness is not None]
        if correctness:
            metrics["avg_answer_correctness"] = np.mean(correctness)

        return metrics

    def generate_report(self) -> str:
        """Genera reporte legible de resultados."""
        metrics = self._aggregate_results()

        report = """
═══════════════════════════════════════════════════════════
                    RAG EVALUATION REPORT
═══════════════════════════════════════════════════════════

OVERALL METRICS
───────────────
"""
        for name, value in metrics.items():
            report += f"  {name}: {value:.3f}\n"

        report += f"""
INDIVIDUAL RESULTS
──────────────────
Total test cases: {len(self.results)}

"""
        for i, result in enumerate(self.results, 1):
            report += f"""
Case {i}: {result.query[:50]}...
  Overall: {result.overall_score:.2f}
  Faithfulness: {result.generation_faithfulness:.2f}
  Relevance: {result.generation_relevance:.2f}
  Hallucination: {result.hallucination_score:.2f}
"""

        return report
```

## Dataset de Evaluación

```python
"""
Creación de dataset de evaluación para RAG
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
import json


@dataclass
class EvalExample:
    """Ejemplo de evaluación."""
    query: str
    relevant_docs: List[str]  # IDs o contenido
    ground_truth_answer: str
    metadata: Dict = None


class EvalDatasetBuilder:
    """
    Constructor de datasets de evaluación para RAG.
    """

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash")

    def generate_questions_from_docs(
        self,
        documents: List[Dict],
        questions_per_doc: int = 3
    ) -> List[EvalExample]:
        """
        Genera preguntas de evaluación a partir de documentos.
        """
        examples = []

        for doc in documents:
            prompt = f"""Genera {questions_per_doc} preguntas que se puedan responder
usando el siguiente documento. Para cada pregunta, proporciona también
la respuesta correcta basada en el documento.

DOCUMENTO:
{doc['content']}

Genera preguntas variadas (factuales, conceptuales, de detalle).

Responde en JSON:
{{
    "questions": [
        {{
            "question": "pregunta 1",
            "answer": "respuesta correcta basada en el documento"
        }},
        ...
    ]
}}"""

            response = self.model.generate_content(prompt)

            try:
                result = json.loads(
                    response.text[response.text.find('{'):response.text.rfind('}')+1]
                )

                for qa in result.get("questions", []):
                    examples.append(EvalExample(
                        query=qa["question"],
                        relevant_docs=[doc.get("id", "unknown")],
                        ground_truth_answer=qa["answer"],
                        metadata={"source_doc": doc.get("id")}
                    ))

            except Exception as e:
                print(f"Error generando preguntas: {e}")

        return examples

    def create_multi_doc_questions(
        self,
        documents: List[Dict],
        num_questions: int = 5
    ) -> List[EvalExample]:
        """
        Crea preguntas que requieren información de múltiples documentos.
        """
        # Combinar contenido de documentos
        combined = "\n\n---\n\n".join([
            f"[Doc {i+1}]: {doc['content']}"
            for i, doc in enumerate(documents)
        ])

        prompt = f"""Genera {num_questions} preguntas complejas que requieran
combinar información de múltiples documentos para responder.

DOCUMENTOS:
{combined}

Para cada pregunta, indica qué documentos son necesarios para responder.

Responde en JSON:
{{
    "questions": [
        {{
            "question": "pregunta que requiere múltiples docs",
            "answer": "respuesta que combina información",
            "required_docs": [1, 3]
        }},
        ...
    ]
}}"""

        response = self.model.generate_content(prompt)

        try:
            result = json.loads(
                response.text[response.text.find('{'):response.text.rfind('}')+1]
            )

            examples = []
            for qa in result.get("questions", []):
                doc_indices = qa.get("required_docs", [])
                relevant_ids = [
                    documents[i-1].get("id") for i in doc_indices
                    if 0 < i <= len(documents)
                ]

                examples.append(EvalExample(
                    query=qa["question"],
                    relevant_docs=relevant_ids,
                    ground_truth_answer=qa["answer"],
                    metadata={"type": "multi_doc"}
                ))

            return examples

        except Exception as e:
            print(f"Error: {e}")
            return []

    def save_dataset(
        self,
        examples: List[EvalExample],
        filepath: str
    ):
        """Guarda dataset en formato JSON."""
        data = [
            {
                "query": ex.query,
                "relevant_docs": ex.relevant_docs,
                "ground_truth_answer": ex.ground_truth_answer,
                "metadata": ex.metadata
            }
            for ex in examples
        ]

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def load_dataset(self, filepath: str) -> List[EvalExample]:
        """Carga dataset desde JSON."""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        return [
            EvalExample(
                query=item["query"],
                relevant_docs=item["relevant_docs"],
                ground_truth_answer=item["ground_truth_answer"],
                metadata=item.get("metadata")
            )
            for item in data
        ]
```

## Ejercicio Práctico

```python
"""
EJERCICIO: Sistema de Monitoreo de Calidad RAG

Implementa un sistema de monitoreo continuo que evalúe
la calidad del RAG en producción.
"""


class RAGQualityMonitor:
    """
    Monitor de calidad para sistemas RAG en producción.

    TODO: Implementar las funcionalidades.
    """

    def __init__(self, api_key: str, alert_threshold: float = 0.6):
        """
        TODO:
        1. Inicializar evaluador
        2. Configurar umbrales de alerta
        3. Inicializar almacenamiento de métricas
        """
        pass

    def log_interaction(
        self,
        query: str,
        retrieved_docs: List[Dict],
        generated_answer: str,
        user_feedback: Optional[float] = None
    ):
        """
        Registra una interacción para evaluación.

        TODO:
        1. Evaluar la interacción
        2. Almacenar métricas
        3. Verificar umbrales
        4. Generar alertas si es necesario
        """
        pass

    def get_metrics_summary(
        self,
        time_window: str = "1h"
    ) -> Dict[str, float]:
        """
        Obtiene resumen de métricas en ventana de tiempo.

        TODO:
        1. Filtrar métricas por tiempo
        2. Calcular estadísticas
        3. Identificar tendencias
        """
        pass

    def detect_quality_degradation(self) -> List[Dict]:
        """
        Detecta degradación de calidad.

        TODO:
        1. Comparar métricas recientes vs históricas
        2. Identificar caídas significativas
        3. Generar alertas
        """
        pass

    def generate_quality_report(
        self,
        period: str = "daily"
    ) -> str:
        """
        Genera reporte de calidad.

        TODO:
        1. Agregar métricas del período
        2. Incluir tendencias
        3. Listar alertas
        4. Sugerir mejoras
        """
        pass


def test_quality_monitor():
    """Prueba el monitor de calidad."""
    # TODO: Implementar pruebas
    pass
```

## Resumen

| Métrica | Qué Mide | Rango |
|---------|----------|-------|
| Precision@K | Relevancia de top-K | 0-1 |
| Recall@K | Cobertura de relevantes | 0-1 |
| MRR | Posición del primer relevante | 0-1 |
| Faithfulness | Fidelidad al contexto | 0-1 |
| Hallucination | Información inventada | 0-1 (menor mejor) |
| Answer Correctness | Corrección vs ground truth | 0-1 |

### Checklist de Evaluación

- [ ] Crear dataset de evaluación con ground truth
- [ ] Implementar métricas de retrieval
- [ ] Implementar evaluación de generación
- [ ] Configurar evaluación LLM-as-Judge
- [ ] Establecer umbrales de calidad
- [ ] Implementar monitoreo continuo

## Siguiente Paso

En el próximo subtema exploraremos **Actualización Incremental de Índices**, técnicas para mantener la base de conocimiento actualizada sin re-indexación completa.
