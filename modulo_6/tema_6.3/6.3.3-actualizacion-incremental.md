# 6.3.3 Actualización Incremental de Índices

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar estrategias de actualización incremental para mantener índices RAG actualizados sin necesidad de re-indexación completa.

## Introducción

En producción, los datos cambian constantemente. Mantener el índice RAG actualizado es crucial para la relevancia de las respuestas. La **actualización incremental** permite agregar, modificar y eliminar documentos sin reconstruir todo el índice.

### Desafíos de Actualización

```
┌─────────────────────────────────────────────────────────────────┐
│                    DESAFÍOS DE ACTUALIZACIÓN                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  DATOS CAMBIANTES                                               │
│  ├── Nuevos documentos agregados constantemente                 │
│  ├── Documentos existentes modificados                          │
│  ├── Documentos obsoletos que deben eliminarse                  │
│  └── Versiones que deben coexistir                              │
│                                                                  │
│  PROBLEMAS DE RE-INDEXACIÓN COMPLETA                            │
│  ├── Tiempo: Horas/días para índices grandes                    │
│  ├── Costo: Regenerar millones de embeddings                    │
│  ├── Disponibilidad: Downtime durante la actualización         │
│  └── Recursos: Alto consumo de memoria/CPU                       │
│                                                                  │
│  SOLUCIÓN: ACTUALIZACIÓN INCREMENTAL                            │
│  ├── Solo procesar cambios (delta)                              │
│  ├── Sin downtime                                                │
│  ├── Costo proporcional a cambios                               │
│  └── Actualizaciones frecuentes posibles                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Arquitectura de Actualización Incremental

### Sistema de Tracking de Cambios

```python
"""
Sistema de tracking de cambios para documentos
"""
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Set
from dataclasses import dataclass, field
from enum import Enum
import json


class ChangeType(Enum):
    """Tipos de cambios en documentos."""
    ADDED = "added"
    MODIFIED = "modified"
    DELETED = "deleted"


@dataclass
class DocumentVersion:
    """Versión de un documento con metadata."""
    doc_id: str
    content_hash: str
    version: int
    created_at: datetime
    modified_at: datetime
    is_active: bool = True
    metadata: Dict = field(default_factory=dict)


@dataclass
class DocumentChange:
    """Representa un cambio detectado."""
    doc_id: str
    change_type: ChangeType
    old_hash: Optional[str] = None
    new_hash: Optional[str] = None
    detected_at: datetime = field(default_factory=datetime.now)


class DocumentTracker:
    """
    Sistema de tracking de documentos para detectar cambios.
    """

    def __init__(self, storage_path: str = "document_tracker.json"):
        self.storage_path = storage_path
        self.documents: Dict[str, DocumentVersion] = {}
        self._load_state()

    def _compute_hash(self, content: str) -> str:
        """Calcula hash del contenido."""
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def _load_state(self):
        """Carga estado desde disco."""
        try:
            with open(self.storage_path, 'r') as f:
                data = json.load(f)
                for doc_id, info in data.items():
                    self.documents[doc_id] = DocumentVersion(
                        doc_id=doc_id,
                        content_hash=info['content_hash'],
                        version=info['version'],
                        created_at=datetime.fromisoformat(info['created_at']),
                        modified_at=datetime.fromisoformat(info['modified_at']),
                        is_active=info['is_active'],
                        metadata=info.get('metadata', {})
                    )
        except FileNotFoundError:
            self.documents = {}

    def _save_state(self):
        """Guarda estado en disco."""
        data = {
            doc_id: {
                'content_hash': v.content_hash,
                'version': v.version,
                'created_at': v.created_at.isoformat(),
                'modified_at': v.modified_at.isoformat(),
                'is_active': v.is_active,
                'metadata': v.metadata
            }
            for doc_id, v in self.documents.items()
        }
        with open(self.storage_path, 'w') as f:
            json.dump(data, f, indent=2)

    def detect_changes(
        self,
        current_documents: List[Dict[str, str]]
    ) -> List[DocumentChange]:
        """
        Detecta cambios comparando documentos actuales con el estado guardado.

        Args:
            current_documents: Lista de dicts con 'id' y 'content'

        Returns:
            Lista de cambios detectados
        """
        changes = []
        current_ids = set()

        # Detectar agregados y modificados
        for doc in current_documents:
            doc_id = doc['id']
            content_hash = self._compute_hash(doc['content'])
            current_ids.add(doc_id)

            if doc_id not in self.documents:
                # Documento nuevo
                changes.append(DocumentChange(
                    doc_id=doc_id,
                    change_type=ChangeType.ADDED,
                    new_hash=content_hash
                ))
            elif self.documents[doc_id].content_hash != content_hash:
                # Documento modificado
                changes.append(DocumentChange(
                    doc_id=doc_id,
                    change_type=ChangeType.MODIFIED,
                    old_hash=self.documents[doc_id].content_hash,
                    new_hash=content_hash
                ))

        # Detectar eliminados
        for doc_id in self.documents:
            if doc_id not in current_ids and self.documents[doc_id].is_active:
                changes.append(DocumentChange(
                    doc_id=doc_id,
                    change_type=ChangeType.DELETED,
                    old_hash=self.documents[doc_id].content_hash
                ))

        return changes

    def apply_changes(
        self,
        changes: List[DocumentChange],
        current_documents: Dict[str, str]
    ):
        """
        Aplica cambios al estado del tracker.

        Args:
            changes: Lista de cambios a aplicar
            current_documents: Dict de doc_id -> content
        """
        now = datetime.now()

        for change in changes:
            if change.change_type == ChangeType.ADDED:
                self.documents[change.doc_id] = DocumentVersion(
                    doc_id=change.doc_id,
                    content_hash=change.new_hash,
                    version=1,
                    created_at=now,
                    modified_at=now
                )
            elif change.change_type == ChangeType.MODIFIED:
                existing = self.documents[change.doc_id]
                existing.content_hash = change.new_hash
                existing.version += 1
                existing.modified_at = now
            elif change.change_type == ChangeType.DELETED:
                self.documents[change.doc_id].is_active = False
                self.documents[change.doc_id].modified_at = now

        self._save_state()

    def get_active_documents(self) -> Set[str]:
        """Retorna IDs de documentos activos."""
        return {
            doc_id for doc_id, v in self.documents.items()
            if v.is_active
        }


# Ejemplo de uso
def demo_document_tracker():
    tracker = DocumentTracker("./tracker_state.json")

    # Documentos iniciales
    initial_docs = [
        {"id": "doc1", "content": "Python es un lenguaje de programación"},
        {"id": "doc2", "content": "JavaScript es para desarrollo web"},
        {"id": "doc3", "content": "Machine Learning es parte de la IA"}
    ]

    # Primera sincronización
    changes = tracker.detect_changes(initial_docs)
    print(f"Cambios iniciales: {len(changes)} (todos ADDED)")
    tracker.apply_changes(changes, {d['id']: d['content'] for d in initial_docs})

    # Simular cambios
    updated_docs = [
        {"id": "doc1", "content": "Python es un lenguaje versátil"},  # Modificado
        {"id": "doc2", "content": "JavaScript es para desarrollo web"},  # Sin cambios
        # doc3 eliminado
        {"id": "doc4", "content": "Docker es para contenedores"}  # Nuevo
    ]

    changes = tracker.detect_changes(updated_docs)

    print(f"\nCambios detectados: {len(changes)}")
    for change in changes:
        print(f"  {change.doc_id}: {change.change_type.value}")
```

## Indexador Incremental

```python
"""
Indexador incremental para sistemas RAG
"""
import google.generativeai as genai
from typing import List, Dict, Optional
from datetime import datetime


class IncrementalIndexer:
    """
    Indexador que soporta actualizaciones incrementales.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,  # ChromaDB, Pinecone, etc.
        tracker: DocumentTracker
    ):
        genai.configure(api_key=api_key)
        self.vector_store = vector_store
        self.tracker = tracker

    def _get_embedding(self, text: str) -> List[float]:
        """Genera embedding para texto."""
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']

    def _chunk_document(self, content: str, doc_id: str) -> List[Dict]:
        """Divide documento en chunks."""
        # Chunking simple por párrafos
        paragraphs = content.split('\n\n')
        chunks = []

        for i, para in enumerate(paragraphs):
            if para.strip():
                chunks.append({
                    "chunk_id": f"{doc_id}_chunk_{i}",
                    "doc_id": doc_id,
                    "content": para.strip(),
                    "chunk_index": i
                })

        return chunks

    def sync(
        self,
        documents: List[Dict[str, str]]
    ) -> Dict[str, int]:
        """
        Sincroniza documentos con el índice.

        Args:
            documents: Lista de dicts con 'id' y 'content'

        Returns:
            Estadísticas de la sincronización
        """
        stats = {"added": 0, "modified": 0, "deleted": 0, "unchanged": 0}

        # Detectar cambios
        changes = self.tracker.detect_changes(documents)
        doc_contents = {d['id']: d['content'] for d in documents}

        if not changes:
            stats["unchanged"] = len(documents)
            return stats

        # Procesar cambios
        for change in changes:
            if change.change_type == ChangeType.ADDED:
                self._add_document(change.doc_id, doc_contents[change.doc_id])
                stats["added"] += 1

            elif change.change_type == ChangeType.MODIFIED:
                self._update_document(change.doc_id, doc_contents[change.doc_id])
                stats["modified"] += 1

            elif change.change_type == ChangeType.DELETED:
                self._delete_document(change.doc_id)
                stats["deleted"] += 1

        # Actualizar tracker
        self.tracker.apply_changes(changes, doc_contents)

        stats["unchanged"] = len(documents) - stats["added"] - stats["modified"]

        return stats

    def _add_document(self, doc_id: str, content: str):
        """Agrega un documento nuevo al índice."""
        chunks = self._chunk_document(content, doc_id)

        for chunk in chunks:
            embedding = self._get_embedding(chunk['content'])
            self.vector_store.add(
                ids=[chunk['chunk_id']],
                embeddings=[embedding],
                documents=[chunk['content']],
                metadatas=[{
                    "doc_id": doc_id,
                    "chunk_index": chunk['chunk_index'],
                    "indexed_at": datetime.now().isoformat()
                }]
            )

    def _update_document(self, doc_id: str, content: str):
        """Actualiza un documento existente."""
        # Eliminar chunks antiguos
        self._delete_document(doc_id)

        # Agregar nuevos chunks
        self._add_document(doc_id, content)

    def _delete_document(self, doc_id: str):
        """Elimina un documento del índice."""
        # Buscar todos los chunks del documento
        # La implementación depende del vector store

        # Para ChromaDB:
        try:
            # Obtener IDs de chunks del documento
            results = self.vector_store.get(
                where={"doc_id": doc_id}
            )
            if results['ids']:
                self.vector_store.delete(ids=results['ids'])
        except Exception as e:
            print(f"Error eliminando documento {doc_id}: {e}")


class BatchIncrementalIndexer(IncrementalIndexer):
    """
    Indexador incremental optimizado para procesamiento en batch.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,
        tracker: DocumentTracker,
        batch_size: int = 100
    ):
        super().__init__(api_key, vector_store, tracker)
        self.batch_size = batch_size

    def _batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Genera embeddings en batch."""
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=texts,
            task_type="retrieval_document"
        )
        return result['embedding']

    def sync(
        self,
        documents: List[Dict[str, str]]
    ) -> Dict[str, int]:
        """Sincronización optimizada con batching."""
        stats = {"added": 0, "modified": 0, "deleted": 0, "unchanged": 0}

        changes = self.tracker.detect_changes(documents)
        doc_contents = {d['id']: d['content'] for d in documents}

        if not changes:
            stats["unchanged"] = len(documents)
            return stats

        # Agrupar cambios por tipo
        to_add = []
        to_delete = []

        for change in changes:
            if change.change_type == ChangeType.DELETED:
                to_delete.append(change.doc_id)
                stats["deleted"] += 1
            elif change.change_type == ChangeType.MODIFIED:
                to_delete.append(change.doc_id)  # Primero eliminar
                to_add.append((change.doc_id, doc_contents[change.doc_id]))
                stats["modified"] += 1
            else:  # ADDED
                to_add.append((change.doc_id, doc_contents[change.doc_id]))
                stats["added"] += 1

        # Eliminar en batch
        if to_delete:
            self._batch_delete(to_delete)

        # Agregar en batch
        if to_add:
            self._batch_add(to_add)

        # Actualizar tracker
        self.tracker.apply_changes(changes, doc_contents)

        stats["unchanged"] = len(documents) - stats["added"] - stats["modified"]

        return stats

    def _batch_delete(self, doc_ids: List[str]):
        """Elimina múltiples documentos."""
        for doc_id in doc_ids:
            self._delete_document(doc_id)

    def _batch_add(self, documents: List[tuple]):
        """Agrega múltiples documentos con batching de embeddings."""
        all_chunks = []

        # Generar todos los chunks
        for doc_id, content in documents:
            chunks = self._chunk_document(content, doc_id)
            all_chunks.extend(chunks)

        # Procesar en batches
        for i in range(0, len(all_chunks), self.batch_size):
            batch = all_chunks[i:i+self.batch_size]
            texts = [c['content'] for c in batch]

            # Embeddings en batch
            embeddings = self._batch_get_embeddings(texts)

            # Agregar al vector store
            self.vector_store.add(
                ids=[c['chunk_id'] for c in batch],
                embeddings=embeddings,
                documents=texts,
                metadatas=[{
                    "doc_id": c['doc_id'],
                    "chunk_index": c['chunk_index'],
                    "indexed_at": datetime.now().isoformat()
                } for c in batch]
            )
```

## Estrategias de Actualización

### Actualización en Tiempo Real

```python
"""
Actualización en tiempo real con colas
"""
from queue import Queue
from threading import Thread
import time


class RealTimeIndexUpdater:
    """
    Actualizador de índice en tiempo real usando cola de eventos.
    """

    def __init__(
        self,
        indexer: IncrementalIndexer,
        debounce_seconds: float = 1.0
    ):
        self.indexer = indexer
        self.debounce_seconds = debounce_seconds
        self.event_queue = Queue()
        self.pending_updates: Dict[str, Dict] = {}
        self._running = False
        self._worker_thread = None

    def start(self):
        """Inicia el procesador de eventos."""
        self._running = True
        self._worker_thread = Thread(target=self._process_events)
        self._worker_thread.daemon = True
        self._worker_thread.start()

    def stop(self):
        """Detiene el procesador."""
        self._running = False
        if self._worker_thread:
            self._worker_thread.join()

    def notify_change(
        self,
        doc_id: str,
        content: Optional[str],
        change_type: str
    ):
        """
        Notifica un cambio para procesamiento.

        Args:
            doc_id: ID del documento
            content: Contenido nuevo (None si es eliminación)
            change_type: "add", "update", "delete"
        """
        self.event_queue.put({
            "doc_id": doc_id,
            "content": content,
            "change_type": change_type,
            "timestamp": time.time()
        })

    def _process_events(self):
        """Worker que procesa eventos de la cola."""
        while self._running:
            try:
                # Recolectar eventos con debounce
                events = self._collect_events()

                if events:
                    self._apply_events(events)

            except Exception as e:
                print(f"Error procesando eventos: {e}")

            time.sleep(0.1)

    def _collect_events(self) -> List[Dict]:
        """Recolecta eventos con debounce."""
        events = []
        last_timestamp = {}

        while not self.event_queue.empty():
            event = self.event_queue.get()
            doc_id = event['doc_id']

            # Debounce: solo mantener el evento más reciente por documento
            if doc_id not in last_timestamp or \
               event['timestamp'] > last_timestamp[doc_id]:
                self.pending_updates[doc_id] = event
                last_timestamp[doc_id] = event['timestamp']

        # Procesar eventos que pasaron el debounce
        now = time.time()
        ready_events = []

        for doc_id, event in list(self.pending_updates.items()):
            if now - event['timestamp'] >= self.debounce_seconds:
                ready_events.append(event)
                del self.pending_updates[doc_id]

        return ready_events

    def _apply_events(self, events: List[Dict]):
        """Aplica eventos al índice."""
        documents = []
        for event in events:
            if event['change_type'] != 'delete':
                documents.append({
                    "id": event['doc_id'],
                    "content": event['content']
                })

        if documents:
            stats = self.indexer.sync(documents)
            print(f"Actualización aplicada: {stats}")


# Ejemplo de uso
def demo_realtime_updater():
    # Configurar
    tracker = DocumentTracker()
    # indexer = IncrementalIndexer(api_key, vector_store, tracker)
    # updater = RealTimeIndexUpdater(indexer, debounce_seconds=2.0)

    # Iniciar
    # updater.start()

    # Simular cambios
    # updater.notify_change("doc1", "Nuevo contenido", "update")
    # updater.notify_change("doc2", "Contenido documento 2", "add")

    # Los cambios se procesarán automáticamente
    pass
```

### Actualización Programada

```python
"""
Actualización programada con sincronización periódica
"""
from datetime import datetime, timedelta
import schedule
import time


class ScheduledIndexUpdater:
    """
    Actualizador con sincronización programada.
    """

    def __init__(
        self,
        indexer: IncrementalIndexer,
        document_source,  # Fuente de documentos (DB, API, etc.)
        sync_interval_minutes: int = 60
    ):
        self.indexer = indexer
        self.document_source = document_source
        self.sync_interval = sync_interval_minutes
        self.last_sync: Optional[datetime] = None
        self.sync_history: List[Dict] = []

    def sync_now(self) -> Dict:
        """Ejecuta sincronización inmediata."""
        start_time = datetime.now()

        # Obtener documentos de la fuente
        documents = self.document_source.get_all_documents()

        # Sincronizar
        stats = self.indexer.sync(documents)

        # Registrar
        sync_record = {
            "timestamp": start_time.isoformat(),
            "duration_seconds": (datetime.now() - start_time).total_seconds(),
            "stats": stats
        }
        self.sync_history.append(sync_record)
        self.last_sync = start_time

        return sync_record

    def sync_incremental(self) -> Dict:
        """Sincroniza solo documentos modificados desde última sync."""
        start_time = datetime.now()

        # Obtener documentos modificados desde última sync
        if self.last_sync:
            documents = self.document_source.get_documents_modified_since(
                self.last_sync
            )
        else:
            documents = self.document_source.get_all_documents()

        stats = self.indexer.sync(documents)

        sync_record = {
            "timestamp": start_time.isoformat(),
            "duration_seconds": (datetime.now() - start_time).total_seconds(),
            "stats": stats,
            "incremental": bool(self.last_sync)
        }
        self.sync_history.append(sync_record)
        self.last_sync = start_time

        return sync_record

    def start_scheduled_sync(self):
        """Inicia sincronización programada."""
        schedule.every(self.sync_interval).minutes.do(self.sync_incremental)

        print(f"Sincronización programada cada {self.sync_interval} minutos")

        while True:
            schedule.run_pending()
            time.sleep(60)

    def get_sync_status(self) -> Dict:
        """Retorna estado de sincronización."""
        return {
            "last_sync": self.last_sync.isoformat() if self.last_sync else None,
            "total_syncs": len(self.sync_history),
            "last_5_syncs": self.sync_history[-5:],
            "next_scheduled": schedule.next_run().isoformat() if schedule.jobs else None
        }
```

## Manejo de Versiones

```python
"""
Sistema de versionado de documentos para RAG
"""
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from datetime import datetime


@dataclass
class DocumentVersionRecord:
    """Registro de versión de documento."""
    doc_id: str
    version: int
    content_hash: str
    created_at: datetime
    is_current: bool
    chunk_ids: List[str] = field(default_factory=list)


class VersionedIndexer:
    """
    Indexador con soporte para versionado de documentos.
    Permite mantener múltiples versiones y rollback.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,
        max_versions_per_doc: int = 5
    ):
        genai.configure(api_key=api_key)
        self.vector_store = vector_store
        self.max_versions = max_versions_per_doc
        self.version_history: Dict[str, List[DocumentVersionRecord]] = {}

    def add_document(
        self,
        doc_id: str,
        content: str,
        keep_old_version: bool = True
    ) -> DocumentVersionRecord:
        """
        Agrega o actualiza documento manteniendo historial de versiones.
        """
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]

        # Obtener versión actual
        current_version = self._get_current_version(doc_id)
        new_version_num = (current_version.version + 1) if current_version else 1

        # Marcar versión anterior como no actual
        if current_version and not keep_old_version:
            self._delete_version_chunks(current_version)
        elif current_version:
            current_version.is_current = False

        # Crear nueva versión
        new_version = DocumentVersionRecord(
            doc_id=doc_id,
            version=new_version_num,
            content_hash=content_hash,
            created_at=datetime.now(),
            is_current=True
        )

        # Indexar chunks
        chunks = self._index_content(doc_id, content, new_version_num)
        new_version.chunk_ids = [c['id'] for c in chunks]

        # Registrar versión
        if doc_id not in self.version_history:
            self.version_history[doc_id] = []
        self.version_history[doc_id].append(new_version)

        # Limpiar versiones antiguas
        self._cleanup_old_versions(doc_id)

        return new_version

    def _get_current_version(
        self,
        doc_id: str
    ) -> Optional[DocumentVersionRecord]:
        """Obtiene la versión actual de un documento."""
        if doc_id not in self.version_history:
            return None

        for version in reversed(self.version_history[doc_id]):
            if version.is_current:
                return version
        return None

    def _index_content(
        self,
        doc_id: str,
        content: str,
        version: int
    ) -> List[Dict]:
        """Indexa contenido y retorna chunks creados."""
        chunks = []
        paragraphs = content.split('\n\n')

        for i, para in enumerate(paragraphs):
            if not para.strip():
                continue

            chunk_id = f"{doc_id}_v{version}_c{i}"
            embedding = self._get_embedding(para)

            self.vector_store.add(
                ids=[chunk_id],
                embeddings=[embedding],
                documents=[para],
                metadatas=[{
                    "doc_id": doc_id,
                    "version": version,
                    "chunk_index": i,
                    "is_current_version": True
                }]
            )

            chunks.append({"id": chunk_id, "content": para})

        return chunks

    def _delete_version_chunks(self, version: DocumentVersionRecord):
        """Elimina chunks de una versión."""
        if version.chunk_ids:
            self.vector_store.delete(ids=version.chunk_ids)

    def _cleanup_old_versions(self, doc_id: str):
        """Elimina versiones que exceden el límite."""
        if doc_id not in self.version_history:
            return

        versions = self.version_history[doc_id]

        while len(versions) > self.max_versions:
            oldest = versions.pop(0)
            if not oldest.is_current:
                self._delete_version_chunks(oldest)

    def rollback(
        self,
        doc_id: str,
        target_version: int
    ) -> bool:
        """
        Restaura una versión anterior como actual.
        """
        if doc_id not in self.version_history:
            return False

        target = None
        for version in self.version_history[doc_id]:
            if version.version == target_version:
                target = version
            elif version.is_current:
                version.is_current = False
                # Actualizar metadata en vector store
                self._mark_chunks_non_current(version.chunk_ids)

        if target:
            target.is_current = True
            self._mark_chunks_current(target.chunk_ids)
            return True

        return False

    def _mark_chunks_current(self, chunk_ids: List[str]):
        """Marca chunks como versión actual."""
        for chunk_id in chunk_ids:
            self.vector_store.update(
                ids=[chunk_id],
                metadatas=[{"is_current_version": True}]
            )

    def _mark_chunks_non_current(self, chunk_ids: List[str]):
        """Marca chunks como versión no actual."""
        for chunk_id in chunk_ids:
            self.vector_store.update(
                ids=[chunk_id],
                metadatas=[{"is_current_version": False}]
            )

    def get_version_history(self, doc_id: str) -> List[Dict]:
        """Obtiene historial de versiones de un documento."""
        if doc_id not in self.version_history:
            return []

        return [
            {
                "version": v.version,
                "created_at": v.created_at.isoformat(),
                "is_current": v.is_current,
                "content_hash": v.content_hash
            }
            for v in self.version_history[doc_id]
        ]
```

## Pipeline de Actualización Completo

```python
"""
Pipeline completo de actualización incremental
"""
from typing import List, Dict, Callable


class IncrementalUpdatePipeline:
    """
    Pipeline completo para actualización incremental de índices RAG.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,
        document_source,
        notification_callback: Callable = None
    ):
        self.tracker = DocumentTracker()
        self.indexer = BatchIncrementalIndexer(
            api_key, vector_store, self.tracker
        )
        self.document_source = document_source
        self.notify = notification_callback or (lambda x: None)

        # Estadísticas
        self.total_syncs = 0
        self.total_docs_processed = 0

    def full_sync(self) -> Dict:
        """Sincronización completa (para inicialización)."""
        self.notify("Iniciando sincronización completa...")

        documents = self.document_source.get_all_documents()
        stats = self.indexer.sync(documents)

        self.total_syncs += 1
        self.total_docs_processed += len(documents)

        self.notify(f"Sincronización completa terminada: {stats}")

        return {
            "type": "full",
            "documents_total": len(documents),
            **stats
        }

    def incremental_sync(
        self,
        since: datetime = None
    ) -> Dict:
        """Sincronización incremental."""
        self.notify("Iniciando sincronización incremental...")

        if since:
            documents = self.document_source.get_documents_modified_since(since)
        else:
            documents = self.document_source.get_all_documents()

        if not documents:
            self.notify("No hay cambios para sincronizar")
            return {"type": "incremental", "changes": 0}

        stats = self.indexer.sync(documents)

        self.total_syncs += 1
        self.total_docs_processed += stats['added'] + stats['modified']

        self.notify(f"Sincronización incremental terminada: {stats}")

        return {
            "type": "incremental",
            "documents_checked": len(documents),
            **stats
        }

    def process_single_document(
        self,
        doc_id: str,
        content: str
    ) -> Dict:
        """Procesa un documento individual."""
        documents = [{"id": doc_id, "content": content}]
        stats = self.indexer.sync(documents)

        return {
            "doc_id": doc_id,
            **stats
        }

    def delete_document(self, doc_id: str) -> bool:
        """Elimina un documento del índice."""
        self.indexer._delete_document(doc_id)
        return True

    def get_statistics(self) -> Dict:
        """Retorna estadísticas del pipeline."""
        return {
            "total_syncs": self.total_syncs,
            "total_docs_processed": self.total_docs_processed,
            "tracked_documents": len(self.tracker.get_active_documents()),
            "last_sync": self.tracker.documents.get("_last_sync", {}).get("modified_at")
        }
```

## Ejercicio Práctico

```python
"""
EJERCICIO: Sistema de Actualización con Webhooks

Implementa un sistema que reciba webhooks de cambios
y actualice el índice automáticamente.
"""


class WebhookIndexUpdater:
    """
    Sistema de actualización via webhooks.

    TODO: Implementar las funcionalidades.
    """

    def __init__(
        self,
        api_key: str,
        vector_store,
        verify_signature: bool = True
    ):
        """
        TODO:
        1. Inicializar indexador incremental
        2. Configurar verificación de firma
        """
        pass

    def handle_webhook(
        self,
        payload: Dict,
        signature: str = None
    ) -> Dict:
        """
        Procesa un webhook de cambio.

        payload = {
            "event": "document.created" | "document.updated" | "document.deleted",
            "doc_id": "...",
            "content": "..." (opcional para deleted),
            "timestamp": "..."
        }

        TODO:
        1. Verificar firma si está configurado
        2. Parsear evento
        3. Aplicar cambio al índice
        4. Retornar resultado
        """
        pass

    def verify_webhook_signature(
        self,
        payload: Dict,
        signature: str
    ) -> bool:
        """
        Verifica firma HMAC del webhook.

        TODO:
        1. Calcular HMAC del payload
        2. Comparar con firma proporcionada
        """
        pass

    def get_webhook_endpoint_info(self) -> Dict:
        """
        Retorna información para configurar webhook en el origen.

        TODO:
        1. Retornar URL del endpoint
        2. Retornar eventos soportados
        3. Retornar formato esperado
        """
        pass


def create_webhook_server(updater: WebhookIndexUpdater):
    """
    Crea servidor Flask/FastAPI para recibir webhooks.

    TODO: Implementar endpoint
    """
    # from flask import Flask, request
    # app = Flask(__name__)
    #
    # @app.route('/webhook', methods=['POST'])
    # def webhook_handler():
    #     payload = request.json
    #     signature = request.headers.get('X-Signature')
    #     result = updater.handle_webhook(payload, signature)
    #     return result
    #
    # return app
    pass
```

## Resumen

| Estrategia | Latencia | Complejidad | Caso de Uso |
|------------|----------|-------------|-------------|
| Tiempo Real | < 1 min | Alta | Apps interactivas |
| Programada | Minutos-horas | Media | Batch processing |
| Por Webhook | Segundos | Media | Sistemas integrados |
| Manual | Variable | Baja | Mantenimiento |

### Checklist de Implementación

- [ ] Implementar tracking de cambios
- [ ] Configurar detección de modificaciones
- [ ] Implementar batch processing
- [ ] Agregar versionado si es necesario
- [ ] Configurar rollback para emergencias
- [ ] Implementar monitoreo de sincronización
- [ ] Documentar estrategia de actualización

## Conclusión del Módulo

Completaste el **Módulo 6: RAG con Agentes**. Aprendiste:

1. **Fundamentos**: Arquitectura RAG, embeddings, vector databases
2. **RAG Avanzado**: Hybrid search, re-ranking, retrieval iterativo
3. **Producción**: Chunking, evaluación, actualización incremental

En el siguiente módulo exploraremos **Sistemas Multi-Agente** para orquestar múltiples agentes trabajando juntos.
