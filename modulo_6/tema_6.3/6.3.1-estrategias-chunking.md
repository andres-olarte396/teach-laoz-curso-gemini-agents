# 6.3.1 Estrategias de Chunking

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de implementar estrategias de chunking optimizadas para diferentes tipos de documentos y casos de uso en sistemas RAG con Google Gemini.

## Introducción

El **chunking** (segmentación) es el proceso de dividir documentos grandes en fragmentos más pequeños para indexación y búsqueda. La estrategia de chunking impacta directamente en la calidad del RAG.

### Impacto del Chunking en RAG

```
┌─────────────────────────────────────────────────────────────────┐
│                    IMPACTO DEL TAMAÑO DE CHUNK                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  CHUNKS MUY PEQUEÑOS (50-100 tokens)                            │
│  ├── ✓ Alta precisión en búsqueda                               │
│  ├── ✓ Menos ruido en el contexto                               │
│  ├── ✗ Pérdida de contexto                                      │
│  ├── ✗ Fragmentación de ideas                                   │
│  └── ✗ Más chunks = más embeddings = más costo                  │
│                                                                  │
│  CHUNKS MUY GRANDES (1000+ tokens)                              │
│  ├── ✓ Contexto completo preservado                             │
│  ├── ✓ Menos chunks para gestionar                              │
│  ├── ✗ Dilución semántica del embedding                         │
│  ├── ✗ Puede incluir info irrelevante                           │
│  └── ✗ Desperdicio de ventana de contexto                       │
│                                                                  │
│  TAMAÑO ÓPTIMO (200-500 tokens)                                 │
│  ├── ✓ Balance entre contexto y precisión                       │
│  ├── ✓ Unidades semánticas coherentes                           │
│  └── Depende del tipo de documento y caso de uso                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Estrategias de Chunking

### 1. Fixed-Size Chunking (Tamaño Fijo)

```python
"""
Chunking de tamaño fijo con overlap
"""
from typing import List
from dataclasses import dataclass


@dataclass
class Chunk:
    """Representa un chunk de texto."""
    content: str
    start_idx: int
    end_idx: int
    metadata: dict


class FixedSizeChunker:
    """
    Chunking por tamaño fijo de caracteres/tokens.
    Simple pero efectivo para texto homogéneo.
    """

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        length_function: callable = len  # Puede ser tokenizador
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function

    def chunk(self, text: str) -> List[Chunk]:
        """Divide texto en chunks de tamaño fijo."""
        chunks = []
        start = 0

        while start < len(text):
            # Calcular fin del chunk
            end = start + self.chunk_size

            # Ajustar para no cortar palabras
            if end < len(text):
                # Buscar el último espacio antes del límite
                last_space = text.rfind(' ', start, end)
                if last_space > start:
                    end = last_space

            chunk_text = text[start:end].strip()

            if chunk_text:
                chunks.append(Chunk(
                    content=chunk_text,
                    start_idx=start,
                    end_idx=end,
                    metadata={
                        "chunk_index": len(chunks),
                        "strategy": "fixed_size"
                    }
                ))

            # Avanzar con overlap
            start = end - self.chunk_overlap
            if start >= len(text) - self.chunk_overlap:
                break

        return chunks


# Ejemplo de uso
def demo_fixed_size():
    text = """
    Python es un lenguaje de programación de alto nivel, interpretado y de propósito general.
    Fue creado por Guido van Rossum y lanzado por primera vez en 1991.

    Python enfatiza la legibilidad del código con su uso de indentación significativa.
    Sus características de diseño permiten escribir código claro y lógico.

    Python soporta múltiples paradigmas de programación, incluyendo programación
    estructurada, orientada a objetos y funcional. A menudo se describe como un
    lenguaje "batteries included" debido a su completa biblioteca estándar.
    """

    chunker = FixedSizeChunker(chunk_size=200, chunk_overlap=30)
    chunks = chunker.chunk(text)

    print(f"Texto dividido en {len(chunks)} chunks:\n")
    for chunk in chunks:
        print(f"--- Chunk {chunk.metadata['chunk_index']} ---")
        print(f"Caracteres: {len(chunk.content)}")
        print(f"Contenido: {chunk.content[:100]}...")
        print()
```

### 2. Recursive Character Splitting

```python
"""
Chunking recursivo con separadores jerárquicos
"""
from typing import List, Optional


class RecursiveChunker:
    """
    Divide texto usando separadores jerárquicos.
    Intenta mantener unidades semánticas (párrafos > oraciones > palabras).
    """

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        # Separadores ordenados de más a menos preferido
        self.separators = separators or [
            "\n\n",     # Párrafos
            "\n",       # Líneas
            ". ",       # Oraciones
            "? ",       # Preguntas
            "! ",       # Exclamaciones
            "; ",       # Puntos y coma
            ", ",       # Comas
            " ",        # Palabras
            ""          # Caracteres (último recurso)
        ]

    def _split_text(
        self,
        text: str,
        separators: List[str]
    ) -> List[str]:
        """Divide texto recursivamente."""
        if not text:
            return []

        # Si el texto cabe en un chunk, retornarlo
        if len(text) <= self.chunk_size:
            return [text]

        # Tomar el primer separador disponible
        separator = separators[0] if separators else ""
        remaining_separators = separators[1:] if len(separators) > 1 else []

        # Dividir por el separador actual
        if separator:
            splits = text.split(separator)
        else:
            # Último recurso: dividir por caracteres
            splits = list(text)

        # Recombinar splits en chunks del tamaño adecuado
        chunks = []
        current_chunk = ""

        for split in splits:
            # Agregar separador de vuelta (excepto al inicio)
            test_chunk = current_chunk + (separator if current_chunk else "") + split

            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # Guardar chunk actual si tiene contenido
                if current_chunk:
                    chunks.append(current_chunk)

                # El split individual es muy grande?
                if len(split) > self.chunk_size:
                    # Recursión con separadores más finos
                    sub_chunks = self._split_text(split, remaining_separators)
                    chunks.extend(sub_chunks[:-1])
                    current_chunk = sub_chunks[-1] if sub_chunks else ""
                else:
                    current_chunk = split

        # No olvidar el último chunk
        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def chunk(self, text: str) -> List[Chunk]:
        """Divide texto en chunks."""
        splits = self._split_text(text, self.separators)

        # Agregar overlap
        chunks = []
        for i, split in enumerate(splits):
            # Agregar contexto del chunk anterior si hay overlap
            if i > 0 and self.chunk_overlap > 0:
                prev_text = splits[i-1]
                overlap_text = prev_text[-self.chunk_overlap:] if len(prev_text) > self.chunk_overlap else prev_text
                content = overlap_text + " " + split
            else:
                content = split

            chunks.append(Chunk(
                content=content.strip(),
                start_idx=0,  # Simplificado
                end_idx=len(content),
                metadata={
                    "chunk_index": i,
                    "strategy": "recursive"
                }
            ))

        return chunks


# Ejemplo
def demo_recursive():
    text = """
# Introducción a Python

Python es un lenguaje de programación de alto nivel. Fue creado por Guido van Rossum.

## Características

Python tiene varias características importantes:
- Sintaxis clara y legible
- Tipado dinámico
- Gestión automática de memoria

## Aplicaciones

Python se usa en muchos campos. Por ejemplo: desarrollo web, ciencia de datos, y automatización.

### Desarrollo Web

Frameworks como Django y Flask permiten crear aplicaciones web robustas.

### Ciencia de Datos

Bibliotecas como NumPy, Pandas y Scikit-learn son fundamentales.
"""

    chunker = RecursiveChunker(chunk_size=300, chunk_overlap=30)
    chunks = chunker.chunk(text)

    print(f"Chunks generados: {len(chunks)}\n")
    for chunk in chunks:
        print(f"--- Chunk {chunk.metadata['chunk_index']} ({len(chunk.content)} chars) ---")
        print(chunk.content[:150] + "...")
        print()
```

### 3. Semantic Chunking

```python
"""
Chunking semántico basado en similitud de embeddings
"""
import google.generativeai as genai
from typing import List
import numpy as np


class SemanticChunker:
    """
    Divide texto basándose en cambios de tema/semántica.
    Usa embeddings para detectar puntos de corte naturales.
    """

    def __init__(
        self,
        api_key: str,
        breakpoint_threshold: float = 0.5,
        min_chunk_size: int = 100,
        max_chunk_size: int = 1000
    ):
        genai.configure(api_key=api_key)
        self.breakpoint_threshold = breakpoint_threshold
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size

    def _get_embedding(self, text: str) -> List[float]:
        """Genera embedding para un texto."""
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calcula similitud coseno."""
        v1, v2 = np.array(vec1), np.array(vec2)
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

    def _split_into_sentences(self, text: str) -> List[str]:
        """Divide texto en oraciones."""
        import re
        # Patrón simple para oraciones
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]

    def chunk(self, text: str) -> List[Chunk]:
        """
        Divide texto en chunks semánticos.
        """
        # Dividir en oraciones
        sentences = self._split_into_sentences(text)

        if len(sentences) <= 1:
            return [Chunk(
                content=text,
                start_idx=0,
                end_idx=len(text),
                metadata={"chunk_index": 0, "strategy": "semantic"}
            )]

        # Generar embeddings para cada oración
        embeddings = [self._get_embedding(s) for s in sentences]

        # Calcular similitudes entre oraciones consecutivas
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = self._cosine_similarity(embeddings[i], embeddings[i+1])
            similarities.append(sim)

        # Identificar puntos de corte (cambios de tema)
        breakpoints = []
        for i, sim in enumerate(similarities):
            if sim < self.breakpoint_threshold:
                breakpoints.append(i + 1)  # Índice de la siguiente oración

        # Crear chunks basados en breakpoints
        chunks = []
        start_idx = 0

        for bp in breakpoints + [len(sentences)]:
            chunk_sentences = sentences[start_idx:bp]
            chunk_text = " ".join(chunk_sentences)

            # Verificar tamaños mínimo y máximo
            if len(chunk_text) >= self.min_chunk_size:
                if len(chunk_text) > self.max_chunk_size:
                    # Subdividir chunks muy grandes
                    for i in range(0, len(chunk_text), self.max_chunk_size):
                        sub_text = chunk_text[i:i+self.max_chunk_size]
                        chunks.append(Chunk(
                            content=sub_text.strip(),
                            start_idx=0,
                            end_idx=len(sub_text),
                            metadata={
                                "chunk_index": len(chunks),
                                "strategy": "semantic"
                            }
                        ))
                else:
                    chunks.append(Chunk(
                        content=chunk_text.strip(),
                        start_idx=0,
                        end_idx=len(chunk_text),
                        metadata={
                            "chunk_index": len(chunks),
                            "strategy": "semantic"
                        }
                    ))

            start_idx = bp

        return chunks


# Ejemplo
def demo_semantic():
    text = """
    Python es un lenguaje de programación muy popular para análisis de datos.
    Bibliotecas como Pandas y NumPy facilitan el procesamiento de información.
    Los científicos de datos prefieren Python por su ecosistema robusto.

    Por otro lado, JavaScript domina el desarrollo web frontend.
    Frameworks como React y Vue.js son muy utilizados.
    Node.js permite usar JavaScript también en el backend.

    En el campo de la inteligencia artificial, TensorFlow y PyTorch son fundamentales.
    Estos frameworks permiten construir redes neuronales complejas.
    El deep learning ha revolucionado el procesamiento de imágenes y texto.
    """

    chunker = SemanticChunker(
        api_key="TU_API_KEY",
        breakpoint_threshold=0.7
    )

    chunks = chunker.chunk(text)

    print(f"Chunks semánticos: {len(chunks)}\n")
    for chunk in chunks:
        print(f"--- Chunk {chunk.metadata['chunk_index']} ---")
        print(chunk.content)
        print()
```

### 4. Document-Structure Chunking

```python
"""
Chunking basado en estructura del documento (headers, secciones)
"""
import re
from typing import List, Dict, Optional


class StructureAwareChunker:
    """
    Divide documentos respetando su estructura (headers, secciones).
    Ideal para documentación técnica, markdown, HTML.
    """

    def __init__(
        self,
        max_chunk_size: int = 1000,
        include_parent_headers: bool = True
    ):
        self.max_chunk_size = max_chunk_size
        self.include_parent_headers = include_parent_headers

    def _parse_markdown_structure(self, text: str) -> List[Dict]:
        """
        Parsea estructura de markdown.
        Retorna lista de secciones con nivel y contenido.
        """
        sections = []
        current_section = {"level": 0, "header": "", "content": ""}

        lines = text.split('\n')

        for line in lines:
            # Detectar headers markdown
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)

            if header_match:
                # Guardar sección anterior si tiene contenido
                if current_section["content"].strip():
                    sections.append(current_section.copy())

                # Iniciar nueva sección
                level = len(header_match.group(1))
                header = header_match.group(2)
                current_section = {
                    "level": level,
                    "header": header,
                    "content": ""
                }
            else:
                current_section["content"] += line + "\n"

        # Guardar última sección
        if current_section["content"].strip() or current_section["header"]:
            sections.append(current_section)

        return sections

    def _build_header_hierarchy(
        self,
        sections: List[Dict]
    ) -> List[Dict]:
        """
        Construye jerarquía de headers para cada sección.
        """
        header_stack = {}

        for section in sections:
            level = section["level"]

            # Actualizar stack de headers
            header_stack[level] = section["header"]

            # Limpiar headers de niveles inferiores
            levels_to_remove = [l for l in header_stack if l > level]
            for l in levels_to_remove:
                del header_stack[l]

            # Construir path de headers
            section["header_path"] = " > ".join([
                header_stack[l] for l in sorted(header_stack.keys())
                if header_stack[l]
            ])

        return sections

    def chunk(self, text: str) -> List[Chunk]:
        """
        Divide documento respetando estructura.
        """
        # Parsear estructura
        sections = self._parse_markdown_structure(text)

        # Agregar jerarquía
        sections = self._build_header_hierarchy(sections)

        chunks = []

        for section in sections:
            content = section["content"].strip()

            if not content and not section["header"]:
                continue

            # Construir texto del chunk
            if self.include_parent_headers and section["header_path"]:
                chunk_text = f"[{section['header_path']}]\n\n{content}"
            elif section["header"]:
                chunk_text = f"# {section['header']}\n\n{content}"
            else:
                chunk_text = content

            # Si el chunk es muy grande, subdividirlo
            if len(chunk_text) > self.max_chunk_size:
                # Usar chunking recursivo para subdividir
                sub_chunker = RecursiveChunker(
                    chunk_size=self.max_chunk_size,
                    chunk_overlap=50
                )
                sub_chunks = sub_chunker.chunk(chunk_text)

                for i, sub in enumerate(sub_chunks):
                    chunks.append(Chunk(
                        content=sub.content,
                        start_idx=0,
                        end_idx=len(sub.content),
                        metadata={
                            "chunk_index": len(chunks),
                            "strategy": "structure",
                            "header": section["header"],
                            "header_path": section.get("header_path", ""),
                            "sub_chunk": i
                        }
                    ))
            else:
                chunks.append(Chunk(
                    content=chunk_text,
                    start_idx=0,
                    end_idx=len(chunk_text),
                    metadata={
                        "chunk_index": len(chunks),
                        "strategy": "structure",
                        "header": section["header"],
                        "header_path": section.get("header_path", "")
                    }
                ))

        return chunks


# Ejemplo
def demo_structure():
    markdown_doc = """
# Manual de Usuario

Este es el manual de usuario de nuestra aplicación.

## Instalación

### Requisitos del Sistema

- Python 3.8 o superior
- 4GB de RAM mínimo
- Conexión a internet

### Pasos de Instalación

1. Descargue el instalador
2. Ejecute el instalador
3. Siga las instrucciones en pantalla

## Configuración

### Configuración Básica

Para configurar la aplicación, edite el archivo config.yaml.

### Configuración Avanzada

Las opciones avanzadas permiten personalizar el comportamiento.

## Uso

### Primeros Pasos

Para comenzar, inicie sesión con sus credenciales.

### Funciones Principales

La aplicación ofrece múltiples funciones para gestión de datos.
"""

    chunker = StructureAwareChunker(
        max_chunk_size=500,
        include_parent_headers=True
    )

    chunks = chunker.chunk(markdown_doc)

    print(f"Chunks estructurales: {len(chunks)}\n")
    for chunk in chunks:
        print(f"--- Chunk {chunk.metadata['chunk_index']} ---")
        print(f"Header: {chunk.metadata.get('header', 'N/A')}")
        print(f"Path: {chunk.metadata.get('header_path', 'N/A')}")
        print(f"Contenido:\n{chunk.content[:200]}...")
        print()
```

### 5. Parent-Child Chunking

```python
"""
Chunking jerárquico: chunks pequeños para búsqueda,
chunks grandes para contexto.
"""
from dataclasses import dataclass, field
from typing import List, Optional


@dataclass
class HierarchicalChunk:
    """Chunk con referencia a padre."""
    content: str
    chunk_id: str
    parent_id: Optional[str] = None
    children_ids: List[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)


class ParentChildChunker:
    """
    Crea chunks pequeños (children) para búsqueda precisa,
    pero mantiene chunks grandes (parents) para contexto completo.
    """

    def __init__(
        self,
        parent_chunk_size: int = 1000,
        child_chunk_size: int = 200,
        child_overlap: int = 50
    ):
        self.parent_chunk_size = parent_chunk_size
        self.child_chunk_size = child_chunk_size
        self.child_overlap = child_overlap

    def chunk(self, text: str) -> tuple[List[HierarchicalChunk], List[HierarchicalChunk]]:
        """
        Divide texto en chunks padre e hijo.

        Returns:
            (parent_chunks, child_chunks)
        """
        # Primero crear chunks padre (grandes)
        parent_chunker = RecursiveChunker(
            chunk_size=self.parent_chunk_size,
            chunk_overlap=0
        )
        raw_parents = parent_chunker.chunk(text)

        parents = []
        children = []

        for i, raw_parent in enumerate(raw_parents):
            parent_id = f"parent_{i}"

            parent = HierarchicalChunk(
                content=raw_parent.content,
                chunk_id=parent_id,
                metadata={
                    "type": "parent",
                    "index": i
                }
            )

            # Crear children para este parent
            child_chunker = FixedSizeChunker(
                chunk_size=self.child_chunk_size,
                chunk_overlap=self.child_overlap
            )
            raw_children = child_chunker.chunk(raw_parent.content)

            for j, raw_child in enumerate(raw_children):
                child_id = f"child_{i}_{j}"

                child = HierarchicalChunk(
                    content=raw_child.content,
                    chunk_id=child_id,
                    parent_id=parent_id,
                    metadata={
                        "type": "child",
                        "parent_index": i,
                        "child_index": j
                    }
                )
                children.append(child)
                parent.children_ids.append(child_id)

            parents.append(parent)

        return parents, children


class ParentChildRetriever:
    """
    Retriever que busca en children pero retorna parents.
    """

    def __init__(
        self,
        api_key: str,
        parents: List[HierarchicalChunk],
        children: List[HierarchicalChunk]
    ):
        genai.configure(api_key=api_key)

        # Índices
        self.parents = {p.chunk_id: p for p in parents}
        self.children = {c.chunk_id: c for c in children}

        # Embeddings de children (para búsqueda)
        self.child_embeddings = {}
        for child in children:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=child.content,
                task_type="retrieval_document"
            )
            self.child_embeddings[child.chunk_id] = result['embedding']

    def search(
        self,
        query: str,
        top_k: int = 3,
        return_children: bool = False
    ) -> List[HierarchicalChunk]:
        """
        Busca en children, retorna parents (o children si se especifica).
        """
        # Embedding de la query
        query_result = genai.embed_content(
            model="models/text-embedding-004",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = query_result['embedding']

        # Buscar en children
        similarities = []
        for child_id, embedding in self.child_embeddings.items():
            sim = np.dot(query_embedding, embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
            )
            similarities.append((child_id, sim))

        similarities.sort(key=lambda x: x[1], reverse=True)

        if return_children:
            return [self.children[cid] for cid, _ in similarities[:top_k]]

        # Obtener parents únicos de los top children
        parent_ids = []
        for child_id, _ in similarities:
            parent_id = self.children[child_id].parent_id
            if parent_id and parent_id not in parent_ids:
                parent_ids.append(parent_id)
            if len(parent_ids) >= top_k:
                break

        return [self.parents[pid] for pid in parent_ids]


# Ejemplo
def demo_parent_child():
    text = """
    Python es un lenguaje de programación de alto nivel, interpretado y
    de propósito general. Fue creado por Guido van Rossum y lanzado
    por primera vez en 1991. Python enfatiza la legibilidad del código
    con su uso de indentación significativa.

    Python soporta múltiples paradigmas de programación. Incluye
    programación estructurada, orientada a objetos y funcional.
    A menudo se describe como un lenguaje "batteries included"
    debido a su completa biblioteca estándar.

    En ciencia de datos, Python es extremadamente popular. Bibliotecas
    como NumPy, Pandas, y Scikit-learn forman el ecosistema principal.
    TensorFlow y PyTorch son las bibliotecas líderes para deep learning.
    """

    chunker = ParentChildChunker(
        parent_chunk_size=400,
        child_chunk_size=100
    )

    parents, children = chunker.chunk(text)

    print(f"Parents: {len(parents)}, Children: {len(children)}\n")

    print("=== PARENTS ===")
    for p in parents:
        print(f"\n{p.chunk_id} ({len(p.children_ids)} children):")
        print(f"  {p.content[:100]}...")

    print("\n=== CHILDREN ===")
    for c in children[:5]:
        print(f"\n{c.chunk_id} (parent: {c.parent_id}):")
        print(f"  {c.content}")
```

## Chunking Especializado por Tipo de Documento

### Chunking para Código

```python
"""
Chunking especializado para código fuente
"""
import re
from typing import List


class CodeChunker:
    """
    Chunker especializado para código fuente.
    Respeta límites de funciones, clases y métodos.
    """

    def __init__(
        self,
        max_chunk_size: int = 1000,
        language: str = "python"
    ):
        self.max_chunk_size = max_chunk_size
        self.language = language

    def _extract_python_blocks(self, code: str) -> List[Dict]:
        """Extrae bloques de código Python."""
        blocks = []

        # Patrones para Python
        patterns = [
            (r'^class\s+(\w+).*?(?=^class|\Z)', 'class'),
            (r'^def\s+(\w+).*?(?=^def|^class|\Z)', 'function'),
        ]

        # Extraer clases
        class_pattern = r'^class\s+\w+.*?(?=\n(?=class\s|\Z))'
        for match in re.finditer(class_pattern, code, re.MULTILINE | re.DOTALL):
            blocks.append({
                'type': 'class',
                'content': match.group(),
                'start': match.start(),
                'end': match.end()
            })

        # Extraer funciones de nivel superior
        func_pattern = r'^def\s+\w+.*?(?=\ndef\s|^class\s|\Z)'
        for match in re.finditer(func_pattern, code, re.MULTILINE | re.DOTALL):
            # Verificar que no está dentro de una clase
            is_in_class = any(
                b['start'] <= match.start() < b['end']
                for b in blocks if b['type'] == 'class'
            )
            if not is_in_class:
                blocks.append({
                    'type': 'function',
                    'content': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })

        return sorted(blocks, key=lambda x: x['start'])

    def chunk(self, code: str) -> List[Chunk]:
        """Divide código en chunks respetando estructura."""
        blocks = self._extract_python_blocks(code)

        chunks = []
        for block in blocks:
            content = block['content'].strip()

            # Si el bloque es muy grande, subdividir por métodos
            if len(content) > self.max_chunk_size and block['type'] == 'class':
                # Extraer métodos de la clase
                method_pattern = r'^\s+def\s+\w+.*?(?=\n\s+def\s|\Z)'
                methods = re.findall(method_pattern, content, re.MULTILINE | re.DOTALL)

                # Primer chunk: definición de clase + primeras líneas
                class_header = content.split('def')[0] if 'def' in content else content
                chunks.append(Chunk(
                    content=class_header.strip(),
                    start_idx=block['start'],
                    end_idx=block['start'] + len(class_header),
                    metadata={
                        'type': 'class_header',
                        'name': block.get('name', ''),
                        'strategy': 'code'
                    }
                ))

                # Chunks por método
                for method in methods:
                    chunks.append(Chunk(
                        content=method.strip(),
                        start_idx=0,
                        end_idx=len(method),
                        metadata={
                            'type': 'method',
                            'strategy': 'code'
                        }
                    ))
            else:
                chunks.append(Chunk(
                    content=content,
                    start_idx=block['start'],
                    end_idx=block['end'],
                    metadata={
                        'type': block['type'],
                        'strategy': 'code'
                    }
                ))

        return chunks
```

## Selección de Estrategia

```python
"""
Selector automático de estrategia de chunking
"""


class ChunkingStrategySelector:
    """
    Selecciona la mejor estrategia de chunking según el documento.
    """

    def __init__(self, api_key: str):
        self.api_key = api_key

    def detect_document_type(self, text: str) -> str:
        """Detecta el tipo de documento."""
        # Indicadores de código
        code_indicators = [
            r'def\s+\w+\s*\(', r'class\s+\w+', r'import\s+\w+',
            r'function\s+\w+', r'const\s+\w+', r'let\s+\w+'
        ]
        code_score = sum(1 for p in code_indicators if re.search(p, text))

        # Indicadores de markdown
        md_indicators = [r'^#{1,6}\s+', r'^\*\s+', r'^\d+\.\s+', r'\[.*\]\(.*\)']
        md_score = sum(1 for p in md_indicators if re.search(p, text, re.MULTILINE))

        # Indicadores de texto técnico
        tech_indicators = [r'\b(?:API|SDK|HTTP|JSON|XML)\b', r'```', r'`\w+`']
        tech_score = sum(1 for p in tech_indicators if re.search(p, text, re.IGNORECASE))

        if code_score >= 3:
            return "code"
        elif md_score >= 3:
            return "markdown"
        elif tech_score >= 2:
            return "technical"
        else:
            return "general"

    def get_chunker(self, text: str) -> object:
        """Retorna el chunker apropiado para el documento."""
        doc_type = self.detect_document_type(text)

        if doc_type == "code":
            return CodeChunker(max_chunk_size=1000)
        elif doc_type == "markdown":
            return StructureAwareChunker(max_chunk_size=800)
        elif doc_type == "technical":
            return RecursiveChunker(chunk_size=500, chunk_overlap=100)
        else:
            return RecursiveChunker(chunk_size=400, chunk_overlap=50)

    def chunk(self, text: str) -> List[Chunk]:
        """Chunking automático con estrategia detectada."""
        chunker = self.get_chunker(text)
        return chunker.chunk(text)
```

## Ejercicio Práctico

```python
"""
EJERCICIO: Chunker Adaptativo con Evaluación

Implementa un chunker que evalúe la calidad de los chunks
generados y ajuste la estrategia dinámicamente.
"""


class AdaptiveChunker:
    """
    Chunker que evalúa y optimiza la segmentación.

    TODO: Implementar las funcionalidades.
    """

    def __init__(self, api_key: str):
        """
        TODO:
        1. Inicializar múltiples estrategias de chunking
        2. Configurar evaluador de calidad
        """
        pass

    def evaluate_chunk_quality(self, chunk: Chunk) -> Dict[str, float]:
        """
        Evalúa la calidad de un chunk.

        Métricas a evaluar:
        - coherence: ¿El chunk es una unidad coherente?
        - completeness: ¿Contiene una idea completa?
        - searchability: ¿Es probable que se encuentre en búsqueda?

        TODO:
        1. Usar Gemini para evaluar coherencia
        2. Verificar que no corta oraciones/párrafos
        3. Evaluar densidad de información
        """
        pass

    def optimize_chunks(
        self,
        chunks: List[Chunk],
        quality_threshold: float = 0.7
    ) -> List[Chunk]:
        """
        Optimiza chunks de baja calidad.

        TODO:
        1. Evaluar cada chunk
        2. Identificar chunks de baja calidad
        3. Intentar re-chunking con otra estrategia
        4. Combinar chunks muy pequeños
        5. Dividir chunks muy grandes
        """
        pass

    def chunk(
        self,
        text: str,
        optimize: bool = True
    ) -> List[Chunk]:
        """
        Chunking adaptativo con optimización.

        TODO:
        1. Detectar tipo de documento
        2. Aplicar estrategia inicial
        3. Evaluar calidad
        4. Optimizar si es necesario
        """
        pass


def test_adaptive_chunker():
    """Prueba el chunker adaptativo."""
    # TODO: Implementar pruebas
    pass
```

## Resumen

| Estrategia | Mejor Para | Complejidad |
|------------|------------|-------------|
| Fixed-Size | Texto homogéneo | Baja |
| Recursive | Documentos generales | Media |
| Semantic | Documentos largos | Alta |
| Structure | Markdown, HTML | Media |
| Parent-Child | RAG de precisión | Alta |
| Code | Código fuente | Media |

### Checklist de Chunking

- [ ] Elegir estrategia según tipo de documento
- [ ] Configurar tamaño óptimo (200-500 tokens típico)
- [ ] Incluir overlap para continuidad
- [ ] Preservar metadata de origen
- [ ] Evaluar calidad de chunks
- [ ] Considerar parent-child para mejor contexto

## Siguiente Paso

En el próximo subtema exploraremos **Evaluación de Calidad de RAG**, con métricas y técnicas para medir y mejorar el rendimiento del sistema.
