# 11.3.3 Retrospectiva y Mejoras

## Objetivo de Aprendizaje

Al finalizar este subtema, habrás realizado una retrospectiva estructurada del proyecto, identificado lecciones aprendidas, y definido un roadmap de mejoras futuras.

## Introducción

La retrospectiva cierra el ciclo de aprendizaje. Evaluamos qué funcionó, qué no, y qué mejorar. Esto aplica tanto al sistema técnico como al proceso de desarrollo, consolidando las lecciones del curso completo.

```
Ciclo de Retrospectiva
──────────────────────

    ┌──────────┐     ┌──────────┐     ┌──────────┐
    │  ¿Qué    │────►│  ¿Qué    │────►│  ¿Qué    │
    │  salió   │     │  no      │     │  mejorar │
    │  bien?   │     │  funcionó│     │  ?       │
    └──────────┘     └──────────┘     └──────────┘
         │                                  │
         │          ┌──────────┐            │
         └─────────►│ Roadmap  │◄───────────┘
                    │ de       │
                    │ Mejoras  │
                    └──────────┘
```

## Framework de Retrospectiva

```python
# scripts/retrospective.py
"""Framework para retrospectiva estructurada del proyecto"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional
from enum import Enum
import json
from datetime import datetime


class Category(str, Enum):
    ARCHITECTURE = "architecture"
    AGENTS = "agents"
    TESTING = "testing"
    DEPLOYMENT = "deployment"
    OBSERVABILITY = "observability"
    PROCESS = "process"


class Priority(str, Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class LessonLearned:
    """Una lección aprendida durante el proyecto"""
    title: str
    category: Category
    description: str
    what_happened: str
    impact: str
    recommendation: str


@dataclass
class ImprovementItem:
    """Un item de mejora para el roadmap"""
    title: str
    category: Category
    priority: Priority
    effort_days: int
    description: str
    expected_impact: str
    dependencies: List[str] = field(default_factory=list)


class ProjectRetrospective:
    """Retrospectiva completa del proyecto"""

    def __init__(self, project_name: str):
        self.project_name = project_name
        self.date = datetime.now().isoformat()
        self.what_went_well: List[LessonLearned] = []
        self.what_went_wrong: List[LessonLearned] = []
        self.improvements: List[ImprovementItem] = []

    def add_positive(self, lesson: LessonLearned):
        self.what_went_well.append(lesson)

    def add_negative(self, lesson: LessonLearned):
        self.what_went_wrong.append(lesson)

    def add_improvement(self, item: ImprovementItem):
        self.improvements.append(item)

    def generate_report(self) -> str:
        """Genera reporte de retrospectiva en markdown"""
        lines = [
            f"# Retrospectiva: {self.project_name}",
            f"Fecha: {self.date}",
            "",
            "## Lo que salió bien",
            ""
        ]

        for lesson in self.what_went_well:
            lines.extend([
                f"### {lesson.title}",
                f"**Categoría:** {lesson.category.value}",
                f"**Qué pasó:** {lesson.what_happened}",
                f"**Impacto:** {lesson.impact}",
                f"**Recomendación:** {lesson.recommendation}",
                ""
            ])

        lines.extend(["## Lo que no funcionó", ""])

        for lesson in self.what_went_wrong:
            lines.extend([
                f"### {lesson.title}",
                f"**Categoría:** {lesson.category.value}",
                f"**Qué pasó:** {lesson.what_happened}",
                f"**Impacto:** {lesson.impact}",
                f"**Recomendación:** {lesson.recommendation}",
                ""
            ])

        lines.extend(["## Roadmap de Mejoras", ""])

        # Ordenar por prioridad
        priority_order = {
            Priority.HIGH: 0,
            Priority.MEDIUM: 1,
            Priority.LOW: 2
        }
        sorted_improvements = sorted(
            self.improvements,
            key=lambda x: priority_order[x.priority]
        )

        for item in sorted_improvements:
            priority_badge = {
                Priority.HIGH: "ALTA",
                Priority.MEDIUM: "MEDIA",
                Priority.LOW: "BAJA"
            }[item.priority]

            lines.extend([
                f"### [{priority_badge}] {item.title}",
                f"**Esfuerzo:** {item.effort_days} días",
                f"**Descripción:** {item.description}",
                f"**Impacto esperado:** {item.expected_impact}",
            ])

            if item.dependencies:
                lines.append(
                    f"**Dependencias:** {', '.join(item.dependencies)}"
                )

            lines.append("")

        return "\n".join(lines)

    def export_json(self, path: str):
        """Exporta retrospectiva como JSON"""
        data = {
            "project": self.project_name,
            "date": self.date,
            "positives": [
                {
                    "title": l.title,
                    "category": l.category.value,
                    "description": l.description
                }
                for l in self.what_went_well
            ],
            "negatives": [
                {
                    "title": l.title,
                    "category": l.category.value,
                    "description": l.description
                }
                for l in self.what_went_wrong
            ],
            "improvements": [
                {
                    "title": i.title,
                    "priority": i.priority.value,
                    "effort_days": i.effort_days,
                    "description": i.description
                }
                for i in self.improvements
            ]
        }

        with open(path, "w") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
```

## Retrospectiva del Proyecto

```python
# scripts/run_retrospective.py
"""Ejecuta la retrospectiva del Research Agent"""

from retrospective import (
    ProjectRetrospective, LessonLearned,
    ImprovementItem, Category, Priority
)


def build_retrospective() -> ProjectRetrospective:
    retro = ProjectRetrospective("Research Agent Multi-Agente")

    # ===== LO QUE SALIÓ BIEN =====

    retro.add_positive(LessonLearned(
        title="Patrón Supervisor bien definido",
        category=Category.ARCHITECTURE,
        description="La arquitectura Supervisor con Orchestrator central",
        what_happened=(
            "Definir roles claros desde el inicio permitió "
            "desarrollar agentes de forma independiente. "
            "El Orchestrator maneja toda la coordinación."
        ),
        impact=(
            "Desarrollo paralelo posible, "
            "fácil agregar nuevos agentes sin modificar existentes"
        ),
        recommendation=(
            "Invertir tiempo en diseño de arquitectura antes de "
            "escribir código. Los ADRs son invaluables."
        )
    ))

    retro.add_positive(LessonLearned(
        title="Presupuesto de tokens por agente",
        category=Category.AGENTS,
        description="Control granular de costos con token budgets",
        what_happened=(
            "Asignar presupuestos individuales de tokens evitó "
            "que un solo agente consumiera todo el presupuesto. "
            "El Researcher con 20K tokens fue suficiente para "
            "5-8 búsquedas con extracción."
        ),
        impact=(
            "Costos predecibles: investigación standard ~$0.05. "
            "Sin tokens ilimitados, los agentes son más concisos."
        ),
        recommendation=(
            "Siempre definir presupuestos de tokens. "
            "Empezar conservador y ajustar basándose en métricas."
        )
    ))

    retro.add_positive(LessonLearned(
        title="Testing adversarial encontró bugs reales",
        category=Category.TESTING,
        description="Tests de prompt injection revelaron vulnerabilidades",
        what_happened=(
            "El test de injection en search_memory reveló que "
            "queries maliciosas podían manipular embeddings. "
            "Se añadió sanitización antes del embedding."
        ),
        impact="Previno vulnerabilidad en producción",
        recommendation=(
            "Incluir testing adversarial desde el inicio, "
            "no como paso final. Mínimo 15 test cases."
        )
    ))

    retro.add_positive(LessonLearned(
        title="Observabilidad desde el diseño",
        category=Category.OBSERVABILITY,
        description="Métricas y tracing integrados desde el inicio",
        what_happened=(
            "Instrumentar desde el BaseAgent significó que cada "
            "agente heredó tracing automáticamente. "
            "Grafana mostró cuellos de botella inmediatamente."
        ),
        impact=(
            "Debugging 10x más rápido. "
            "Identificamos que Researcher consumía 60% de tokens."
        ),
        recommendation=(
            "La observabilidad no es una feature, es infraestructura. "
            "Implementarla en la clase base, no en cada agente."
        )
    ))

    # ===== LO QUE NO FUNCIONÓ =====

    retro.add_negative(LessonLearned(
        title="System instructions demasiado largas",
        category=Category.AGENTS,
        description="Instructions iniciales eran 2000+ tokens",
        what_happened=(
            "Las primeras versiones del system instruction del "
            "Researcher tenían 2000+ tokens con ejemplos extensos. "
            "Esto consumía presupuesto y a veces confundía al modelo."
        ),
        impact=(
            "~10% del token budget se iba en el system prompt. "
            "Respuestas menos enfocadas por instrucciones vagas."
        ),
        recommendation=(
            "System instructions < 500 tokens. "
            "Ser específico y conciso. "
            "Los ejemplos van en few-shot, no en el system prompt."
        )
    ))

    retro.add_negative(LessonLearned(
        title="Falta de retry en comunicación",
        category=Category.ARCHITECTURE,
        description="Sin retry, fallos transitorios causaban fallos totales",
        what_happened=(
            "Cuando Redis tenía un blip de conexión, "
            "el mensaje se perdía y la investigación fallaba. "
            "No había mecanismo de retry ni dead letter queue."
        ),
        impact=(
            "~5% de investigaciones fallaban por errores transitorios "
            "de red, no por problemas reales."
        ),
        recommendation=(
            "Implementar retry con exponential backoff desde el inicio. "
            "Usar dead letter queue para mensajes que fallan 3+ veces."
        )
    ))

    retro.add_negative(LessonLearned(
        title="Tests de integración frágiles",
        category=Category.TESTING,
        description="Tests E2E dependían de servicios externos",
        what_happened=(
            "Los tests de integración llamaban a la API real de "
            "búsqueda web. Cuando la API tenía rate limits o "
            "estaba caída, los tests fallaban."
        ),
        impact="Pipeline CI rojo frecuentemente por falsos negativos",
        recommendation=(
            "Usar mocks para APIs externas en CI. "
            "Reservar tests con APIs reales para suite nocturna."
        )
    ))

    retro.add_negative(LessonLearned(
        title="Memoria semántica sin TTL",
        category=Category.ARCHITECTURE,
        description="ChromaDB acumulaba embeddings sin límite",
        what_happened=(
            "Después de 100+ investigaciones, ChromaDB tenía "
            "miles de embeddings. Las búsquedas semánticas "
            "empezaron a devolver resultados irrelevantes de "
            "investigaciones antiguas."
        ),
        impact="Degradación gradual de calidad de resultados",
        recommendation=(
            "Implementar TTL y relevance decay en memoria semántica. "
            "Limitar a los últimos N embeddings por topic."
        )
    ))

    # ===== MEJORAS FUTURAS =====

    retro.add_improvement(ImprovementItem(
        title="Ejecución paralela de subtemas",
        category=Category.AGENTS,
        priority=Priority.HIGH,
        effort_days=3,
        description=(
            "Actualmente el Researcher investiga subtemas "
            "secuencialmente. Implementar investigación paralela "
            "con asyncio.gather para reducir latencia."
        ),
        expected_impact="Reducción de latencia del 50-60%"
    ))

    retro.add_improvement(ImprovementItem(
        title="Streaming de respuestas",
        category=Category.ARCHITECTURE,
        priority=Priority.HIGH,
        effort_days=5,
        description=(
            "Implementar SSE o WebSocket para enviar progreso "
            "en tiempo real al cliente. Actualmente se usa polling."
        ),
        expected_impact="Mejor UX, feedback inmediato al usuario"
    ))

    retro.add_improvement(ImprovementItem(
        title="Cache de investigaciones similares",
        category=Category.AGENTS,
        priority=Priority.MEDIUM,
        effort_days=4,
        description=(
            "Usar embeddings para detectar investigaciones similares "
            "previas y reutilizar hallazgos relevantes. "
            "Reduce tokens y tiempo para temas recurrentes."
        ),
        expected_impact="30-40% reducción en tokens para temas similares",
        dependencies=["Memoria semántica con TTL"]
    ))

    retro.add_improvement(ImprovementItem(
        title="Agente Fact-Checker dedicado",
        category=Category.AGENTS,
        priority=Priority.MEDIUM,
        effort_days=5,
        description=(
            "Añadir un agente que verifique afirmaciones contra "
            "fuentes primarias antes de incluirlas en el reporte. "
            "Actualmente el Critic verifica coherencia pero no hechos."
        ),
        expected_impact="Reducción significativa de alucinaciones"
    ))

    retro.add_improvement(ImprovementItem(
        title="Soporte multi-modelo",
        category=Category.ARCHITECTURE,
        priority=Priority.MEDIUM,
        effort_days=3,
        description=(
            "Abstraer la capa de LLM para soportar múltiples "
            "proveedores (OpenAI, Anthropic, modelos locales). "
            "Permitir asignar diferente modelo a cada agente."
        ),
        expected_impact=(
            "Flexibilidad para optimizar costo/calidad por agente. "
            "Resiliencia ante caídas de un proveedor."
        )
    ))

    retro.add_improvement(ImprovementItem(
        title="UI web con React",
        category=Category.ARCHITECTURE,
        priority=Priority.LOW,
        effort_days=10,
        description=(
            "Frontend web con visualización de progreso, "
            "historial de investigaciones, y editor de reportes."
        ),
        expected_impact="Accesibilidad para usuarios no técnicos",
        dependencies=["Streaming de respuestas"]
    ))

    retro.add_improvement(ImprovementItem(
        title="Evaluación continua automatizada",
        category=Category.TESTING,
        priority=Priority.LOW,
        effort_days=4,
        description=(
            "Pipeline que ejecuta evaluaciones de calidad "
            "automáticamente con cada deploy, comparando "
            "métricas contra baseline."
        ),
        expected_impact="Detección automática de regresiones de calidad",
        dependencies=["Suite de evaluación completa"]
    ))

    return retro
```

## Métricas del Proyecto

```python
# scripts/project_metrics.py
"""Recopila y presenta métricas del proyecto"""

from dataclasses import dataclass
from typing import Dict


@dataclass
class ProjectMetrics:
    """Métricas finales del proyecto"""

    # Código
    total_lines_of_code: int = 0
    total_files: int = 0
    languages: Dict[str, int] = None  # lang -> lines

    # Tests
    total_tests: int = 0
    unit_tests: int = 0
    integration_tests: int = 0
    adversarial_tests: int = 0
    coverage_percent: float = 0.0

    # Agentes
    total_agents: int = 0
    total_tools: int = 0
    avg_token_budget: int = 0

    # Performance
    avg_research_time_seconds: float = 0.0
    avg_tokens_per_research: int = 0
    avg_cost_per_research: float = 0.0
    success_rate: float = 0.0

    def __post_init__(self):
        if self.languages is None:
            self.languages = {}


def collect_project_metrics() -> ProjectMetrics:
    """Recopila métricas del proyecto"""
    metrics = ProjectMetrics(
        # Código
        total_lines_of_code=4500,
        total_files=45,
        languages={
            "Python": 3800,
            "YAML": 400,
            "Dockerfile": 50,
            "Markdown": 250
        },

        # Tests
        total_tests=85,
        unit_tests=40,
        integration_tests=20,
        adversarial_tests=25,
        coverage_percent=82.0,

        # Agentes
        total_agents=5,
        total_tools=16,
        avg_token_budget=20000,

        # Performance
        avg_research_time_seconds=180,
        avg_tokens_per_research=15000,
        avg_cost_per_research=0.05,
        success_rate=94.5
    )

    return metrics


def print_metrics_report(metrics: ProjectMetrics):
    """Imprime reporte de métricas"""
    print("=" * 55)
    print("  MÉTRICAS FINALES DEL PROYECTO")
    print("=" * 55)

    print("\n  CÓDIGO")
    print("  " + "-" * 40)
    print(f"  Líneas de código:  {metrics.total_lines_of_code:,}")
    print(f"  Archivos:          {metrics.total_files}")
    for lang, lines in metrics.languages.items():
        print(f"    {lang:<15} {lines:,} líneas")

    print("\n  TESTING")
    print("  " + "-" * 40)
    print(f"  Tests totales:     {metrics.total_tests}")
    print(f"    Unit:            {metrics.unit_tests}")
    print(f"    Integration:     {metrics.integration_tests}")
    print(f"    Adversarial:     {metrics.adversarial_tests}")
    print(f"  Coverage:          {metrics.coverage_percent}%")

    print("\n  AGENTES")
    print("  " + "-" * 40)
    print(f"  Total agentes:     {metrics.total_agents}")
    print(f"  Total tools:       {metrics.total_tools}")
    print(f"  Token budget avg:  {metrics.avg_token_budget:,}")

    print("\n  PERFORMANCE")
    print("  " + "-" * 40)
    print(f"  Tiempo promedio:   {metrics.avg_research_time_seconds}s")
    print(f"  Tokens promedio:   {metrics.avg_tokens_per_research:,}")
    print(f"  Costo promedio:    ${metrics.avg_cost_per_research:.2f}")
    print(f"  Tasa de éxito:     {metrics.success_rate}%")

    print("\n" + "=" * 55)


if __name__ == "__main__":
    metrics = collect_project_metrics()
    print_metrics_report(metrics)
```

## Roadmap Visual

```
Roadmap de Mejoras - Research Agent
════════════════════════════════════

Corto Plazo (1-2 semanas)
─────────────────────────
  [ALTA] Ejecución paralela de subtemas     (3 días)
  [ALTA] Streaming de respuestas             (5 días)

Mediano Plazo (3-4 semanas)
───────────────────────────
  [MEDIA] Cache de investigaciones           (4 días)
  [MEDIA] Agente Fact-Checker                (5 días)
  [MEDIA] Soporte multi-modelo               (3 días)

Largo Plazo (5-8 semanas)
─────────────────────────
  [BAJA] UI web con React                    (10 días)
  [BAJA] Evaluación continua automatizada    (4 días)

Dependencias:
  Streaming ──► UI Web
  Cache ──► Memoria con TTL
  Evaluación ──► Suite de evaluación
```

## Cierre del Curso

A lo largo de este curso, hemos recorrido el camino completo de construcción de sistemas basados en agentes con Google Gemini:

| Módulo | Tema Central | Habilidad Adquirida |
|--------|-------------|-------------------|
| 0 | Fundamentos | Configuración y primer agente |
| 1 | Modelos Gemini | Selección y uso de modelos |
| 2 | Prompting | System instructions efectivas |
| 3 | Function Calling | Agentes que usan herramientas |
| 4 | Conversaciones | Gestión de contexto multi-turno |
| 5 | Agentes ReAct | Razonamiento y acción iterativa |
| 6 | Multi-Agente | Patrones de colaboración |
| 7 | Memoria | Persistencia y conocimiento |
| 8 | Autonomía | Agentes que planifican y deciden |
| 9 | Testing | Evaluación y calidad |
| 10 | Producción | Despliegue seguro y observable |
| 11 | Proyecto | Integración de todo lo aprendido |

## Ejercicios

### Ejercicio 1: Ejecuta la retrospectiva completa de tu proyecto y genera el reporte markdown. Comparte con tu equipo.
### Ejercicio 2: Implementa la mejora de mayor prioridad (ejecución paralela) y mide el impacto real en latencia.
### Ejercicio 3: Diseña un ADR para la mejora "soporte multi-modelo" explicando los trade-offs de cada proveedor.

## Resumen

| Aspecto | Positivo | A Mejorar |
|---------|----------|-----------|
| Arquitectura | Patrón Supervisor claro | Agregar retry y DLQ |
| Agentes | Token budgets efectivos | System instructions más cortas |
| Testing | Adversarial encontró bugs | Tests E2E menos frágiles |
| Observabilidad | Integrada desde el diseño | Alertas más granulares |
| Memoria | Funcional con ChromaDB | Necesita TTL y decay |

---

**Fin del curso. Has completado todos los módulos de "Automatización con Agentes en Google Gemini".**
