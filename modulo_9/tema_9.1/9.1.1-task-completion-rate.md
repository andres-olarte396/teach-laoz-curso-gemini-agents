# 9.1.1 Task Completion Rate y Success Criteria

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de definir y medir métricas de éxito para agentes, implementando sistemas de evaluación que determinen objetivamente si un agente completa sus tareas correctamente.

## Introducción

La evaluación de agentes requiere métricas claras y objetivas. A diferencia de modelos tradicionales donde el accuracy es suficiente, los agentes necesitan criterios multidimensionales que capturen tanto el resultado final como el proceso.

```
┌─────────────────────────────────────────────────────────────────┐
│                    FRAMEWORK DE EVALUACIÓN                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐           │
│  │   SUCCESS   │   │  PARTIAL    │   │   FAILURE   │           │
│  │   CRITERIA  │   │  SUCCESS    │   │   ANALYSIS  │           │
│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘           │
│         │                 │                 │                   │
│         ▼                 ▼                 ▼                   │
│  ┌─────────────────────────────────────────────────┐           │
│  │              EVALUATION ENGINE                   │           │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐         │           │
│  │  │Funcional│  │Semantic │  │ Multi-  │         │           │
│  │  │  Match  │  │  Match  │  │Criteria │         │           │
│  └──┴─────────┴──┴─────────┴──┴─────────┴─────────┘           │
│                         │                                       │
│                         ▼                                       │
│  ┌─────────────────────────────────────────────────┐           │
│  │           METRICS AGGREGATION                    │           │
│  │  • Task Completion Rate  • Partial Credit       │           │
│  │  • Success by Category   • Failure Taxonomy     │           │
│  └─────────────────────────────────────────────────┘           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Definición de Success Criteria

### Modelo de Criterios de Éxito

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
from abc import ABC, abstractmethod
import json
from datetime import datetime

class CompletionStatus(Enum):
    SUCCESS = "success"
    PARTIAL = "partial"
    FAILURE = "failure"
    ERROR = "error"
    TIMEOUT = "timeout"

@dataclass
class SuccessCriterion:
    """Criterio individual de éxito"""
    name: str
    description: str
    weight: float = 1.0
    required: bool = True  # Si es obligatorio para éxito
    evaluator: Optional[Callable] = None

@dataclass
class TaskResult:
    """Resultado de ejecución de tarea"""
    task_id: str
    task_description: str
    output: Any
    steps_taken: List[Dict[str, Any]]
    start_time: datetime
    end_time: datetime
    error: Optional[str] = None

@dataclass
class EvaluationResult:
    """Resultado de evaluación"""
    task_id: str
    status: CompletionStatus
    score: float  # 0.0 - 1.0
    criteria_scores: Dict[str, float]
    passed_criteria: List[str]
    failed_criteria: List[str]
    feedback: str
    details: Dict[str, Any] = field(default_factory=dict)

class SuccessCriteriaDefinition:
    """Define criterios de éxito para tipos de tareas"""

    def __init__(self):
        self.criteria_templates = {}
        self._register_default_templates()

    def _register_default_templates(self):
        """Registra templates de criterios comunes"""

        # Criterios para tareas de búsqueda
        self.criteria_templates["search"] = [
            SuccessCriterion(
                name="relevant_results",
                description="Los resultados son relevantes a la consulta",
                weight=2.0,
                required=True
            ),
            SuccessCriterion(
                name="sufficient_results",
                description="Se encontró un número suficiente de resultados",
                weight=1.0,
                required=False
            ),
            SuccessCriterion(
                name="source_quality",
                description="Las fuentes son confiables",
                weight=1.5,
                required=False
            )
        ]

        # Criterios para tareas de análisis
        self.criteria_templates["analysis"] = [
            SuccessCriterion(
                name="correctness",
                description="El análisis es factualmente correcto",
                weight=3.0,
                required=True
            ),
            SuccessCriterion(
                name="completeness",
                description="El análisis cubre todos los aspectos solicitados",
                weight=2.0,
                required=True
            ),
            SuccessCriterion(
                name="clarity",
                description="El análisis es claro y bien estructurado",
                weight=1.0,
                required=False
            ),
            SuccessCriterion(
                name="actionable_insights",
                description="Proporciona insights accionables",
                weight=1.5,
                required=False
            )
        ]

        # Criterios para tareas de código
        self.criteria_templates["coding"] = [
            SuccessCriterion(
                name="functionality",
                description="El código funciona correctamente",
                weight=3.0,
                required=True
            ),
            SuccessCriterion(
                name="test_passing",
                description="Todos los tests pasan",
                weight=2.5,
                required=True
            ),
            SuccessCriterion(
                name="code_quality",
                description="El código sigue buenas prácticas",
                weight=1.5,
                required=False
            ),
            SuccessCriterion(
                name="documentation",
                description="El código está documentado",
                weight=1.0,
                required=False
            )
        ]

        # Criterios para tareas de automatización
        self.criteria_templates["automation"] = [
            SuccessCriterion(
                name="task_completed",
                description="La tarea automatizada se completó",
                weight=3.0,
                required=True
            ),
            SuccessCriterion(
                name="no_side_effects",
                description="No hubo efectos secundarios no deseados",
                weight=2.0,
                required=True
            ),
            SuccessCriterion(
                name="idempotent",
                description="La operación es idempotente",
                weight=1.0,
                required=False
            )
        ]

    def get_criteria(self, task_type: str) -> List[SuccessCriterion]:
        """Obtiene criterios para un tipo de tarea"""
        return self.criteria_templates.get(task_type, [])

    def add_custom_criteria(
        self,
        task_type: str,
        criteria: List[SuccessCriterion]
    ):
        """Añade criterios personalizados"""
        if task_type not in self.criteria_templates:
            self.criteria_templates[task_type] = []
        self.criteria_templates[task_type].extend(criteria)
```

## Evaluador de Task Completion

### Sistema de Evaluación Multi-Criterio

```python
class TaskEvaluator:
    """Evalúa completitud de tareas usando múltiples criterios"""

    def __init__(self, model_name: str = "gemini-2.0-flash"):
        self.model = genai.GenerativeModel(model_name)
        self.criteria_definition = SuccessCriteriaDefinition()
        self.evaluation_history: List[EvaluationResult] = []

    async def evaluate_task(
        self,
        task_result: TaskResult,
        task_type: str,
        custom_criteria: Optional[List[SuccessCriterion]] = None,
        ground_truth: Optional[Any] = None
    ) -> EvaluationResult:
        """
        Evalúa una tarea completada

        Args:
            task_result: Resultado de la ejecución
            task_type: Tipo de tarea para seleccionar criterios
            custom_criteria: Criterios adicionales
            ground_truth: Respuesta esperada si existe
        """
        # Obtener criterios aplicables
        criteria = self.criteria_definition.get_criteria(task_type)
        if custom_criteria:
            criteria.extend(custom_criteria)

        if not criteria:
            criteria = self._generate_default_criteria(task_result)

        # Evaluar cada criterio
        criteria_scores = {}
        passed = []
        failed = []

        for criterion in criteria:
            score = await self._evaluate_criterion(
                task_result,
                criterion,
                ground_truth
            )
            criteria_scores[criterion.name] = score

            if score >= 0.7:
                passed.append(criterion.name)
            else:
                failed.append(criterion.name)

        # Calcular score ponderado
        total_weight = sum(c.weight for c in criteria)
        weighted_score = sum(
            criteria_scores[c.name] * c.weight / total_weight
            for c in criteria
        )

        # Determinar status
        status = self._determine_status(criteria, criteria_scores)

        # Generar feedback
        feedback = await self._generate_feedback(
            task_result,
            criteria_scores,
            passed,
            failed
        )

        result = EvaluationResult(
            task_id=task_result.task_id,
            status=status,
            score=weighted_score,
            criteria_scores=criteria_scores,
            passed_criteria=passed,
            failed_criteria=failed,
            feedback=feedback,
            details={
                "task_type": task_type,
                "duration_seconds": (
                    task_result.end_time - task_result.start_time
                ).total_seconds(),
                "steps_count": len(task_result.steps_taken)
            }
        )

        self.evaluation_history.append(result)
        return result

    async def _evaluate_criterion(
        self,
        task_result: TaskResult,
        criterion: SuccessCriterion,
        ground_truth: Optional[Any]
    ) -> float:
        """Evalúa un criterio individual"""

        # Si hay evaluador custom, usarlo
        if criterion.evaluator:
            try:
                return criterion.evaluator(task_result, ground_truth)
            except Exception:
                pass

        # Usar LLM para evaluación semántica
        prompt = f"""Evalúa el siguiente resultado de tarea según el criterio especificado.

TAREA: {task_result.task_description}

RESULTADO:
{json.dumps(task_result.output, indent=2, default=str)[:2000]}

PASOS REALIZADOS:
{self._format_steps(task_result.steps_taken)[:1000]}

CRITERIO A EVALUAR:
- Nombre: {criterion.name}
- Descripción: {criterion.description}

{f"RESULTADO ESPERADO: {ground_truth}" if ground_truth else ""}

Evalúa qué tan bien el resultado cumple con el criterio.
Responde SOLO con un JSON:
{{
    "score": <float entre 0.0 y 1.0>,
    "justification": "<breve justificación>"
}}"""

        response = self.model.generate_content(prompt)

        try:
            evaluation = json.loads(response.text)
            return float(evaluation.get("score", 0.0))
        except:
            return 0.5  # Score neutro si falla parsing

    def _format_steps(self, steps: List[Dict]) -> str:
        """Formatea pasos para el prompt"""
        formatted = []
        for i, step in enumerate(steps[:10]):  # Limitar a 10 pasos
            formatted.append(f"{i+1}. {step.get('action', 'unknown')}: {step.get('result', '')[:100]}")
        return "\n".join(formatted)

    def _determine_status(
        self,
        criteria: List[SuccessCriterion],
        scores: Dict[str, float]
    ) -> CompletionStatus:
        """Determina el status basado en criterios requeridos"""

        required_criteria = [c for c in criteria if c.required]

        # Verificar criterios requeridos
        all_required_passed = all(
            scores.get(c.name, 0) >= 0.7
            for c in required_criteria
        )

        # Calcular score promedio
        avg_score = sum(scores.values()) / len(scores) if scores else 0

        if all_required_passed and avg_score >= 0.8:
            return CompletionStatus.SUCCESS
        elif all_required_passed and avg_score >= 0.5:
            return CompletionStatus.PARTIAL
        else:
            return CompletionStatus.FAILURE

    async def _generate_feedback(
        self,
        task_result: TaskResult,
        scores: Dict[str, float],
        passed: List[str],
        failed: List[str]
    ) -> str:
        """Genera feedback detallado"""

        prompt = f"""Genera feedback constructivo para el siguiente resultado de evaluación:

TAREA: {task_result.task_description}

CRITERIOS APROBADOS: {', '.join(passed) if passed else 'Ninguno'}
CRITERIOS FALLIDOS: {', '.join(failed) if failed else 'Ninguno'}

SCORES:
{json.dumps(scores, indent=2)}

Proporciona:
1. Resumen breve del desempeño
2. Puntos fuertes
3. Áreas de mejora específicas

Sé conciso y constructivo."""

        response = self.model.generate_content(prompt)
        return response.text

    def _generate_default_criteria(
        self,
        task_result: TaskResult
    ) -> List[SuccessCriterion]:
        """Genera criterios por defecto si no hay específicos"""
        return [
            SuccessCriterion(
                name="task_addressed",
                description="La tarea fue abordada según la descripción",
                weight=2.0,
                required=True
            ),
            SuccessCriterion(
                name="output_valid",
                description="El output es válido y utilizable",
                weight=1.5,
                required=True
            ),
            SuccessCriterion(
                name="no_errors",
                description="No hubo errores durante la ejecución",
                weight=1.0,
                required=False
            )
        ]
```

## Task Completion Rate Calculator

### Agregador de Métricas

```python
from collections import defaultdict
from statistics import mean, stdev
from typing import Tuple

class TaskCompletionMetrics:
    """Calcula métricas agregadas de completion rate"""

    def __init__(self):
        self.evaluations: List[EvaluationResult] = []
        self.metrics_by_category: Dict[str, List[EvaluationResult]] = defaultdict(list)

    def add_evaluation(
        self,
        evaluation: EvaluationResult,
        category: Optional[str] = None
    ):
        """Añade una evaluación al tracker"""
        self.evaluations.append(evaluation)
        if category:
            self.metrics_by_category[category].append(evaluation)

    def calculate_completion_rate(
        self,
        include_partial: bool = False
    ) -> float:
        """
        Calcula tasa de completitud

        Args:
            include_partial: Si incluir éxitos parciales como completados
        """
        if not self.evaluations:
            return 0.0

        successful_statuses = [CompletionStatus.SUCCESS]
        if include_partial:
            successful_statuses.append(CompletionStatus.PARTIAL)

        completed = sum(
            1 for e in self.evaluations
            if e.status in successful_statuses
        )

        return completed / len(self.evaluations)

    def calculate_metrics_summary(self) -> Dict[str, Any]:
        """Calcula resumen completo de métricas"""

        if not self.evaluations:
            return {"error": "No evaluations available"}

        scores = [e.score for e in self.evaluations]

        status_counts = defaultdict(int)
        for e in self.evaluations:
            status_counts[e.status.value] += 1

        # Métricas por criterio
        criteria_aggregates = defaultdict(list)
        for e in self.evaluations:
            for criterion, score in e.criteria_scores.items():
                criteria_aggregates[criterion].append(score)

        criteria_means = {
            criterion: mean(scores)
            for criterion, scores in criteria_aggregates.items()
        }

        return {
            "total_tasks": len(self.evaluations),
            "completion_rate": self.calculate_completion_rate(),
            "completion_rate_with_partial": self.calculate_completion_rate(
                include_partial=True
            ),
            "status_distribution": dict(status_counts),
            "score_statistics": {
                "mean": mean(scores),
                "std": stdev(scores) if len(scores) > 1 else 0,
                "min": min(scores),
                "max": max(scores)
            },
            "criteria_performance": criteria_means,
            "most_failed_criteria": self._get_most_failed_criteria(),
            "best_performing_criteria": self._get_best_criteria()
        }

    def calculate_by_category(self) -> Dict[str, Dict[str, Any]]:
        """Calcula métricas por categoría"""

        results = {}
        for category, evals in self.metrics_by_category.items():
            if evals:
                scores = [e.score for e in evals]
                success_count = sum(
                    1 for e in evals
                    if e.status == CompletionStatus.SUCCESS
                )

                results[category] = {
                    "total": len(evals),
                    "completion_rate": success_count / len(evals),
                    "avg_score": mean(scores),
                    "score_range": (min(scores), max(scores))
                }

        return results

    def _get_most_failed_criteria(self, top_n: int = 3) -> List[Tuple[str, float]]:
        """Obtiene criterios más frecuentemente fallidos"""

        criteria_failures = defaultdict(int)
        criteria_totals = defaultdict(int)

        for e in self.evaluations:
            for criterion in e.failed_criteria:
                criteria_failures[criterion] += 1
            for criterion in e.criteria_scores.keys():
                criteria_totals[criterion] += 1

        failure_rates = [
            (criterion, failures / criteria_totals[criterion])
            for criterion, failures in criteria_failures.items()
            if criteria_totals[criterion] > 0
        ]

        return sorted(failure_rates, key=lambda x: x[1], reverse=True)[:top_n]

    def _get_best_criteria(self, top_n: int = 3) -> List[Tuple[str, float]]:
        """Obtiene criterios con mejor desempeño"""

        criteria_scores = defaultdict(list)

        for e in self.evaluations:
            for criterion, score in e.criteria_scores.items():
                criteria_scores[criterion].append(score)

        avg_scores = [
            (criterion, mean(scores))
            for criterion, scores in criteria_scores.items()
        ]

        return sorted(avg_scores, key=lambda x: x[1], reverse=True)[:top_n]

    def generate_report(self) -> str:
        """Genera reporte de texto"""

        summary = self.calculate_metrics_summary()
        by_category = self.calculate_by_category()

        report = []
        report.append("=" * 60)
        report.append("REPORTE DE TASK COMPLETION")
        report.append("=" * 60)

        report.append(f"\nTotal de tareas evaluadas: {summary['total_tasks']}")
        report.append(f"Tasa de completitud: {summary['completion_rate']:.1%}")
        report.append(f"Con parciales: {summary['completion_rate_with_partial']:.1%}")

        report.append("\n--- Distribución por Status ---")
        for status, count in summary['status_distribution'].items():
            report.append(f"  {status}: {count}")

        report.append("\n--- Estadísticas de Score ---")
        stats = summary['score_statistics']
        report.append(f"  Promedio: {stats['mean']:.2f}")
        report.append(f"  Desv. Est.: {stats['std']:.2f}")
        report.append(f"  Rango: [{stats['min']:.2f}, {stats['max']:.2f}]")

        if summary['most_failed_criteria']:
            report.append("\n--- Criterios más fallidos ---")
            for criterion, rate in summary['most_failed_criteria']:
                report.append(f"  {criterion}: {rate:.1%} fallas")

        if by_category:
            report.append("\n--- Métricas por Categoría ---")
            for category, metrics in by_category.items():
                report.append(f"\n  {category}:")
                report.append(f"    Total: {metrics['total']}")
                report.append(f"    Completion: {metrics['completion_rate']:.1%}")
                report.append(f"    Avg Score: {metrics['avg_score']:.2f}")

        report.append("\n" + "=" * 60)

        return "\n".join(report)
```

## Evaluadores Especializados

### Evaluadores con Ground Truth

```python
class GroundTruthEvaluator:
    """Evaluador usando respuestas de referencia"""

    def __init__(self, model_name: str = "gemini-2.0-flash"):
        self.model = genai.GenerativeModel(model_name)

    async def evaluate_exact_match(
        self,
        predicted: Any,
        expected: Any,
        normalize: bool = True
    ) -> float:
        """Evaluación por coincidencia exacta"""

        if normalize:
            pred_str = str(predicted).strip().lower()
            exp_str = str(expected).strip().lower()
        else:
            pred_str = str(predicted)
            exp_str = str(expected)

        return 1.0 if pred_str == exp_str else 0.0

    async def evaluate_semantic_similarity(
        self,
        predicted: str,
        expected: str,
        threshold: float = 0.8
    ) -> float:
        """Evaluación por similitud semántica usando embeddings"""

        # Usar embeddings de Gemini
        embed_model = genai.GenerativeModel("models/text-embedding-004")

        pred_embedding = genai.embed_content(
            model="models/text-embedding-004",
            content=predicted,
            task_type="semantic_similarity"
        )["embedding"]

        exp_embedding = genai.embed_content(
            model="models/text-embedding-004",
            content=expected,
            task_type="semantic_similarity"
        )["embedding"]

        # Calcular similitud coseno
        similarity = self._cosine_similarity(pred_embedding, exp_embedding)

        return similarity

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calcula similitud coseno"""
        import math

        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    async def evaluate_structured_output(
        self,
        predicted: Dict,
        expected: Dict,
        key_weights: Optional[Dict[str, float]] = None
    ) -> float:
        """Evalúa outputs estructurados campo por campo"""

        if not key_weights:
            all_keys = set(predicted.keys()) | set(expected.keys())
            key_weights = {k: 1.0 for k in all_keys}

        total_weight = sum(key_weights.values())
        weighted_score = 0.0

        for key, weight in key_weights.items():
            pred_val = predicted.get(key)
            exp_val = expected.get(key)

            if pred_val is None and exp_val is None:
                score = 1.0
            elif pred_val is None or exp_val is None:
                score = 0.0
            elif isinstance(exp_val, str):
                score = await self.evaluate_semantic_similarity(
                    str(pred_val), exp_val
                )
            elif isinstance(exp_val, (int, float)):
                # Tolerancia numérica
                if exp_val == 0:
                    score = 1.0 if pred_val == 0 else 0.0
                else:
                    rel_error = abs(pred_val - exp_val) / abs(exp_val)
                    score = max(0, 1 - rel_error)
            elif isinstance(exp_val, list):
                score = self._evaluate_list_overlap(pred_val, exp_val)
            else:
                score = await self.evaluate_exact_match(pred_val, exp_val)

            weighted_score += score * weight

        return weighted_score / total_weight if total_weight > 0 else 0.0

    def _evaluate_list_overlap(
        self,
        predicted: List,
        expected: List
    ) -> float:
        """Evalúa overlap entre listas"""
        if not expected:
            return 1.0 if not predicted else 0.0

        pred_set = set(str(x).lower() for x in (predicted or []))
        exp_set = set(str(x).lower() for x in expected)

        intersection = len(pred_set & exp_set)
        union = len(pred_set | exp_set)

        return intersection / union if union > 0 else 0.0


class LLMJudgeEvaluator:
    """Usa un LLM como juez para evaluación"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash",
        judge_prompt_template: Optional[str] = None
    ):
        self.model = genai.GenerativeModel(model_name)
        self.prompt_template = judge_prompt_template or self._default_template()

    def _default_template(self) -> str:
        return """Eres un evaluador experto. Tu tarea es evaluar qué tan bien
un agente completó una tarea asignada.

TAREA ORIGINAL:
{task}

RESULTADO DEL AGENTE:
{result}

{ground_truth_section}

CRITERIOS DE EVALUACIÓN:
{criteria}

Evalúa el resultado y proporciona:
1. Un score de 0 a 10 para cada criterio
2. Un score general de 0 a 10
3. Justificación breve para cada score
4. Sugerencias de mejora

Responde en JSON:
{{
    "criteria_scores": {{"criterio1": score, ...}},
    "overall_score": score,
    "justifications": {{"criterio1": "...", ...}},
    "suggestions": ["..."]
}}"""

    async def evaluate(
        self,
        task: str,
        result: Any,
        criteria: List[str],
        ground_truth: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Evalúa usando LLM como juez

        Args:
            task: Descripción de la tarea
            result: Resultado del agente
            criteria: Lista de criterios a evaluar
            ground_truth: Respuesta de referencia opcional
        """

        gt_section = ""
        if ground_truth:
            gt_section = f"RESPUESTA ESPERADA (referencia):\n{ground_truth}"

        criteria_text = "\n".join(f"- {c}" for c in criteria)

        prompt = self.prompt_template.format(
            task=task,
            result=json.dumps(result, indent=2, default=str)[:3000],
            ground_truth_section=gt_section,
            criteria=criteria_text
        )

        response = self.model.generate_content(prompt)

        try:
            evaluation = json.loads(response.text)
            # Normalizar scores a 0-1
            evaluation["overall_score_normalized"] = evaluation["overall_score"] / 10
            for criterion in evaluation.get("criteria_scores", {}):
                evaluation["criteria_scores"][criterion] /= 10
            return evaluation
        except json.JSONDecodeError:
            return {
                "error": "Failed to parse evaluation",
                "raw_response": response.text
            }
```

## Ejemplo Práctico Completo

```python
import asyncio
from datetime import datetime, timedelta

async def main():
    # Configurar API
    genai.configure(api_key="YOUR_API_KEY")

    # Inicializar evaluadores
    task_evaluator = TaskEvaluator()
    metrics = TaskCompletionMetrics()

    # Simular resultados de tareas
    sample_results = [
        TaskResult(
            task_id="task_001",
            task_description="Buscar información sobre machine learning",
            output={
                "summary": "Machine learning es una rama de la IA...",
                "sources": ["arxiv.org", "scholar.google.com"],
                "key_points": ["Supervised", "Unsupervised", "Reinforcement"]
            },
            steps_taken=[
                {"action": "search", "result": "Found 10 sources"},
                {"action": "filter", "result": "Selected 3 relevant"},
                {"action": "summarize", "result": "Generated summary"}
            ],
            start_time=datetime.now() - timedelta(seconds=30),
            end_time=datetime.now()
        ),
        TaskResult(
            task_id="task_002",
            task_description="Analizar sentimiento del texto: 'Este producto es excelente'",
            output={
                "sentiment": "positive",
                "confidence": 0.95,
                "aspects": ["quality"]
            },
            steps_taken=[
                {"action": "analyze", "result": "Detected positive sentiment"}
            ],
            start_time=datetime.now() - timedelta(seconds=5),
            end_time=datetime.now()
        ),
        TaskResult(
            task_id="task_003",
            task_description="Escribir función que calcule factorial",
            output={
                "code": "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
                "tests_passed": True,
                "documentation": "Calcula el factorial de n"
            },
            steps_taken=[
                {"action": "design", "result": "Recursive approach"},
                {"action": "implement", "result": "Code written"},
                {"action": "test", "result": "All tests passed"}
            ],
            start_time=datetime.now() - timedelta(seconds=45),
            end_time=datetime.now()
        )
    ]

    # Evaluar cada tarea
    task_types = ["search", "analysis", "coding"]

    for result, task_type in zip(sample_results, task_types):
        evaluation = await task_evaluator.evaluate_task(
            task_result=result,
            task_type=task_type
        )

        metrics.add_evaluation(evaluation, category=task_type)

        print(f"\n--- Evaluación {result.task_id} ---")
        print(f"Status: {evaluation.status.value}")
        print(f"Score: {evaluation.score:.2f}")
        print(f"Criterios aprobados: {evaluation.passed_criteria}")
        print(f"Criterios fallidos: {evaluation.failed_criteria}")

    # Generar reporte final
    print("\n" + metrics.generate_report())

    # Usar LLM Judge para evaluación detallada
    llm_judge = LLMJudgeEvaluator()
    detailed_eval = await llm_judge.evaluate(
        task="Escribir función factorial",
        result=sample_results[2].output,
        criteria=[
            "Correctitud funcional",
            "Manejo de edge cases",
            "Legibilidad del código",
            "Eficiencia"
        ]
    )

    print("\n--- Evaluación LLM Judge ---")
    print(json.dumps(detailed_eval, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Definir Criterios Custom
Crea un set de criterios para evaluar un agente de customer support:
- Incluir criterios de precisión, empatía, resolución, y tiempo
- Asignar pesos apropiados a cada criterio

### Ejercicio 2: Implementar Evaluador de A/B Testing
Extiende el sistema para comparar dos versiones de agentes:
- Mismas tareas, diferentes configuraciones
- Análisis estadístico de diferencias
- Determinar si las diferencias son significativas

### Ejercicio 3: Dashboard de Métricas
Crea un sistema que:
- Almacene evaluaciones en una base de datos
- Calcule tendencias temporales
- Genere alertas si el completion rate baja

## Resumen

| Concepto | Descripción |
|----------|-------------|
| Success Criteria | Criterios específicos que definen éxito de una tarea |
| Task Completion Rate | Porcentaje de tareas completadas exitosamente |
| Evaluación Multi-criterio | Combinación ponderada de múltiples métricas |
| Ground Truth Evaluation | Comparación con respuestas de referencia |
| LLM Judge | Usar LLM como evaluador semántico |
| Metrics Aggregation | Consolidación de métricas para análisis |

---

**Siguiente:** [9.1.2 Eficiencia: Tokens, Latencia, Costo](./9.1.2-eficiencia-tokens-latencia.md)
