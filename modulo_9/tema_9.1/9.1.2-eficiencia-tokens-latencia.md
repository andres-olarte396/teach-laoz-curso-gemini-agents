# 9.1.2 Eficiencia: Tokens, Latencia, Costo

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de medir, optimizar y balancear las métricas de eficiencia de agentes incluyendo consumo de tokens, latencia de respuesta y costos operativos.

## Introducción

La eficiencia operativa es crítica para agentes en producción. Un agente puede ser funcionalmente correcto pero económicamente inviable si consume demasiados tokens o es demasiado lento.

```
┌─────────────────────────────────────────────────────────────────┐
│                TRIÁNGULO DE EFICIENCIA                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                         CALIDAD                                 │
│                            ▲                                    │
│                           /│\                                   │
│                          / │ \                                  │
│                         /  │  \                                 │
│                        /   │   \                                │
│                       /    │    \                               │
│                      /     │     \                              │
│                     /   ÓPTIMO    \                             │
│                    /       │       \                            │
│                   /_______─┼─_______\                           │
│                  ▼                   ▼                          │
│              VELOCIDAD ◄──────────► COSTO                       │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  MÉTRICAS CLAVE:                                        │   │
│  │  • Tokens/tarea  • Latencia p50/p95  • $/1000 tareas   │   │
│  │  • Throughput    • Time-to-first    • Token efficiency │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Sistema de Tracking de Eficiencia

### Modelo de Métricas

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict
import time
import asyncio

@dataclass
class TokenUsage:
    """Registro de uso de tokens"""
    input_tokens: int = 0
    output_tokens: int = 0
    cached_tokens: int = 0

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens

    @property
    def effective_input_tokens(self) -> int:
        """Tokens de entrada efectivos (sin cache)"""
        return self.input_tokens - self.cached_tokens

    def __add__(self, other: 'TokenUsage') -> 'TokenUsage':
        return TokenUsage(
            input_tokens=self.input_tokens + other.input_tokens,
            output_tokens=self.output_tokens + other.output_tokens,
            cached_tokens=self.cached_tokens + other.cached_tokens
        )

@dataclass
class LatencyMetrics:
    """Métricas de latencia"""
    time_to_first_token: float = 0.0  # Segundos
    total_response_time: float = 0.0
    model_inference_time: float = 0.0
    tool_execution_time: float = 0.0
    network_overhead: float = 0.0

    @property
    def tokens_per_second(self) -> float:
        """Velocidad de generación"""
        # Asumiendo output_tokens conocido externamente
        return 0.0

@dataclass
class CostMetrics:
    """Métricas de costo"""
    input_cost: float = 0.0
    output_cost: float = 0.0
    cached_cost: float = 0.0
    tool_costs: float = 0.0
    total_cost: float = 0.0

    @classmethod
    def calculate(
        cls,
        tokens: TokenUsage,
        pricing: 'ModelPricing'
    ) -> 'CostMetrics':
        """Calcula costos basado en tokens y pricing"""
        input_cost = tokens.effective_input_tokens * pricing.input_per_million / 1_000_000
        cached_cost = tokens.cached_tokens * pricing.cached_per_million / 1_000_000
        output_cost = tokens.output_tokens * pricing.output_per_million / 1_000_000

        return cls(
            input_cost=input_cost,
            output_cost=output_cost,
            cached_cost=cached_cost,
            total_cost=input_cost + output_cost + cached_cost
        )

@dataclass
class ModelPricing:
    """Precios por modelo"""
    model_name: str
    input_per_million: float  # USD
    output_per_million: float  # USD
    cached_per_million: float = 0.0  # Para context caching

    # Precios de Gemini (aproximados, verificar actuales)
    @classmethod
    def gemini_flash(cls) -> 'ModelPricing':
        return cls(
            model_name="gemini-2.0-flash",
            input_per_million=0.075,
            output_per_million=0.30,
            cached_per_million=0.01875
        )

    @classmethod
    def gemini_pro(cls) -> 'ModelPricing':
        return cls(
            model_name="gemini-1.5-pro",
            input_per_million=1.25,
            output_per_million=5.00,
            cached_per_million=0.3125
        )

@dataclass
class EfficiencyRecord:
    """Registro completo de eficiencia para una ejecución"""
    execution_id: str
    task_description: str
    timestamp: datetime
    tokens: TokenUsage
    latency: LatencyMetrics
    cost: CostMetrics
    model_used: str
    steps_count: int
    success: bool
    metadata: Dict[str, Any] = field(default_factory=dict)
```

## Tracker de Eficiencia en Tiempo Real

```python
class EfficiencyTracker:
    """Rastrea eficiencia de operaciones del agente"""

    def __init__(self, default_pricing: Optional[ModelPricing] = None):
        self.pricing = default_pricing or ModelPricing.gemini_flash()
        self.records: List[EfficiencyRecord] = []
        self.current_session_start: Optional[datetime] = None

        # Contadores de sesión
        self.session_tokens = TokenUsage()
        self.session_latency_samples: List[float] = []
        self.session_cost = 0.0

    def start_session(self):
        """Inicia una nueva sesión de tracking"""
        self.current_session_start = datetime.now()
        self.session_tokens = TokenUsage()
        self.session_latency_samples = []
        self.session_cost = 0.0

    def track_api_call(
        self,
        response: genai.types.GenerateContentResponse,
        start_time: float,
        end_time: float,
        time_to_first: Optional[float] = None
    ) -> TokenUsage:
        """
        Rastrea una llamada a la API

        Args:
            response: Respuesta de Gemini
            start_time: Timestamp de inicio
            end_time: Timestamp de fin
            time_to_first: Tiempo al primer token (streaming)
        """
        # Extraer uso de tokens de la respuesta
        usage_metadata = response.usage_metadata

        tokens = TokenUsage(
            input_tokens=usage_metadata.prompt_token_count,
            output_tokens=usage_metadata.candidates_token_count,
            cached_tokens=getattr(usage_metadata, 'cached_content_token_count', 0)
        )

        # Actualizar totales de sesión
        self.session_tokens = self.session_tokens + tokens

        # Registrar latencia
        latency = end_time - start_time
        self.session_latency_samples.append(latency)

        # Calcular costo
        cost = CostMetrics.calculate(tokens, self.pricing)
        self.session_cost += cost.total_cost

        return tokens

    def create_record(
        self,
        execution_id: str,
        task_description: str,
        tokens: TokenUsage,
        latency: LatencyMetrics,
        model_used: str,
        steps_count: int,
        success: bool,
        metadata: Optional[Dict] = None
    ) -> EfficiencyRecord:
        """Crea y almacena un registro de eficiencia"""

        cost = CostMetrics.calculate(tokens, self.pricing)

        record = EfficiencyRecord(
            execution_id=execution_id,
            task_description=task_description,
            timestamp=datetime.now(),
            tokens=tokens,
            latency=latency,
            cost=cost,
            model_used=model_used,
            steps_count=steps_count,
            success=success,
            metadata=metadata or {}
        )

        self.records.append(record)
        return record


class InstrumentedModel:
    """Wrapper de modelo con instrumentación de eficiencia"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash",
        tracker: Optional[EfficiencyTracker] = None
    ):
        self.model = genai.GenerativeModel(model_name)
        self.model_name = model_name
        self.tracker = tracker or EfficiencyTracker()

        # Contadores internos
        self._call_count = 0
        self._total_tokens = TokenUsage()
        self._latencies: List[float] = []

    async def generate_content(
        self,
        prompt: str,
        generation_config: Optional[Dict] = None
    ) -> tuple:
        """
        Genera contenido con tracking de eficiencia

        Returns:
            Tuple de (response, TokenUsage, latency_seconds)
        """
        self._call_count += 1
        start_time = time.time()

        try:
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )

            end_time = time.time()
            latency = end_time - start_time

            # Trackear
            tokens = self.tracker.track_api_call(
                response,
                start_time,
                end_time
            )

            self._total_tokens = self._total_tokens + tokens
            self._latencies.append(latency)

            return response, tokens, latency

        except Exception as e:
            end_time = time.time()
            self._latencies.append(end_time - start_time)
            raise

    async def generate_content_stream(
        self,
        prompt: str,
        generation_config: Optional[Dict] = None
    ):
        """Genera contenido en streaming con métricas"""

        self._call_count += 1
        start_time = time.time()
        time_to_first = None

        tokens = TokenUsage()
        chunks = []

        response_stream = self.model.generate_content(
            prompt,
            generation_config=generation_config,
            stream=True
        )

        for chunk in response_stream:
            if time_to_first is None:
                time_to_first = time.time() - start_time

            chunks.append(chunk.text)
            yield chunk.text

        end_time = time.time()

        # Obtener métricas finales del último chunk
        # (En producción, agregar tokens de cada chunk)
        self._latencies.append(end_time - start_time)

    def get_statistics(self) -> Dict[str, Any]:
        """Obtiene estadísticas acumuladas"""
        import statistics

        if not self._latencies:
            return {"error": "No calls recorded"}

        return {
            "call_count": self._call_count,
            "total_tokens": {
                "input": self._total_tokens.input_tokens,
                "output": self._total_tokens.output_tokens,
                "total": self._total_tokens.total_tokens
            },
            "latency": {
                "mean": statistics.mean(self._latencies),
                "median": statistics.median(self._latencies),
                "p95": sorted(self._latencies)[int(len(self._latencies) * 0.95)] if len(self._latencies) >= 20 else max(self._latencies),
                "min": min(self._latencies),
                "max": max(self._latencies)
            },
            "avg_tokens_per_call": self._total_tokens.total_tokens / self._call_count
        }
```

## Optimizador de Eficiencia

```python
from enum import Enum

class OptimizationStrategy(Enum):
    COST_FIRST = "cost_first"           # Minimizar costo
    LATENCY_FIRST = "latency_first"     # Minimizar latencia
    BALANCED = "balanced"                # Balance entre ambos
    QUALITY_FIRST = "quality_first"     # Calidad sobre eficiencia

class EfficiencyOptimizer:
    """Optimiza eficiencia de agentes"""

    def __init__(self, strategy: OptimizationStrategy = OptimizationStrategy.BALANCED):
        self.strategy = strategy
        self.model_options = {
            "flash": ModelPricing.gemini_flash(),
            "pro": ModelPricing.gemini_pro()
        }

        # Umbrales configurables
        self.max_tokens_per_task = 50000
        self.max_latency_seconds = 30.0
        self.max_cost_per_task = 0.10  # USD

    def recommend_model(
        self,
        task_complexity: str,  # "simple", "medium", "complex"
        quality_requirement: str  # "low", "medium", "high"
    ) -> str:
        """Recomienda modelo basado en requisitos"""

        if self.strategy == OptimizationStrategy.COST_FIRST:
            return "gemini-2.0-flash"

        if self.strategy == OptimizationStrategy.QUALITY_FIRST:
            return "gemini-1.5-pro"

        if self.strategy == OptimizationStrategy.LATENCY_FIRST:
            return "gemini-2.0-flash"

        # Balanced: depende de la tarea
        if task_complexity == "simple":
            return "gemini-2.0-flash"
        elif task_complexity == "complex" and quality_requirement == "high":
            return "gemini-1.5-pro"
        else:
            return "gemini-2.0-flash"

    def optimize_prompt(
        self,
        prompt: str,
        max_tokens: Optional[int] = None
    ) -> str:
        """Optimiza prompt para eficiencia de tokens"""

        optimizations = []

        # Eliminar espacios redundantes
        lines = prompt.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip()]
        optimized = '\n'.join(cleaned_lines)

        # Comprimir instrucciones repetitivas
        # (En producción, usar técnicas más sofisticadas)

        if max_tokens:
            # Truncar si excede límite
            words = optimized.split()
            estimated_tokens = len(words) * 1.3  # Aproximación

            if estimated_tokens > max_tokens:
                target_words = int(max_tokens / 1.3)
                optimized = ' '.join(words[:target_words])
                optimizations.append("truncated")

        return optimized

    def should_use_caching(
        self,
        prompt_size: int,
        expected_reuse: int
    ) -> bool:
        """Determina si usar context caching es beneficioso"""

        # Context caching es beneficioso si:
        # - El prompt es grande (>10k tokens)
        # - Se reutilizará múltiples veces

        min_size_for_caching = 10000  # tokens
        min_reuse_for_caching = 3     # veces

        return prompt_size >= min_size_for_caching and expected_reuse >= min_reuse_for_caching

    def calculate_batch_efficiency(
        self,
        individual_costs: List[float],
        batch_cost: float,
        individual_latencies: List[float],
        batch_latency: float
    ) -> Dict[str, float]:
        """Compara eficiencia de batch vs individual"""

        total_individual_cost = sum(individual_costs)
        total_individual_time = sum(individual_latencies)

        return {
            "cost_savings_percent": (1 - batch_cost / total_individual_cost) * 100 if total_individual_cost > 0 else 0,
            "time_savings_percent": (1 - batch_latency / total_individual_time) * 100 if total_individual_time > 0 else 0,
            "cost_per_item_individual": total_individual_cost / len(individual_costs) if individual_costs else 0,
            "cost_per_item_batch": batch_cost / len(individual_costs) if individual_costs else 0
        }


class TokenBudgetManager:
    """Gestiona presupuesto de tokens para tareas"""

    def __init__(self, total_budget: int, cost_limit: Optional[float] = None):
        self.total_budget = total_budget
        self.remaining_budget = total_budget
        self.cost_limit = cost_limit
        self.spent_cost = 0.0

        self.allocations: Dict[str, int] = {}
        self.usage: Dict[str, int] = {}

    def allocate(self, task_id: str, tokens: int) -> bool:
        """Aloca tokens para una tarea"""

        if tokens > self.remaining_budget:
            return False

        self.allocations[task_id] = tokens
        self.remaining_budget -= tokens
        return True

    def record_usage(
        self,
        task_id: str,
        tokens_used: int,
        cost: float
    ):
        """Registra uso real de tokens"""

        self.usage[task_id] = tokens_used
        self.spent_cost += cost

        # Ajustar remaining si usó menos de lo alocado
        allocated = self.allocations.get(task_id, 0)
        if tokens_used < allocated:
            self.remaining_budget += (allocated - tokens_used)

    def check_budget(self, required_tokens: int) -> Dict[str, Any]:
        """Verifica disponibilidad de presupuesto"""

        return {
            "available": required_tokens <= self.remaining_budget,
            "remaining_tokens": self.remaining_budget,
            "remaining_percent": self.remaining_budget / self.total_budget * 100,
            "cost_available": self.cost_limit is None or self.spent_cost < self.cost_limit,
            "spent_cost": self.spent_cost
        }

    def get_summary(self) -> Dict[str, Any]:
        """Obtiene resumen de uso de presupuesto"""

        total_used = sum(self.usage.values())

        return {
            "total_budget": self.total_budget,
            "total_used": total_used,
            "remaining": self.remaining_budget,
            "utilization_percent": total_used / self.total_budget * 100,
            "tasks_count": len(self.usage),
            "avg_tokens_per_task": total_used / len(self.usage) if self.usage else 0,
            "cost_spent": self.spent_cost
        }
```

## Analizador de Eficiencia

```python
from statistics import mean, median, stdev
from typing import Tuple

class EfficiencyAnalyzer:
    """Analiza patrones de eficiencia"""

    def __init__(self, records: List[EfficiencyRecord]):
        self.records = records

    def analyze_token_efficiency(self) -> Dict[str, Any]:
        """Analiza eficiencia de uso de tokens"""

        if not self.records:
            return {"error": "No records to analyze"}

        input_tokens = [r.tokens.input_tokens for r in self.records]
        output_tokens = [r.tokens.output_tokens for r in self.records]
        total_tokens = [r.tokens.total_tokens for r in self.records]

        # Tokens por paso
        tokens_per_step = [
            r.tokens.total_tokens / r.steps_count
            for r in self.records if r.steps_count > 0
        ]

        # Eficiencia por éxito
        successful = [r for r in self.records if r.success]
        failed = [r for r in self.records if not r.success]

        return {
            "input_tokens": {
                "mean": mean(input_tokens),
                "median": median(input_tokens),
                "std": stdev(input_tokens) if len(input_tokens) > 1 else 0
            },
            "output_tokens": {
                "mean": mean(output_tokens),
                "median": median(output_tokens),
                "std": stdev(output_tokens) if len(output_tokens) > 1 else 0
            },
            "total_tokens": {
                "mean": mean(total_tokens),
                "median": median(total_tokens),
                "total": sum(total_tokens)
            },
            "tokens_per_step": {
                "mean": mean(tokens_per_step) if tokens_per_step else 0,
                "median": median(tokens_per_step) if tokens_per_step else 0
            },
            "efficiency_by_success": {
                "successful_avg": mean([r.tokens.total_tokens for r in successful]) if successful else 0,
                "failed_avg": mean([r.tokens.total_tokens for r in failed]) if failed else 0
            },
            "input_output_ratio": mean(input_tokens) / mean(output_tokens) if mean(output_tokens) > 0 else 0
        }

    def analyze_latency(self) -> Dict[str, Any]:
        """Analiza patrones de latencia"""

        latencies = [r.latency.total_response_time for r in self.records]

        if not latencies:
            return {"error": "No latency data"}

        sorted_latencies = sorted(latencies)
        n = len(sorted_latencies)

        return {
            "mean": mean(latencies),
            "median": median(latencies),
            "std": stdev(latencies) if len(latencies) > 1 else 0,
            "min": min(latencies),
            "max": max(latencies),
            "p50": sorted_latencies[int(n * 0.50)],
            "p90": sorted_latencies[int(n * 0.90)] if n >= 10 else max(latencies),
            "p95": sorted_latencies[int(n * 0.95)] if n >= 20 else max(latencies),
            "p99": sorted_latencies[int(n * 0.99)] if n >= 100 else max(latencies),
            "under_1s": sum(1 for l in latencies if l < 1.0) / n * 100,
            "under_5s": sum(1 for l in latencies if l < 5.0) / n * 100,
            "under_10s": sum(1 for l in latencies if l < 10.0) / n * 100
        }

    def analyze_cost(self) -> Dict[str, Any]:
        """Analiza costos"""

        costs = [r.cost.total_cost for r in self.records]

        if not costs:
            return {"error": "No cost data"}

        # Costo por resultado exitoso
        successful_costs = [r.cost.total_cost for r in self.records if r.success]

        return {
            "total_cost": sum(costs),
            "mean_per_task": mean(costs),
            "median_per_task": median(costs),
            "min_per_task": min(costs),
            "max_per_task": max(costs),
            "cost_per_success": mean(successful_costs) if successful_costs else 0,
            "cost_breakdown": {
                "input": sum(r.cost.input_cost for r in self.records),
                "output": sum(r.cost.output_cost for r in self.records),
                "cached": sum(r.cost.cached_cost for r in self.records)
            },
            "projected_monthly": {
                "100_tasks_day": mean(costs) * 100 * 30,
                "1000_tasks_day": mean(costs) * 1000 * 30,
                "10000_tasks_day": mean(costs) * 10000 * 30
            }
        }

    def identify_inefficiencies(self) -> List[Dict[str, Any]]:
        """Identifica casos de ineficiencia"""

        issues = []

        token_analysis = self.analyze_token_efficiency()
        latency_analysis = self.analyze_latency()

        # Detectar outliers de tokens
        if token_analysis.get("total_tokens"):
            avg_tokens = token_analysis["total_tokens"]["mean"]
            for r in self.records:
                if r.tokens.total_tokens > avg_tokens * 3:
                    issues.append({
                        "type": "high_token_usage",
                        "execution_id": r.execution_id,
                        "tokens": r.tokens.total_tokens,
                        "avg": avg_tokens,
                        "ratio": r.tokens.total_tokens / avg_tokens
                    })

        # Detectar latencias altas
        if latency_analysis.get("p95"):
            p95 = latency_analysis["p95"]
            for r in self.records:
                if r.latency.total_response_time > p95 * 2:
                    issues.append({
                        "type": "high_latency",
                        "execution_id": r.execution_id,
                        "latency": r.latency.total_response_time,
                        "p95": p95
                    })

        # Detectar tareas fallidas con alto costo
        for r in self.records:
            if not r.success and r.cost.total_cost > 0.05:
                issues.append({
                    "type": "expensive_failure",
                    "execution_id": r.execution_id,
                    "cost": r.cost.total_cost,
                    "tokens": r.tokens.total_tokens
                })

        return issues

    def generate_report(self) -> str:
        """Genera reporte de eficiencia"""

        token_stats = self.analyze_token_efficiency()
        latency_stats = self.analyze_latency()
        cost_stats = self.analyze_cost()
        issues = self.identify_inefficiencies()

        report = []
        report.append("=" * 60)
        report.append("REPORTE DE EFICIENCIA")
        report.append("=" * 60)

        report.append(f"\nTotal de ejecuciones: {len(self.records)}")

        report.append("\n--- TOKENS ---")
        if "total_tokens" in token_stats:
            report.append(f"Total consumidos: {token_stats['total_tokens']['total']:,}")
            report.append(f"Promedio por tarea: {token_stats['total_tokens']['mean']:,.0f}")
            report.append(f"Ratio input/output: {token_stats['input_output_ratio']:.2f}")

        report.append("\n--- LATENCIA ---")
        if "mean" in latency_stats:
            report.append(f"Promedio: {latency_stats['mean']:.2f}s")
            report.append(f"P50: {latency_stats['p50']:.2f}s")
            report.append(f"P95: {latency_stats['p95']:.2f}s")
            report.append(f"Bajo 5s: {latency_stats['under_5s']:.1f}%")

        report.append("\n--- COSTO ---")
        if "total_cost" in cost_stats:
            report.append(f"Total: ${cost_stats['total_cost']:.4f}")
            report.append(f"Por tarea: ${cost_stats['mean_per_task']:.4f}")
            report.append(f"Por éxito: ${cost_stats['cost_per_success']:.4f}")
            report.append(f"Proyección 1000/día/mes: ${cost_stats['projected_monthly']['1000_tasks_day']:.2f}")

        if issues:
            report.append(f"\n--- PROBLEMAS DETECTADOS ({len(issues)}) ---")
            for issue in issues[:5]:
                report.append(f"  • {issue['type']}: {issue['execution_id']}")

        report.append("\n" + "=" * 60)

        return "\n".join(report)
```

## Ejemplo Práctico

```python
import asyncio

async def main():
    genai.configure(api_key="YOUR_API_KEY")

    # Crear modelo instrumentado
    tracker = EfficiencyTracker(
        default_pricing=ModelPricing.gemini_flash()
    )
    model = InstrumentedModel(tracker=tracker)

    # Simular tareas
    prompts = [
        "Explica qué es machine learning en 50 palabras",
        "Lista 5 frameworks de Python para web",
        "Resume los beneficios del cloud computing",
        "Describe el patrón MVC en desarrollo web",
        "Explica qué es una API REST"
    ]

    records = []

    for i, prompt in enumerate(prompts):
        start = time.time()

        response, tokens, latency = await model.generate_content(prompt)

        end = time.time()

        # Crear registro
        record = tracker.create_record(
            execution_id=f"task_{i+1:03d}",
            task_description=prompt[:50],
            tokens=tokens,
            latency=LatencyMetrics(total_response_time=latency),
            model_used="gemini-2.0-flash",
            steps_count=1,
            success=True
        )
        records.append(record)

        print(f"Task {i+1}: {tokens.total_tokens} tokens, {latency:.2f}s, ${record.cost.total_cost:.5f}")

    # Analizar eficiencia
    analyzer = EfficiencyAnalyzer(records)
    print("\n" + analyzer.generate_report())

    # Obtener estadísticas del modelo
    print("\n--- Estadísticas del Modelo ---")
    stats = model.get_statistics()
    print(f"Llamadas: {stats['call_count']}")
    print(f"Latencia media: {stats['latency']['mean']:.2f}s")
    print(f"Tokens totales: {stats['total_tokens']['total']:,}")

    # Gestión de presupuesto
    budget = TokenBudgetManager(
        total_budget=100000,
        cost_limit=1.00
    )

    # Simular alocación
    for i in range(10):
        task_id = f"future_task_{i}"
        if budget.allocate(task_id, 5000):
            # Simular uso
            budget.record_usage(task_id, 4500, 0.05)

    print("\n--- Presupuesto ---")
    summary = budget.get_summary()
    print(f"Usado: {summary['total_used']:,} / {summary['total_budget']:,}")
    print(f"Costo: ${summary['cost_spent']:.2f}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Comparador de Modelos
Implementa un sistema que ejecute la misma tarea con diferentes modelos y compare:
- Costo por tarea
- Latencia
- Calidad (usando evaluación semántica)

### Ejercicio 2: Alertas de Eficiencia
Crea un sistema de alertas que notifique cuando:
- El costo por hora exceda un umbral
- La latencia p95 supere un límite
- El token efficiency baje de cierto nivel

### Ejercicio 3: Optimizador Automático
Implementa un optimizador que automáticamente:
- Seleccione el modelo más eficiente por tipo de tarea
- Ajuste parámetros de generación
- Aplique caching cuando sea beneficioso

## Resumen

| Métrica | Descripción | Optimización |
|---------|-------------|--------------|
| Token Usage | Tokens consumidos por tarea | Prompts concisos, caching |
| Latencia | Tiempo de respuesta | Streaming, modelo más rápido |
| Costo | USD por tarea | Modelo económico, batch |
| Tokens/Step | Eficiencia por paso | Reducir pasos redundantes |
| P95 Latency | Latencia del percentil 95 | Timeouts, retry logic |
| Cost per Success | Costo por tarea exitosa | Mejorar success rate |

---

**Siguiente:** [9.1.3 Calidad de Razonamiento (Chain Quality)](./9.1.3-calidad-razonamiento.md)
