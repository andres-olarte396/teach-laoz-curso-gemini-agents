# 9.2.2 Integration Testing de Flujos

## Objetivo de Aprendizaje

Al finalizar este subtema, serás capaz de diseñar y ejecutar tests de integración que validen flujos completos de agentes, verificando la interacción correcta entre componentes y el comportamiento end-to-end.

## Introducción

Los tests de integración verifican que los componentes trabajen correctamente juntos. Para agentes, esto significa testear el flujo completo: desde el prompt inicial hasta la respuesta final, pasando por function calling, tools, y memoria.

```
┌─────────────────────────────────────────────────────────────────┐
│                  FLUJO DE INTEGRACIÓN                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  USER INPUT                                                     │
│      │                                                          │
│      ▼                                                          │
│  ┌────────┐     ┌────────┐     ┌────────┐     ┌────────┐       │
│  │ PROMPT │────►│ MODEL  │────►│ TOOLS  │────►│RESPONSE│       │
│  │PROCESS │     │  CALL  │     │  EXEC  │     │PROCESS │       │
│  └────────┘     └────────┘     └────────┘     └────────┘       │
│      │              │              │              │             │
│      ▼              ▼              ▼              ▼             │
│  ┌──────────────────────────────────────────────────────┐      │
│  │              PUNTOS DE INTEGRACIÓN                   │      │
│  │  • Prompt ↔ Model        • Model ↔ Tools            │      │
│  │  • Tools ↔ External APIs • Memory ↔ Context         │      │
│  │  • Multi-turn coherence  • Error propagation        │      │
│  └──────────────────────────────────────────────────────┘      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Framework de Integration Testing

### Configuración del Test Environment

```python
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable, Tuple
from enum import Enum
from abc import ABC, abstractmethod
import json
import asyncio
from datetime import datetime

@dataclass
class TestScenario:
    """Escenario de test de integración"""
    name: str
    description: str
    initial_context: Dict[str, Any]
    steps: List['TestStep']
    expected_final_state: Dict[str, Any]
    timeout_seconds: float = 60.0
    tags: List[str] = field(default_factory=list)

@dataclass
class TestStep:
    """Paso individual en un escenario"""
    name: str
    input_type: str  # "user_message", "tool_response", "system_event"
    input_data: Any
    expected_actions: List[str]  # Acciones esperadas del agente
    assertions: List['Assertion']
    max_duration_seconds: float = 10.0

@dataclass
class Assertion:
    """Aserción para validar comportamiento"""
    type: str  # "contains", "equals", "regex", "custom"
    target: str  # Qué validar: "response", "tool_call", "memory", "state"
    expected: Any
    custom_validator: Optional[Callable] = None

@dataclass
class TestResult:
    """Resultado de ejecución de test"""
    scenario_name: str
    passed: bool
    duration_seconds: float
    step_results: List['StepResult']
    final_state: Dict[str, Any]
    errors: List[str]
    logs: List[str]

@dataclass
class StepResult:
    """Resultado de un paso"""
    step_name: str
    passed: bool
    assertions_passed: int
    assertions_failed: int
    actual_output: Any
    errors: List[str]


class IntegrationTestRunner:
    """Ejecutor de tests de integración"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash",
        tools: Optional[List] = None
    ):
        self.model = genai.GenerativeModel(
            model_name,
            tools=tools or []
        )
        self.logs: List[str] = []
        self.current_state: Dict[str, Any] = {}

    def log(self, message: str):
        """Registra log"""
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.logs.append(f"[{timestamp}] {message}")

    async def run_scenario(
        self,
        scenario: TestScenario,
        mock_tools: Optional[Dict[str, Callable]] = None
    ) -> TestResult:
        """
        Ejecuta un escenario de test completo

        Args:
            scenario: Escenario a ejecutar
            mock_tools: Mocks para tools (nombre -> función)
        """
        self.logs = []
        self.current_state = scenario.initial_context.copy()

        start_time = datetime.now()
        step_results = []
        errors = []
        all_passed = True

        self.log(f"Iniciando escenario: {scenario.name}")

        # Inicializar chat
        chat = self.model.start_chat(history=[])

        try:
            for step in scenario.steps:
                self.log(f"Ejecutando paso: {step.name}")

                step_result = await self._execute_step(
                    chat=chat,
                    step=step,
                    mock_tools=mock_tools
                )

                step_results.append(step_result)

                if not step_result.passed:
                    all_passed = False
                    errors.extend(step_result.errors)

                    # Decidir si continuar o abortar
                    if step.input_type == "critical":
                        self.log("Paso crítico fallido, abortando")
                        break

        except asyncio.TimeoutError:
            errors.append(f"Timeout: escenario excedió {scenario.timeout_seconds}s")
            all_passed = False
        except Exception as e:
            errors.append(f"Error inesperado: {str(e)}")
            all_passed = False

        duration = (datetime.now() - start_time).total_seconds()

        # Validar estado final
        if all_passed:
            final_state_valid = self._validate_final_state(
                scenario.expected_final_state
            )
            if not final_state_valid:
                all_passed = False
                errors.append("Estado final no coincide con esperado")

        return TestResult(
            scenario_name=scenario.name,
            passed=all_passed,
            duration_seconds=duration,
            step_results=step_results,
            final_state=self.current_state,
            errors=errors,
            logs=self.logs
        )

    async def _execute_step(
        self,
        chat,
        step: TestStep,
        mock_tools: Optional[Dict[str, Callable]]
    ) -> StepResult:
        """Ejecuta un paso del escenario"""

        errors = []
        assertions_passed = 0
        assertions_failed = 0
        actual_output = None

        try:
            if step.input_type == "user_message":
                # Enviar mensaje al chat
                response = chat.send_message(step.input_data)
                actual_output = {
                    "text": response.text,
                    "candidates": response.candidates
                }

                # Procesar function calls si hay
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'function_call'):
                                    fc = part.function_call
                                    actual_output["function_call"] = {
                                        "name": fc.name,
                                        "args": dict(fc.args)
                                    }

                                    # Ejecutar mock si existe
                                    if mock_tools and fc.name in mock_tools:
                                        mock_result = mock_tools[fc.name](dict(fc.args))
                                        actual_output["tool_result"] = mock_result

            elif step.input_type == "tool_response":
                # Simular respuesta de tool
                actual_output = step.input_data

            elif step.input_type == "system_event":
                # Procesar evento del sistema
                self._handle_system_event(step.input_data)
                actual_output = {"event_processed": True}

            # Ejecutar aserciones
            for assertion in step.assertions:
                passed = self._evaluate_assertion(assertion, actual_output)
                if passed:
                    assertions_passed += 1
                else:
                    assertions_failed += 1
                    errors.append(
                        f"Aserción fallida: {assertion.type} en {assertion.target}"
                    )

        except Exception as e:
            errors.append(f"Error en paso: {str(e)}")

        return StepResult(
            step_name=step.name,
            passed=len(errors) == 0 and assertions_failed == 0,
            assertions_passed=assertions_passed,
            assertions_failed=assertions_failed,
            actual_output=actual_output,
            errors=errors
        )

    def _evaluate_assertion(
        self,
        assertion: Assertion,
        actual_output: Any
    ) -> bool:
        """Evalúa una aserción"""

        # Obtener valor a evaluar
        target_value = self._get_target_value(assertion.target, actual_output)

        if assertion.type == "contains":
            return assertion.expected in str(target_value)

        elif assertion.type == "equals":
            return target_value == assertion.expected

        elif assertion.type == "regex":
            import re
            return bool(re.search(assertion.expected, str(target_value)))

        elif assertion.type == "exists":
            return target_value is not None

        elif assertion.type == "custom":
            if assertion.custom_validator:
                return assertion.custom_validator(target_value, assertion.expected)
            return False

        elif assertion.type == "type":
            return type(target_value).__name__ == assertion.expected

        return False

    def _get_target_value(self, target: str, output: Any) -> Any:
        """Obtiene valor del target especificado"""

        if target == "response":
            return output.get("text", "") if isinstance(output, dict) else output

        elif target == "function_call":
            return output.get("function_call") if isinstance(output, dict) else None

        elif target == "tool_result":
            return output.get("tool_result") if isinstance(output, dict) else None

        elif target == "state":
            return self.current_state

        elif target.startswith("state."):
            key = target[6:]
            return self.current_state.get(key)

        return output

    def _validate_final_state(self, expected: Dict[str, Any]) -> bool:
        """Valida que el estado final sea correcto"""

        for key, expected_value in expected.items():
            if key not in self.current_state:
                self.log(f"Estado faltante: {key}")
                return False

            if self.current_state[key] != expected_value:
                self.log(f"Estado incorrecto: {key}")
                return False

        return True

    def _handle_system_event(self, event_data: Dict):
        """Procesa evento del sistema"""
        event_type = event_data.get("type")

        if event_type == "state_update":
            self.current_state.update(event_data.get("data", {}))

        elif event_type == "reset":
            self.current_state = {}
```

## Test de Flujos Multi-Turn

```python
class MultiTurnTestSuite:
    """Suite de tests para conversaciones multi-turn"""

    def __init__(self, runner: IntegrationTestRunner):
        self.runner = runner

    def create_conversation_scenario(
        self,
        name: str,
        turns: List[Tuple[str, List[Assertion]]]
    ) -> TestScenario:
        """
        Crea escenario de conversación

        Args:
            name: Nombre del escenario
            turns: Lista de (mensaje_usuario, aserciones)
        """
        steps = []
        for i, (message, assertions) in enumerate(turns):
            steps.append(TestStep(
                name=f"turn_{i+1}",
                input_type="user_message",
                input_data=message,
                expected_actions=["respond"],
                assertions=assertions
            ))

        return TestScenario(
            name=name,
            description=f"Conversación de {len(turns)} turnos",
            initial_context={},
            steps=steps,
            expected_final_state={},
            tags=["multi-turn", "conversation"]
        )

    async def test_context_retention(self) -> TestResult:
        """Test: El agente recuerda contexto entre turnos"""

        scenario = self.create_conversation_scenario(
            name="context_retention",
            turns=[
                (
                    "Mi nombre es Carlos",
                    [Assertion("contains", "response", "Carlos")]
                ),
                (
                    "¿Cuál es mi nombre?",
                    [Assertion("contains", "response", "Carlos")]
                ),
                (
                    "Tengo 30 años",
                    [Assertion("exists", "response", None)]
                ),
                (
                    "¿Qué información tienes sobre mí?",
                    [
                        Assertion("contains", "response", "Carlos"),
                        Assertion("contains", "response", "30")
                    ]
                )
            ]
        )

        return await self.runner.run_scenario(scenario)

    async def test_topic_switching(self) -> TestResult:
        """Test: El agente maneja cambios de tema"""

        scenario = self.create_conversation_scenario(
            name="topic_switching",
            turns=[
                (
                    "Explícame qué es Python",
                    [Assertion("contains", "response", "Python")]
                ),
                (
                    "Ahora háblame de JavaScript",
                    [
                        Assertion("contains", "response", "JavaScript"),
                        # No debería mezclar con Python
                    ]
                ),
                (
                    "Compara ambos lenguajes",
                    [
                        Assertion("contains", "response", "Python"),
                        Assertion("contains", "response", "JavaScript")
                    ]
                )
            ]
        )

        return await self.runner.run_scenario(scenario)

    async def test_error_recovery(self) -> TestResult:
        """Test: El agente se recupera de errores"""

        steps = [
            TestStep(
                name="initial_request",
                input_type="user_message",
                input_data="Busca información sobre AI",
                expected_actions=["tool_call"],
                assertions=[
                    Assertion("exists", "function_call", None)
                ]
            ),
            TestStep(
                name="tool_error",
                input_type="tool_response",
                input_data={"error": "Service unavailable"},
                expected_actions=["handle_error"],
                assertions=[]
            ),
            TestStep(
                name="retry_or_alternative",
                input_type="user_message",
                input_data="Intenta de otra forma",
                expected_actions=["respond"],
                assertions=[
                    Assertion("exists", "response", None)
                ]
            )
        ]

        scenario = TestScenario(
            name="error_recovery",
            description="Test de recuperación ante errores",
            initial_context={},
            steps=steps,
            expected_final_state={},
            tags=["error-handling", "resilience"]
        )

        return await self.runner.run_scenario(scenario)
```

## Test de Tool Chains

```python
class ToolChainTester:
    """Tests para cadenas de herramientas"""

    def __init__(self, runner: IntegrationTestRunner):
        self.runner = runner
        self.tool_call_log: List[Dict] = []

    def mock_tool_factory(self, tool_name: str, response: Any) -> Callable:
        """Crea mock de tool que registra llamadas"""

        def mock_tool(args: Dict) -> Any:
            self.tool_call_log.append({
                "tool": tool_name,
                "args": args,
                "response": response
            })
            return response

        return mock_tool

    async def test_sequential_tools(self) -> TestResult:
        """Test: Tools se ejecutan en secuencia correcta"""

        self.tool_call_log = []

        # Mocks de tools
        mock_tools = {
            "search": self.mock_tool_factory("search", {
                "results": [{"title": "AI Overview", "content": "..."}]
            }),
            "summarize": self.mock_tool_factory("summarize", {
                "summary": "AI is transforming technology..."
            })
        }

        steps = [
            TestStep(
                name="complex_request",
                input_type="user_message",
                input_data="Busca información sobre AI y resúmela",
                expected_actions=["search", "summarize"],
                assertions=[
                    Assertion("exists", "response", None)
                ]
            )
        ]

        scenario = TestScenario(
            name="sequential_tools",
            description="Verifica ejecución secuencial de tools",
            initial_context={},
            steps=steps,
            expected_final_state={},
            tags=["tools", "sequence"]
        )

        result = await self.runner.run_scenario(scenario, mock_tools)

        # Verificar orden de llamadas
        if len(self.tool_call_log) >= 2:
            # Search debería ir antes de summarize
            search_idx = next(
                (i for i, call in enumerate(self.tool_call_log)
                 if call["tool"] == "search"),
                -1
            )
            summarize_idx = next(
                (i for i, call in enumerate(self.tool_call_log)
                 if call["tool"] == "summarize"),
                -1
            )

            if search_idx > summarize_idx:
                result.errors.append("Orden incorrecto: summarize antes de search")
                result.passed = False

        return result

    async def test_parallel_tools(self) -> TestResult:
        """Test: Tools se ejecutan en paralelo cuando es posible"""

        self.tool_call_log = []

        mock_tools = {
            "get_weather": self.mock_tool_factory("get_weather", {"temp": 25}),
            "get_news": self.mock_tool_factory("get_news", {"headlines": []})
        }

        steps = [
            TestStep(
                name="parallel_request",
                input_type="user_message",
                input_data="Dame el clima y las noticias de hoy",
                expected_actions=["get_weather", "get_news"],
                assertions=[]
            )
        ]

        scenario = TestScenario(
            name="parallel_tools",
            description="Verifica ejecución paralela de tools",
            initial_context={},
            steps=steps,
            expected_final_state={},
            tags=["tools", "parallel"]
        )

        return await self.runner.run_scenario(scenario, mock_tools)

    async def test_conditional_tool_use(self) -> TestResult:
        """Test: Agent decide cuándo usar tools"""

        self.tool_call_log = []

        mock_tools = {
            "calculator": self.mock_tool_factory("calculator", {"result": 42})
        }

        # Primer paso: pregunta que NO necesita tool
        # Segundo paso: pregunta que SÍ necesita tool
        steps = [
            TestStep(
                name="no_tool_needed",
                input_type="user_message",
                input_data="¿Cuál es la capital de Francia?",
                expected_actions=["respond"],
                assertions=[
                    Assertion("contains", "response", "París")
                ]
            ),
            TestStep(
                name="tool_needed",
                input_type="user_message",
                input_data="Calcula 15 * 2.8",
                expected_actions=["calculator"],
                assertions=[
                    Assertion("exists", "function_call", None)
                ]
            )
        ]

        scenario = TestScenario(
            name="conditional_tool_use",
            description="Verifica decisión inteligente de uso de tools",
            initial_context={},
            steps=steps,
            expected_final_state={},
            tags=["tools", "conditional"]
        )

        result = await self.runner.run_scenario(scenario, mock_tools)

        # Verificar que calculator solo se llamó en el segundo paso
        calculator_calls = [c for c in self.tool_call_log if c["tool"] == "calculator"]
        if len(calculator_calls) != 1:
            result.errors.append(
                f"Calculator debería llamarse 1 vez, se llamó {len(calculator_calls)}"
            )

        return result
```

## Test de Estado y Memoria

```python
class StateAndMemoryTester:
    """Tests para estado y memoria del agente"""

    def __init__(self, runner: IntegrationTestRunner):
        self.runner = runner

    async def test_state_persistence(self) -> TestResult:
        """Test: Estado persiste correctamente entre operaciones"""

        steps = [
            TestStep(
                name="set_preference",
                input_type="user_message",
                input_data="Prefiero respuestas cortas",
                expected_actions=["acknowledge", "update_state"],
                assertions=[
                    Assertion("contains", "response", "cortas")
                ]
            ),
            TestStep(
                name="update_state",
                input_type="system_event",
                input_data={
                    "type": "state_update",
                    "data": {"response_style": "concise"}
                },
                expected_actions=[],
                assertions=[]
            ),
            TestStep(
                name="verify_preference_applied",
                input_type="user_message",
                input_data="Explica qué es machine learning",
                expected_actions=["respond"],
                assertions=[
                    # La respuesta debería ser concisa
                    Assertion(
                        "custom",
                        "response",
                        200,  # máximo de caracteres esperados
                        lambda text, max_len: len(str(text)) < max_len
                    )
                ]
            )
        ]

        scenario = TestScenario(
            name="state_persistence",
            description="Verifica persistencia de estado",
            initial_context={"response_style": "default"},
            steps=steps,
            expected_final_state={"response_style": "concise"},
            tags=["state", "persistence"]
        )

        return await self.runner.run_scenario(scenario)

    async def test_memory_retrieval(self) -> TestResult:
        """Test: Memoria se recupera correctamente"""

        # Simular memoria preexistente
        initial_memory = {
            "user_facts": [
                {"fact": "Usuario trabaja en tecnología", "confidence": 0.9},
                {"fact": "Usuario prefiere Python", "confidence": 0.8}
            ],
            "conversation_summary": "Usuario preguntó sobre frameworks web"
        }

        steps = [
            TestStep(
                name="query_requiring_memory",
                input_type="user_message",
                input_data="¿Qué framework me recomendarías basado en lo que sabes de mí?",
                expected_actions=["retrieve_memory", "respond"],
                assertions=[
                    # Debería mencionar Python o tecnología
                    Assertion("regex", "response", r"(Python|Django|Flask|tecnología)")
                ]
            )
        ]

        scenario = TestScenario(
            name="memory_retrieval",
            description="Verifica recuperación de memoria",
            initial_context={"memory": initial_memory},
            steps=steps,
            expected_final_state={},
            tags=["memory", "retrieval"]
        )

        return await self.runner.run_scenario(scenario)


class EndToEndTestSuite:
    """Suite de tests end-to-end"""

    def __init__(self, model_name: str = "gemini-2.0-flash"):
        self.runner = IntegrationTestRunner(model_name=model_name)
        self.multi_turn = MultiTurnTestSuite(self.runner)
        self.tool_chain = ToolChainTester(self.runner)
        self.state_memory = StateAndMemoryTester(self.runner)
        self.results: List[TestResult] = []

    async def run_all_tests(self) -> Dict[str, Any]:
        """Ejecuta todos los tests de integración"""

        test_methods = [
            ("context_retention", self.multi_turn.test_context_retention),
            ("topic_switching", self.multi_turn.test_topic_switching),
            ("error_recovery", self.multi_turn.test_error_recovery),
            ("sequential_tools", self.tool_chain.test_sequential_tools),
            ("conditional_tool_use", self.tool_chain.test_conditional_tool_use),
            ("state_persistence", self.state_memory.test_state_persistence),
        ]

        for name, test_method in test_methods:
            print(f"Ejecutando: {name}...")
            try:
                result = await test_method()
                self.results.append(result)
                status = "✓" if result.passed else "✗"
                print(f"  {status} {result.duration_seconds:.2f}s")
            except Exception as e:
                print(f"  ✗ Error: {str(e)}")
                self.results.append(TestResult(
                    scenario_name=name,
                    passed=False,
                    duration_seconds=0,
                    step_results=[],
                    final_state={},
                    errors=[str(e)],
                    logs=[]
                ))

        return self.generate_report()

    def generate_report(self) -> Dict[str, Any]:
        """Genera reporte de resultados"""

        passed = sum(1 for r in self.results if r.passed)
        failed = len(self.results) - passed

        return {
            "summary": {
                "total": len(self.results),
                "passed": passed,
                "failed": failed,
                "success_rate": passed / len(self.results) if self.results else 0
            },
            "results": [
                {
                    "name": r.scenario_name,
                    "passed": r.passed,
                    "duration": r.duration_seconds,
                    "errors": r.errors
                }
                for r in self.results
            ],
            "total_duration": sum(r.duration_seconds for r in self.results)
        }
```

## Ejemplo Práctico Completo

```python
import asyncio

async def main():
    genai.configure(api_key="YOUR_API_KEY")

    print("=" * 60)
    print("INTEGRATION TEST SUITE")
    print("=" * 60)

    # Crear suite de tests
    suite = EndToEndTestSuite(model_name="gemini-2.0-flash")

    # Ejecutar todos los tests
    report = await suite.run_all_tests()

    # Mostrar reporte
    print("\n" + "=" * 60)
    print("REPORTE FINAL")
    print("=" * 60)

    summary = report["summary"]
    print(f"\nTotal: {summary['total']}")
    print(f"Pasados: {summary['passed']}")
    print(f"Fallidos: {summary['failed']}")
    print(f"Tasa de éxito: {summary['success_rate']:.1%}")
    print(f"Duración total: {report['total_duration']:.2f}s")

    if summary['failed'] > 0:
        print("\n--- Tests Fallidos ---")
        for result in report["results"]:
            if not result["passed"]:
                print(f"\n  {result['name']}:")
                for error in result["errors"][:3]:
                    print(f"    • {error}")

    # Test manual de un escenario específico
    print("\n" + "=" * 60)
    print("TEST MANUAL: Conversación compleja")
    print("=" * 60)

    runner = IntegrationTestRunner()

    custom_scenario = TestScenario(
        name="complex_conversation",
        description="Conversación con múltiples capacidades",
        initial_context={"session_id": "test_123"},
        steps=[
            TestStep(
                name="greeting",
                input_type="user_message",
                input_data="Hola, soy un desarrollador Python",
                expected_actions=["greet", "acknowledge"],
                assertions=[
                    Assertion("exists", "response", None),
                    Assertion("regex", "response", r"[Hh]ola|[Bb]ienvenido")
                ]
            ),
            TestStep(
                name="technical_question",
                input_type="user_message",
                input_data="¿Cuáles son las mejores prácticas para testing en Python?",
                expected_actions=["respond"],
                assertions=[
                    Assertion("contains", "response", "pytest"),
                ]
            ),
            TestStep(
                name="follow_up",
                input_type="user_message",
                input_data="Dame un ejemplo de test unitario",
                expected_actions=["respond"],
                assertions=[
                    Assertion("regex", "response", r"def test_|assert")
                ]
            )
        ],
        expected_final_state={},
        tags=["manual", "complex"]
    )

    result = await runner.run_scenario(custom_scenario)

    print(f"\nResultado: {'PASÓ' if result.passed else 'FALLÓ'}")
    print(f"Duración: {result.duration_seconds:.2f}s")

    print("\n--- Pasos ---")
    for step_result in result.step_results:
        status = "✓" if step_result.passed else "✗"
        print(f"  {status} {step_result.step_name}")
        print(f"    Aserciones: {step_result.assertions_passed} pasadas, {step_result.assertions_failed} fallidas")

    if result.errors:
        print("\n--- Errores ---")
        for error in result.errors:
            print(f"  • {error}")

    print("\n--- Logs ---")
    for log in result.logs[-10:]:
        print(f"  {log}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Ejercicios Prácticos

### Ejercicio 1: Test de Flujo de Compra
Crea tests de integración para un agente de e-commerce:
- Búsqueda de productos
- Añadir al carrito
- Proceso de checkout
- Confirmación de orden

### Ejercicio 2: Test de Manejo de Errores en Cascada
Implementa tests que verifiquen:
- Error en primera tool se propaga correctamente
- Agent intenta alternativas
- Mensaje de error final es útil

### Ejercicio 3: Test de Performance
Añade métricas de performance a los tests:
- Latencia por paso
- Consumo de tokens
- Alertas si excede umbrales

## Resumen

| Concepto | Descripción |
|----------|-------------|
| TestScenario | Define flujo completo a testear |
| TestStep | Paso individual con aserciones |
| IntegrationTestRunner | Ejecuta escenarios de test |
| Multi-turn Testing | Tests de conversaciones largas |
| Tool Chain Testing | Tests de secuencias de tools |
| State Testing | Tests de persistencia de estado |

---

**Siguiente:** [9.2.3 Adversarial Testing y Edge Cases](./9.2.3-adversarial-testing.md)
