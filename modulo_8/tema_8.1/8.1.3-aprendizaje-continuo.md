# 8.1.3 Agentes con Aprendizaje Continuo

## Objetivo de Aprendizaje

Implementar agentes que mejoran su rendimiento a travÃ©s de la experiencia, manteniendo una base de conocimiento evolutiva, aprendiendo de errores y adaptÃ¡ndose a nuevos contextos usando Google Gemini.

## IntroducciÃ³n

Los agentes con aprendizaje continuo van mÃ¡s allÃ¡ de la simple ejecuciÃ³n de tareas. Mantienen un registro de experiencias, extraen patrones de Ã©xitos y fracasos, y utilizan este conocimiento para mejorar decisiones futuras. Este enfoque se inspira en el aprendizaje por refuerzo y la memoria episÃ³dica humana.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONTINUOUS LEARNING AGENT ARCHITECTURE             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                    EXPERIENCE LOOP                       â”‚  â”‚
â”‚   â”‚                                                          â”‚  â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚   â”‚    â”‚ ACTION  â”‚â”€â”€â”€>â”‚ OUTCOME â”‚â”€â”€â”€>â”‚ REFLECT â”‚            â”‚  â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚   â”‚         ^                              â”‚                 â”‚  â”‚
â”‚   â”‚         â”‚                              v                 â”‚  â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚   â”‚    â”‚ DECIDE  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  LEARN  â”‚            â”‚  â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚   â”‚         ^                              â”‚                 â”‚  â”‚
â”‚   â”‚         â”‚                              v                 â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                              â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                   KNOWLEDGE BASE                         â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚  â”‚
â”‚   â”‚  â”‚ Patterns â”‚  â”‚ Heuristicsâ”‚  â”‚ Failures â”‚              â”‚  â”‚
â”‚   â”‚  â”‚ (Ã©xitos) â”‚  â”‚  (reglas) â”‚  â”‚(errores) â”‚              â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚   MÃ‰TRICAS: Success Rate â†‘ | Error Rate â†“ | Adaptation Speed   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ImplementaciÃ³n del Sistema de Aprendizaje

```python
"""
Agente con aprendizaje continuo usando Google Gemini.
"""
import google.generativeai as genai
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from enum import Enum
import json
import os
import numpy as np
from collections import defaultdict


class OutcomeType(Enum):
    """Tipos de resultado."""
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FAILURE = "failure"
    ERROR = "error"


@dataclass
class Experience:
    """Representa una experiencia del agente."""
    experience_id: str
    context: Dict[str, Any]
    action_taken: str
    action_params: Dict[str, Any]
    outcome: OutcomeType
    outcome_details: str
    reward: float  # -1.0 a 1.0
    timestamp: datetime = field(default_factory=datetime.now)
    lessons_learned: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "experience_id": self.experience_id,
            "context": self.context,
            "action_taken": self.action_taken,
            "action_params": self.action_params,
            "outcome": self.outcome.value,
            "outcome_details": self.outcome_details,
            "reward": self.reward,
            "timestamp": self.timestamp.isoformat(),
            "lessons_learned": self.lessons_learned,
            "tags": self.tags
        }


@dataclass
class Heuristic:
    """Regla heurÃ­stica aprendida."""
    heuristic_id: str
    condition: str  # CuÃ¡ndo aplicar
    action: str  # QuÃ© hacer
    confidence: float  # 0-1
    success_count: int = 0
    failure_count: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    last_used: Optional[datetime] = None

    @property
    def success_rate(self) -> float:
        total = self.success_count + self.failure_count
        return self.success_count / total if total > 0 else 0.5

    def update(self, success: bool):
        """Actualiza estadÃ­sticas."""
        if success:
            self.success_count += 1
        else:
            self.failure_count += 1
        self.last_used = datetime.now()
        # Actualizar confianza
        self.confidence = 0.7 * self.confidence + 0.3 * self.success_rate


class ExperienceMemory:
    """Memoria de experiencias del agente."""

    def __init__(self, max_experiences: int = 1000):
        self.experiences: List[Experience] = []
        self.max_experiences = max_experiences
        self.experience_index: Dict[str, List[int]] = defaultdict(list)

    def add(self, experience: Experience):
        """Agrega experiencia a la memoria."""
        idx = len(self.experiences)
        self.experiences.append(experience)

        # Indexar por tags
        for tag in experience.tags:
            self.experience_index[tag].append(idx)

        # Indexar por acciÃ³n
        self.experience_index[f"action:{experience.action_taken}"].append(idx)

        # Indexar por resultado
        self.experience_index[f"outcome:{experience.outcome.value}"].append(idx)

        # Mantener lÃ­mite
        if len(self.experiences) > self.max_experiences:
            self._prune_experiences()

    def search(
        self,
        context: Dict[str, Any] = None,
        action: str = None,
        outcome: OutcomeType = None,
        tags: List[str] = None,
        limit: int = 10
    ) -> List[Experience]:
        """Busca experiencias relevantes."""
        candidates = set(range(len(self.experiences)))

        # Filtrar por acciÃ³n
        if action:
            action_indices = set(self.experience_index.get(f"action:{action}", []))
            candidates &= action_indices

        # Filtrar por resultado
        if outcome:
            outcome_indices = set(self.experience_index.get(f"outcome:{outcome.value}", []))
            candidates &= outcome_indices

        # Filtrar por tags
        if tags:
            for tag in tags:
                tag_indices = set(self.experience_index.get(tag, []))
                candidates &= tag_indices

        # Obtener experiencias
        results = [self.experiences[i] for i in candidates]

        # Ordenar por relevancia (recencia + reward)
        results.sort(
            key=lambda e: (e.reward, e.timestamp),
            reverse=True
        )

        return results[:limit]

    def get_success_patterns(self, action: str) -> List[Experience]:
        """Obtiene experiencias exitosas para una acciÃ³n."""
        return self.search(
            action=action,
            outcome=OutcomeType.SUCCESS,
            limit=5
        )

    def get_failure_patterns(self, action: str) -> List[Experience]:
        """Obtiene experiencias fallidas para una acciÃ³n."""
        return self.search(
            action=action,
            outcome=OutcomeType.FAILURE,
            limit=5
        )

    def _prune_experiences(self):
        """Elimina experiencias menos valiosas."""
        # Mantener experiencias con alto reward o recientes
        scored = [
            (i, exp.reward + (1 if (datetime.now() - exp.timestamp).days < 7 else 0))
            for i, exp in enumerate(self.experiences)
        ]
        scored.sort(key=lambda x: x[1], reverse=True)

        keep_indices = set(idx for idx, _ in scored[:self.max_experiences // 2])
        self.experiences = [
            exp for i, exp in enumerate(self.experiences)
            if i in keep_indices
        ]

        # Reconstruir Ã­ndice
        self.experience_index.clear()
        for i, exp in enumerate(self.experiences):
            for tag in exp.tags:
                self.experience_index[tag].append(i)
            self.experience_index[f"action:{exp.action_taken}"].append(i)
            self.experience_index[f"outcome:{exp.outcome.value}"].append(i)

    def get_statistics(self) -> Dict[str, Any]:
        """Obtiene estadÃ­sticas de la memoria."""
        if not self.experiences:
            return {"total": 0}

        outcomes = defaultdict(int)
        for exp in self.experiences:
            outcomes[exp.outcome.value] += 1

        return {
            "total": len(self.experiences),
            "outcomes": dict(outcomes),
            "avg_reward": np.mean([e.reward for e in self.experiences]),
            "success_rate": outcomes["success"] / len(self.experiences)
        }


class HeuristicLearner:
    """Aprende y gestiona heurÃ­sticas."""

    def __init__(self, api_key: str = None):
        genai.configure(api_key=api_key or os.getenv("GOOGLE_API_KEY"))
        self.model = genai.GenerativeModel("gemini-1.5-flash")
        self.heuristics: Dict[str, Heuristic] = {}

    def extract_heuristics(
        self,
        experiences: List[Experience]
    ) -> List[Heuristic]:
        """Extrae heurÃ­sticas de experiencias."""
        if len(experiences) < 3:
            return []

        # Preparar datos para anÃ¡lisis
        exp_summaries = [
            f"Contexto: {json.dumps(e.context)[:200]}, "
            f"AcciÃ³n: {e.action_taken}, "
            f"Resultado: {e.outcome.value}, "
            f"Reward: {e.reward}"
            for e in experiences[:10]
        ]

        prompt = f"""Analiza estas experiencias de un agente y extrae reglas heurÃ­sticas.

EXPERIENCIAS:
{chr(10).join(exp_summaries)}

Identifica patrones de Ã©xito y fracaso. Genera reglas en formato JSON:
{{
    "heuristics": [
        {{
            "condition": "cuando [situaciÃ³n especÃ­fica]",
            "action": "entonces [acciÃ³n recomendada]",
            "confidence": 0.0-1.0,
            "reasoning": "porque [razÃ³n]"
        }}
    ]
}}

Genera solo heurÃ­sticas con soporte en los datos:"""

        response = self.model.generate_content(prompt)

        try:
            text = response.text
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]

            result = json.loads(text.strip())
            new_heuristics = []

            for h_data in result.get("heuristics", []):
                heuristic = Heuristic(
                    heuristic_id=f"h_{len(self.heuristics) + len(new_heuristics)}",
                    condition=h_data.get("condition", ""),
                    action=h_data.get("action", ""),
                    confidence=float(h_data.get("confidence", 0.5))
                )
                new_heuristics.append(heuristic)
                self.heuristics[heuristic.heuristic_id] = heuristic

            return new_heuristics

        except (json.JSONDecodeError, KeyError):
            return []

    def get_applicable_heuristics(
        self,
        context: Dict[str, Any]
    ) -> List[Tuple[Heuristic, float]]:
        """Encuentra heurÃ­sticas aplicables al contexto."""
        if not self.heuristics:
            return []

        context_str = json.dumps(context)
        heuristics_desc = "\n".join([
            f"ID: {h.heuristic_id}, CondiciÃ³n: {h.condition}, AcciÃ³n: {h.action}, Confianza: {h.confidence:.2f}"
            for h in self.heuristics.values()
        ])

        prompt = f"""Dado el contexto actual, determina quÃ© heurÃ­sticas son aplicables.

CONTEXTO ACTUAL:
{context_str}

HEURÃSTICAS DISPONIBLES:
{heuristics_desc}

Responde con JSON:
{{
    "applicable": [
        {{"heuristic_id": "id", "relevance": 0.0-1.0, "reason": "razÃ³n"}}
    ]
}}"""

        response = self.model.generate_content(prompt)

        try:
            text = response.text
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]

            result = json.loads(text.strip())
            applicable = []

            for item in result.get("applicable", []):
                h_id = item.get("heuristic_id")
                if h_id in self.heuristics:
                    relevance = float(item.get("relevance", 0.5))
                    applicable.append((self.heuristics[h_id], relevance))

            return sorted(applicable, key=lambda x: x[1], reverse=True)

        except (json.JSONDecodeError, KeyError):
            return []

    def update_heuristic(self, heuristic_id: str, success: bool):
        """Actualiza heurÃ­stica basado en resultado."""
        if heuristic_id in self.heuristics:
            self.heuristics[heuristic_id].update(success)

    def prune_weak_heuristics(self, min_confidence: float = 0.3):
        """Elimina heurÃ­sticas con baja confianza."""
        to_remove = [
            h_id for h_id, h in self.heuristics.items()
            if h.confidence < min_confidence and (h.success_count + h.failure_count) > 5
        ]
        for h_id in to_remove:
            del self.heuristics[h_id]


class ContinuousLearningAgent:
    """Agente con capacidad de aprendizaje continuo."""

    def __init__(
        self,
        name: str,
        api_key: str = None,
        learning_rate: float = 0.1
    ):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel("gemini-1.5-flash")

        self.name = name
        self.learning_rate = learning_rate

        self.experience_memory = ExperienceMemory()
        self.heuristic_learner = HeuristicLearner(self.api_key)

        self.action_success_rates: Dict[str, List[float]] = defaultdict(list)
        self.experience_counter = 0

    def _generate_experience_id(self) -> str:
        self.experience_counter += 1
        return f"exp_{self.experience_counter}"

    def decide(self, context: Dict[str, Any], available_actions: List[str]) -> str:
        """Decide la mejor acciÃ³n basÃ¡ndose en experiencia."""
        # Buscar heurÃ­sticas aplicables
        applicable_heuristics = self.heuristic_learner.get_applicable_heuristics(context)

        # Buscar experiencias similares
        relevant_experiences = []
        for action in available_actions:
            successes = self.experience_memory.get_success_patterns(action)
            failures = self.experience_memory.get_failure_patterns(action)
            relevant_experiences.extend(successes[:2])
            relevant_experiences.extend(failures[:1])

        # Preparar contexto para decisiÃ³n
        heuristics_text = "\n".join([
            f"- {h.condition} â†’ {h.action} (confianza: {h.confidence:.2f})"
            for h, _ in applicable_heuristics[:3]
        ])

        experience_text = "\n".join([
            f"- AcciÃ³n '{e.action_taken}': {e.outcome.value} (reward: {e.reward:.2f})"
            for e in relevant_experiences[:5]
        ])

        # Calcular tasas de Ã©xito
        action_stats = {}
        for action in available_actions:
            rates = self.action_success_rates.get(action, [0.5])
            action_stats[action] = np.mean(rates[-10:])  # Ãšltimas 10

        prompt = f"""Eres un agente que aprende de la experiencia. Decide la mejor acciÃ³n.

CONTEXTO ACTUAL:
{json.dumps(context, indent=2)}

ACCIONES DISPONIBLES:
{available_actions}

HEURÃSTICAS APRENDIDAS:
{heuristics_text if heuristics_text else "Ninguna aÃºn."}

EXPERIENCIAS RELEVANTES:
{experience_text if experience_text else "Sin experiencias previas similares."}

TASAS DE Ã‰XITO HISTÃ“RICAS:
{json.dumps(action_stats, indent=2)}

Selecciona la mejor acciÃ³n considerando:
1. HeurÃ­sticas con alta confianza
2. Experiencias pasadas exitosas
3. Evitar acciones con alto historial de fallos

Responde con JSON:
{{
    "selected_action": "nombre_accion",
    "reasoning": "por quÃ© esta acciÃ³n",
    "confidence": 0.0-1.0,
    "risk_assessment": "bajo|medio|alto"
}}"""

        response = self.model.generate_content(prompt)

        try:
            text = response.text
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]

            result = json.loads(text.strip())
            return result.get("selected_action", available_actions[0])

        except (json.JSONDecodeError, KeyError):
            # Fallback: elegir basado en tasa de Ã©xito
            best_action = max(available_actions, key=lambda a: action_stats.get(a, 0.5))
            return best_action

    def execute_and_learn(
        self,
        context: Dict[str, Any],
        action: str,
        action_params: Dict[str, Any],
        executor: callable
    ) -> Experience:
        """Ejecuta acciÃ³n y aprende del resultado."""
        # Ejecutar
        try:
            result = executor(action, action_params)
            outcome = OutcomeType.SUCCESS if result.get("success", False) else OutcomeType.FAILURE
            outcome_details = result.get("details", "")
            reward = result.get("reward", 1.0 if outcome == OutcomeType.SUCCESS else -0.5)
        except Exception as e:
            outcome = OutcomeType.ERROR
            outcome_details = str(e)
            reward = -1.0

        # Crear experiencia
        experience = Experience(
            experience_id=self._generate_experience_id(),
            context=context,
            action_taken=action,
            action_params=action_params,
            outcome=outcome,
            outcome_details=outcome_details,
            reward=reward,
            tags=self._extract_tags(context, action)
        )

        # Extraer lecciones
        experience.lessons_learned = self._extract_lessons(experience)

        # Guardar en memoria
        self.experience_memory.add(experience)

        # Actualizar tasas de Ã©xito
        success_value = 1.0 if outcome == OutcomeType.SUCCESS else 0.0
        self.action_success_rates[action].append(success_value)

        # Aprendizaje periÃ³dico
        if len(self.experience_memory.experiences) % 10 == 0:
            self._periodic_learning()

        return experience

    def _extract_tags(self, context: Dict[str, Any], action: str) -> List[str]:
        """Extrae tags del contexto."""
        tags = [action]

        # Agregar tags basados en contexto
        if "type" in context:
            tags.append(f"type:{context['type']}")
        if "domain" in context:
            tags.append(f"domain:{context['domain']}")

        return tags

    def _extract_lessons(self, experience: Experience) -> List[str]:
        """Extrae lecciones de una experiencia."""
        prompt = f"""Analiza esta experiencia y extrae 1-3 lecciones clave.

EXPERIENCIA:
- Contexto: {json.dumps(experience.context)[:300]}
- AcciÃ³n: {experience.action_taken}
- Resultado: {experience.outcome.value}
- Detalles: {experience.outcome_details[:200]}
- Reward: {experience.reward}

Extrae lecciones concretas y accionables. Responde con JSON:
{{
    "lessons": [
        "lecciÃ³n 1",
        "lecciÃ³n 2"
    ]
}}"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            result = json.loads(text.strip())
            return result.get("lessons", [])
        except:
            return []

    def _periodic_learning(self):
        """Ejecuta aprendizaje periÃ³dico."""
        # Extraer nuevas heurÃ­sticas de experiencias recientes
        recent = self.experience_memory.experiences[-20:]
        new_heuristics = self.heuristic_learner.extract_heuristics(recent)

        if new_heuristics:
            print(f"ğŸ“š Aprendidas {len(new_heuristics)} nuevas heurÃ­sticas")

        # Podar heurÃ­sticas dÃ©biles
        self.heuristic_learner.prune_weak_heuristics()

    def reflect(self) -> Dict[str, Any]:
        """Genera reflexiÃ³n sobre el aprendizaje."""
        stats = self.experience_memory.get_statistics()

        prompt = f"""Como agente de aprendizaje, reflexiona sobre tu desempeÃ±o.

ESTADÃSTICAS:
{json.dumps(stats, indent=2)}

HEURÃSTICAS ACTUALES: {len(self.heuristic_learner.heuristics)}

Genera una reflexiÃ³n sobre:
1. QuÃ© has aprendido
2. Ãreas de mejora
3. Patrones identificados
4. PrÃ³ximos pasos sugeridos

ReflexiÃ³n:"""

        response = self.model.generate_content(prompt)

        return {
            "statistics": stats,
            "heuristics_count": len(self.heuristic_learner.heuristics),
            "reflection": response.text
        }


# Ejemplo de uso
if __name__ == "__main__":
    agent = ContinuousLearningAgent(name="LearnerBot")

    # Simulador de ejecuciÃ³n
    def mock_executor(action: str, params: Dict[str, Any]) -> Dict[str, Any]:
        import random
        success = random.random() > 0.3
        return {
            "success": success,
            "details": f"Ejecutado {action} con params {params}",
            "reward": 1.0 if success else -0.5
        }

    # Simular varias interacciones
    actions = ["search", "analyze", "generate", "validate"]

    for i in range(15):
        context = {
            "task": f"task_{i}",
            "type": "analysis" if i % 2 == 0 else "generation",
            "complexity": "high" if i % 3 == 0 else "medium"
        }

        # Decidir
        chosen_action = agent.decide(context, actions)
        print(f"\nğŸ¯ IteraciÃ³n {i+1}: Elegida acciÃ³n '{chosen_action}'")

        # Ejecutar y aprender
        experience = agent.execute_and_learn(
            context=context,
            action=chosen_action,
            action_params={"iteration": i},
            executor=mock_executor
        )

        print(f"   Resultado: {experience.outcome.value} (reward: {experience.reward})")

    # ReflexiÃ³n final
    print("\n" + "="*50)
    print("ğŸ“Š REFLEXIÃ“N FINAL")
    print("="*50)
    reflection = agent.reflect()
    print(reflection["reflection"])
```

## Aprendizaje por ImitaciÃ³n

```python
"""
Aprendizaje por imitaciÃ³n de expertos.
"""
import google.generativeai as genai
from typing import List, Dict, Any
import json
import os


class ExpertDemonstration:
    """DemostraciÃ³n de un experto."""

    def __init__(
        self,
        context: Dict[str, Any],
        action: str,
        reasoning: str,
        outcome: str
    ):
        self.context = context
        self.action = action
        self.reasoning = reasoning
        self.outcome = outcome


class ImitationLearner:
    """Aprende imitando demostraciones de expertos."""

    def __init__(self, api_key: str = None):
        genai.configure(api_key=api_key or os.getenv("GOOGLE_API_KEY"))
        self.model = genai.GenerativeModel("gemini-1.5-flash")
        self.demonstrations: List[ExpertDemonstration] = []
        self.learned_policies: Dict[str, str] = {}

    def add_demonstration(self, demo: ExpertDemonstration):
        """Agrega demostraciÃ³n de experto."""
        self.demonstrations.append(demo)

    def add_demonstrations_from_data(self, data: List[Dict[str, Any]]):
        """Agrega mÃºltiples demostraciones desde datos."""
        for item in data:
            demo = ExpertDemonstration(
                context=item.get("context", {}),
                action=item.get("action", ""),
                reasoning=item.get("reasoning", ""),
                outcome=item.get("outcome", "")
            )
            self.demonstrations.append(demo)

    def learn_policy(self, domain: str) -> str:
        """Aprende polÃ­tica del dominio basada en demostraciones."""
        demos_text = "\n\n".join([
            f"Contexto: {json.dumps(d.context)}\n"
            f"AcciÃ³n: {d.action}\n"
            f"Razonamiento: {d.reasoning}\n"
            f"Resultado: {d.outcome}"
            for d in self.demonstrations[-10:]
        ])

        prompt = f"""Analiza estas demostraciones de un experto en {domain}.

DEMOSTRACIONES:
{demos_text}

Extrae una polÃ­tica general que capture el comportamiento del experto.
La polÃ­tica debe ser:
1. Generalizable a nuevos contextos
2. Basada en patrones observados
3. Expresada como reglas claras

POLÃTICA APRENDIDA:"""

        response = self.model.generate_content(prompt)
        policy = response.text

        self.learned_policies[domain] = policy
        return policy

    def imitate(
        self,
        context: Dict[str, Any],
        domain: str = None
    ) -> Dict[str, Any]:
        """Imita comportamiento de experto para nuevo contexto."""
        # Encontrar demostraciones similares
        similar = self._find_similar_demonstrations(context, limit=3)

        similar_text = "\n\n".join([
            f"Contexto similar: {json.dumps(d.context)}\n"
            f"AcciÃ³n del experto: {d.action}\n"
            f"Razonamiento: {d.reasoning}"
            for d in similar
        ])

        policy_text = ""
        if domain and domain in self.learned_policies:
            policy_text = f"\nPOLÃTICA APRENDIDA:\n{self.learned_policies[domain]}"

        prompt = f"""Imita el comportamiento de un experto para este nuevo contexto.

NUEVO CONTEXTO:
{json.dumps(context, indent=2)}

DEMOSTRACIONES SIMILARES:
{similar_text if similar_text else "No hay demostraciones similares."}
{policy_text}

Decide quÃ© acciÃ³n tomar imitando al experto. Responde con JSON:
{{
    "action": "acciÃ³n a tomar",
    "reasoning": "razonamiento basado en patrones del experto",
    "confidence": 0.0-1.0,
    "similar_demo_used": true/false
}}"""

        response = self.model.generate_content(prompt)

        try:
            text = response.text
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            return json.loads(text.strip())
        except:
            return {
                "action": "default",
                "reasoning": "No se pudo imitar",
                "confidence": 0.3
            }

    def _find_similar_demonstrations(
        self,
        context: Dict[str, Any],
        limit: int = 3
    ) -> List[ExpertDemonstration]:
        """Encuentra demostraciones con contexto similar."""
        if not self.demonstrations:
            return []

        # Similaridad simple por keys comunes
        scored = []
        context_keys = set(context.keys())

        for demo in self.demonstrations:
            demo_keys = set(demo.context.keys())
            overlap = len(context_keys & demo_keys)

            # Verificar valores similares
            matching_values = sum(
                1 for k in context_keys & demo_keys
                if context.get(k) == demo.context.get(k)
            )

            score = overlap + matching_values * 2
            scored.append((score, demo))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [demo for _, demo in scored[:limit]]


class AdaptiveLearningAgent(ContinuousLearningAgent):
    """Agente que combina aprendizaje continuo e imitaciÃ³n."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.imitation_learner = ImitationLearner(self.api_key)
        self.use_imitation = True

    def learn_from_expert(self, demonstrations: List[Dict[str, Any]]):
        """Aprende de demostraciones de experto."""
        self.imitation_learner.add_demonstrations_from_data(demonstrations)
        print(f"ğŸ“š Aprendidas {len(demonstrations)} demostraciones")

    def decide(self, context: Dict[str, Any], available_actions: List[str]) -> str:
        """Decide combinando experiencia propia e imitaciÃ³n."""
        # Obtener sugerencia de imitaciÃ³n
        imitation_result = None
        if self.use_imitation and self.imitation_learner.demonstrations:
            imitation_result = self.imitation_learner.imitate(context)

        # Obtener decisiÃ³n basada en experiencia propia
        experience_based = super().decide(context, available_actions)

        # Combinar si hay conflicto
        if imitation_result and imitation_result.get("confidence", 0) > 0.7:
            imitation_action = imitation_result.get("action")
            if imitation_action in available_actions:
                # Ponderar entre experiencia e imitaciÃ³n
                return imitation_action

        return experience_based


# Ejemplo de uso
if __name__ == "__main__":
    agent = AdaptiveLearningAgent(name="AdaptiveBot")

    # Cargar demostraciones de experto
    expert_demos = [
        {
            "context": {"task_type": "analysis", "data_size": "large"},
            "action": "chunk_and_analyze",
            "reasoning": "Para datos grandes, dividir en chunks mejora precisiÃ³n",
            "outcome": "success"
        },
        {
            "context": {"task_type": "analysis", "data_size": "small"},
            "action": "direct_analyze",
            "reasoning": "Datos pequeÃ±os se pueden analizar directamente",
            "outcome": "success"
        },
        {
            "context": {"task_type": "generation", "complexity": "high"},
            "action": "plan_then_generate",
            "reasoning": "Tareas complejas requieren planificaciÃ³n previa",
            "outcome": "success"
        }
    ]

    agent.learn_from_expert(expert_demos)

    # Probar imitaciÃ³n
    new_context = {"task_type": "analysis", "data_size": "large"}
    action = agent.decide(
        context=new_context,
        available_actions=["chunk_and_analyze", "direct_analyze", "skip"]
    )

    print(f"AcciÃ³n decidida para {new_context}: {action}")
```

## Ejercicios PrÃ¡cticos

### Ejercicio 1: Sistema de Feedback
```python
"""
Ejercicio: Implementar sistema de feedback humano.
"""

class HumanFeedbackLearner:
    """
    TODO: Implementar sistema que:
    1. Solicite feedback humano periÃ³dicamente
    2. Incorpore correcciones a las heurÃ­sticas
    3. Pondere feedback humano vs automÃ¡tico
    4. Detecte cuando el feedback es inconsistente
    """

    def request_feedback(self, experience: Experience) -> Dict[str, Any]:
        """Solicita feedback sobre una experiencia."""
        pass

    def incorporate_feedback(
        self,
        experience: Experience,
        feedback: Dict[str, Any]
    ):
        """Incorpora feedback a los modelos."""
        pass
```

### Ejercicio 2: Transferencia de Aprendizaje
```python
"""
Ejercicio: Transferir aprendizaje entre dominios.
"""

class TransferLearningAgent:
    """
    TODO: Implementar agente que:
    1. Identifique heurÃ­sticas transferibles entre dominios
    2. Adapte experiencias de un dominio a otro
    3. Mantenga conocimiento especÃ­fico de dominio
    4. Detecte cuando la transferencia no es apropiada
    """

    def transfer_knowledge(
        self,
        source_domain: str,
        target_domain: str
    ) -> List[Heuristic]:
        """Transfiere conocimiento entre dominios."""
        pass
```

## Resumen

| Componente | FunciÃ³n | CaracterÃ­sticas |
|------------|---------|-----------------|
| **ExperienceMemory** | Almacena experiencias | IndexaciÃ³n, bÃºsqueda, pruning |
| **HeuristicLearner** | Extrae reglas | Confianza adaptiva, poda automÃ¡tica |
| **ImitationLearner** | Aprende de expertos | Demostraciones, polÃ­ticas |
| **Reflection** | Auto-evaluaciÃ³n | EstadÃ­sticas, insights |
| **Feedback Loop** | Mejora continua | ActualizaciÃ³n de confianza |

---

**Siguiente:** [8.2.1 Razonamiento Causal](./8.2.1-razonamiento-causal.md)
